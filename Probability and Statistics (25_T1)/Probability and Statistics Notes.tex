\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\usepackage{color}
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linktoc=all,
    linkcolor=black,
}
\allowdisplaybreaks
\newcommand{\indep}{\perp\!\!\!\perp}

\title{Probability and Statistics Notes}
\author{Siheon Lee}
\date{October-December 2025}

\begin{document}

\maketitle
\tableofcontents
\newpage

\section{Lecture 1 (6/10/2025)}
\subsection{Slide 1}
\begin{itemize}
    \item Dr Fanyin Zheng
    \begin{itemize}
        \item Market places and online platforms
        \item Hospital data
        \item PhD in economics from Harvard
        \item Three years ago faculty at Columbia University Business School
    \end{itemize}
    \item Office Hours
    \begin{itemize}
        \item 14:00-15:00 on Monday or by Appointment
        \item Tutorial leader TBD
    \end{itemize}
    \item Textbooks
    \begin{itemize}
        \item Larsen, R., and M. Marx. Introduction to Mathematical Statistics and Its Applications. (LM in lecture slides)
        \item Stock, J. H., and M. W. Watson. Introduction to Econometrics. (SW in lecture slides)
    \end{itemize}
    \item 	Experiment – repeatable procedure and set of well-defined possible outcomes
	\item Sample space S – set of all possible outcomes
    \begin{itemize}
        \item $s \in S$ is one outcome
    \end{itemize}
    \item Event A is any collection of outcomes
    \begin{itemize}
        \item Any individual outcome, any subset of or the entire sample space
	   \item Said to occur if the realized outcome of the experiment is in A
    \end{itemize}
    \item Sample space is $S={HH,HT,TH,TT}$
    \begin{itemize}
        \item Event $A$ is at least one head, $A={HH,HT,TH}$
	   \item Event $B$ is exactly two heads $B={HH}$
    \end{itemize}
    \item Set - collection of elements
    \begin{itemize}
        \item $s \in A$ if $s$ is an element of $A$
        \item $s \notin A$ if $s$ is not an element of $A$
        \item $\emptyset$ is the empty/null set without any elements
        \item $A \subseteq S$ if all elements of $A$ are also elements of $S$
    \end{itemize}
    \item Intersection - $A \cap B$ is event whose outcomes belong to both A and B
    \item Union - $A \cup B$ is event whose outcomes belong to either A or B or both
    \item Mutually exclusive – no events in common $A \cap B=\emptyset$
    \item Complement – $A^c$ is event consisting of all the outcomes in $S$ which are not in $A$
    \item Partition - if $\bigcup_{i=1}^n A_i=S$ and $\forall i\neq j, i,j \in \{ 1,2, \dots , n \} A_i \cap A_j = \emptyset$
    \item Properties of Set Operator
    \begin{itemize}
        \item Commutative - $A \cap B = B \cap A, A \cup B = B \cup A$
        \item Associative - $(A \cap B)\cap C = A \cap (B \cap C), (A \cup B) \cup C = A \cup (B \cup C)$
        \item Distributive - $A \cup (B \cap C) = (A \cup B) \cap (A \cup C), A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
        \item De Morgan's theorem - $(A \cap B)^c = A^c \cup B^c, (A \cup B)^c = A^c \cap B^c$
    \end{itemize}
    \item There are two events, $A$ and $B$ in sample space $S$
    \begin{itemize}
        \item What are the expressions for the following events?
        \begin{itemize}
            \item Exactly one of the two occurs: $E = (A \cap B^c) \cup (B \cap A^c)$
            \item At most one of the two occurs: $E = (A \cup B)^c \cup (A \cap B^c) \cup (B \cap A^c) = (A \cap B)^c$
        \end{itemize}
    \end{itemize}
    \item Probability function - $P$ function from events to real numbers
    \begin{itemize}
        \item Probability of event $A$ is $P(A)$
        \item Axiom 1 - $P(a) \ge 0$ for any event $A$
        \item Axiom 2 - $P(S)=1$ ($S$ is sample space)
        \item Axiom 3 - If $A$ and $B$ are mutually exclusive ($A_i \cap A_j = \emptyset$ for $i \neq j$), $P(A \cup B) = P(A) + P(B)$
        \begin{itemize}
            \item If $A_i$ and $A_j$ are pairwise mutually exclusive, $P(\bigcup_{i \ge 1} A_i) = \sum_{i \ge 1} P(A_i), i=1,2,\dots$
            \item Pairwise mutually exclusive means that all pairs of events are mutually exclusive, while a mutually exclusive relationship between $n$ events just means that the $n$ events can't occur simultaneously
        \end{itemize}
        \item $P(\emptyset)=0$
        \begin{itemize}
            \item $\emptyset = S^c$ (definition of sample space and empty set)
            \item $S^c \cap S = \emptyset \cap S = \emptyset$ (definition of complement)
            \item $P(\emptyset) = P(S^c) = 1-P(S) = 1-1 = 0$ (Definition of probability of complement and Axiom 2)
        \end{itemize}
        \item $P(A^c)=1-P(A)$
        \begin{itemize}
            \item $A \cup A^c = S$ (definition of complement)
            \item $P(S) = P(A \cup A^c) = 1$ (Axiom 2)
            \item Since $A \cap A^c = 0$ (mutually exclusive), $P(A \cup A^c) = P(A) + P(A^c) = 1$ (Axiom 3)
            \item $\Rightarrow P(A^c) = 1 - P(A)$
        \end{itemize}
        \item $P(A) \le 1$
        \begin{itemize}
            \item $P(A^c) = 1 - P(A)$ (conculsion from above)
            \item $P(A^c) \ge 0$ (Axiom 1)
            \item $0 \le 1 - P(A) \Rightarrow P(A) \le 1$
        \end{itemize}
        \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        \begin{itemize}
            \item $P(A) = P(A \cap B) + P(A \cap B^c)$
            \item $P(B) = P(A \cap B) + P(A^c \cap B)$
            \item $P(A) + P(B) = \underset{P(A \cup B)}{\underbrace{[P(A \cap B) + P(A \cap B^c) + P(A^c \cap B)]}} + P(A \cap B) = P(A \cup B) + P(A \cap B)$
            \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        \end{itemize}
    \end{itemize}
    \item If the sample space $S$ contains finite number of equally likely outcomes: $P(A) = \frac{\text{number of outcomes in } A}{\text{total number of outcomes in } S}$
    \item If an experiment has 2 parts, the experiment has $m \times n$ outcomes if first part has $m$ possible outcomes and second part has $n$ possible outcomes
    \item If a password is required to have 8 characters (letters or numbers) and is case-sensitive, how many possible outcomes are there?
    \begin{itemize}
        \item $(26 \cdot 2 + 10)^8 = 62^8 \approx 218$ trillion
    \end{itemize}
    \item 10 runners compete in a race. Assume ties are not possible. All 10 completes the race. How many possible outcomes are there for the first, second, and third winners?
    \begin{itemize}
        \item ${}_{10}P_3 = \frac{10!}{(10-3)!} = 10 \cdot 9 \cdot 8$
    \end{itemize}
    \item Sampling With Replacement - take $k$ draws from group of size $n$ options $(n \ge k)$
    \begin{itemize}
        \item $n \times n \times \cdots = n^k$
    \end{itemize}
    \item Sampling Without Replacement - take $k$ draws from a group of size $n$ options
    \begin{itemize}
        \item $n \times (n-1) \times (n-2) \times \cdots \times (n - (k-1)) = \frac{n(n-1)(n-2) \cdots 3 \cdot 2 \cdot 1}{(n-k)(n-k-1) \cdots 3 \cdot 2 \cdot 1} = \frac{n!}{(n-k)!}$
        \item If $n=k$, we have $n!$ - the number of permutations of $n$ objects
        \item Note $0! = 1$
    \end{itemize}
    \item Consider our runners example. Suppose we only care about which 3 runners won the top 3 places, but not which one of the three is first, second, or third
    \begin{itemize}
        \item $\binom{10}{3} = \frac{10!}{(10-3)!3!} = \frac{10!}{7! \cdot 3!} = \frac{10 \cdot 9 \cdot 8}{3 \cdot 2} = 10 \cdot 3 \cdot 4 = 120$
    \end{itemize}
    \item Combination - Unordered collection of $k$ elements
    \begin{itemize}
        \item Recall the number of ordered collection is $_nP_k = \frac{n!}{(n-k)!}$
        \item Each combination of $k$ distinct elements shows up $k!$ times
        \item So the number of unordered combinations is $\binom{n}{k} = \frac{n!}{k!(n-k)!}$
        \item Note $0 \le k \le n, \binom{n}{0} = \binom{n}{n}=1$
    \end{itemize}
\end{itemize}


\section{Tutorial 1 (10/10/2025)}
\begin{itemize}
    \item Probability is a social science because it deals with the world of uncertainty
    \item $A \cap B = \emptyset \Rightarrow $ mutually exclusive (ME)
    \item $A \cup B = S \Rightarrow $ collectively exhaustive (CE)
    \item If $A$ and $B$ are ME and CE, $A$ and $B$ form a partition of $S$
    \item Property 1 - $P(A^c)=1-P(A)$
    \begin{itemize}
        \item Since $A \cap A^c = \emptyset$, it holds by Axiom 3 that $P(A \cup A^c) = P(A) + P(A^c)$. Further, since $A \cup A^c = S$, it holds by Axiom 2 that $P(A \cup A^c) = P(S) = 1$. Putting these two results together, the required result follows.
    \end{itemize}
    \item Property 2 - $P(\emptyset) = 0$
    \begin{itemize}
        \item Let $A$ be $S$ and $A^c$ be $\emptyset$ in the proof of Property 1. The required result follows.
    \end{itemize}
    \item Property 3 - $P(A) \le 1$ for any event $A$
    \begin{itemize}
        \item If $P(A) > 1$, then Property 1 implies that $P(A^c) < 0$ but this is in violation of Axiom 1. The required result is thus proved by contradiction
    \end{itemize}
    \item Theorem 4 - $P(A \cup B)=P(A)+P(B)-P(A \cap B)$
    \begin{itemize}
        \item Note that $A=(A \cap B) \cup (A \cap B^c)$ and $B=(A \cap B) \cup (A^c \cap B)$
        \item Also note that $(A \cup B)=(A \cap B^c) \cup (A \cap B) \cup (A^c \cap B)$
        \item Note that all bracketed objects are mutually exclusive events in their respective equations
        \item So by Axiom 3, we have $P(A \cap B^c) = P(A) - P
        (A \cap B)$, $P(A^c \cap B) = P(B) - P
        (A \cap B)$, and $P(A \cup B) = P(A \cap B^c) + P(A \cap B) + P(A^c \cap B) = P(A) - P
        (A \cap B) + P(A \cap B) + P(B) - P
        (A \cap B) = P(A) + P(B) - P(A \cap B)$
    \end{itemize}
    \item Integration by Parts
    \begin{itemize}
        \item $\int udv = uv - \int vdu$
        \item Choose $u$ as the function that comes first on this list:
        \begin{itemize}
            \item \textbf{L}ogarithmic
            \item \textbf{A}lgebraic
            \item \textbf{T}rigonometric
            \item \textbf{E}xponential
        \end{itemize}
    \end{itemize}
    \item Example 1. Find $\int_0^\infty x \lambda e^{-\lambda x} dx$
    \begin{itemize}
        \item $u=x$ and $v=-e^{-\lambda x}$
        \item $du=dx$ and $dv = \lambda e^{-\lambda x} dx$
        \item $\int_0^\infty x \lambda e^{-\lambda x} dx = -x e^{-\lambda x} \vert_0^\infty + \int_0^\infty e^{-\lambda x} dx = 0 + \frac{e^{-\lambda x}}{-\lambda} \vert_0^\infty = \frac{1}{\lambda}$
        \begin{itemize}
            \item Helps obtain the first moment of an exponentially distributed random variable
        \end{itemize}
    \end{itemize}
    \item Example 2. Find $\int_0^\infty x^2 \lambda e^{-\lambda x} dx$
    \begin{itemize}
        \item $u=x^2$ and $v=-e^{-\lambda x}$
        \item $du=2xdx$ and $dv = \lambda e^{-\lambda x} dx$
        \item $\int_0^\infty x^2 \lambda e^{-\lambda x} dx = -x^2 e^{-\lambda x} \vert_0^\infty + \int_0^\infty 2x e^{-\lambda x} dx = 0 + \frac{2}{\lambda} \int_0^\infty x \lambda e^{-\lambda x} dx = \frac{2}{\lambda^2}$
        \begin{itemize}
            \item Helps obtain the second moment of an exponentially distributed random variable
        \end{itemize}
    \end{itemize}
    \item Double Integrals
    \begin{itemize}
        \item
        \begin{flalign*}
            &f(x,y) = \left\{ \begin{array}{rcl}
            x+y \quad &, &0<x<1 \ \& \ 0<y<1 \\
            0 \quad &, &\text{ otherwise.}
            \end{array} \right. \\
            &0<x<1 \ \& \ 0<y<1 \text{ is support of function}
        \end{flalign*}
        \item
        \begin{flalign*}
            &\int_{-\infty}^\infty f_{X,Y}(x,y)dxdy \\
            &= \int_0^1 \int_0^1 (x+y)dxdy = \int_0^1 [\frac{x^2}{2} + xy]_0^1 dy \\
            &= \int_0^1 (\frac{1}{2} + y)dy = [\frac{1}{2}y+\frac{y^2}{2}]_0^1 = 1
        \end{flalign*}
        \item Suppose we only want to integrate over the triangular region given by $\{(x,y): 0<x<1, 0<y<1, y>2x\}$
        \begin{flalign*}
            \int_0^{\frac{1}{2}} \int_{2x}^1 (x+y)dydx = \int_0^{1} \int_0^{\frac{y}{2}} (x+y) dxdy = \int_0^{1} [\frac{x^2}{2} + xy]_0^{\frac{y}{2}} dy = \int_0^{1} (\frac{5y^2}{8}) dy = [\frac{5y^3}{24}]_0^{1} = \frac{5}{24} 
        \end{flalign*}
    \end{itemize}
    \item Support - region of domain that takes nonzero values
\end{itemize}

\section{Lecture 2 (13/10/2025)}
\begin{itemize}
    \item Permutation with replacement - $n^k$
    \item Permutation without replacement - $\frac{n!}{(n-k)!}$
    \item Combination without replacement - $\binom{n}{k} = \frac{n!}{(n-k)!k!}$
    \item Combination with replacement - $\binom{n+k-1}{k} = \frac{(n+k-1)!}{k!(n-1)!}$
    \item An urn contains eight chips, numbered 1 through 8. Three chips are drawn without replacement. What is the probability that the largest chip in the sample is a 5?
    \begin{itemize}
        \item 3 chips are not ordered, draws are taken without replacement
        \item Chip \#5 must be selected: $\binom{1}{1}$
        \item 2 chips drawn from \#1, \#2, \#3, \#4: $\binom{4}{2}$
        \item $\frac{\binom{1}{1}\binom{4}{2}}{\binom{8}{3}}$
        \item $P(A)=\frac{\binom{1}{1} \binom{4}{2}}{\binom{8}{3}} = \frac{4 \cdot 3 \cdot 3! \cdot 5!}{8! c\dot 2} = \frac{4 \cdot 2 \cdot 3 \cdot 3}{8 \cdot 7 \cdot 6 \cdot 2} = \frac{3}{28} = 0.11$
    \end{itemize}
    \item  An urn contains n red chips numbered 1 to n, n white chips numbered 1 to n, and n blue chips numbered 1 to n. Two are drawn at random and without replacement. What is the probability that the two drawn are either the same color or the same number?
    \begin{itemize}
        \item unordered, without replacement
        \item $P(\text{same color or same number}) = P(\text{same color}) + P(\text{same number}) + P(\text{same color and number}) = P(\text{same color}) + P(\text{same number})$ since you can't choose the same color and number chip twice
        \item Note the total number of ways to draw 2 chips from $3n$ chips is $\binom{3n}{2}$
        \item $P(\text{same color}) = P(\text{2 reds}) + P(\text{2 whites}) + P(\text{2 blues}) = 3 \cdot \frac{\binom{n}{2}}{\binom{3n}{2}} = \frac{3 \cdot n!(3n-2)!2!}{3n!(n-2)!2!} = \frac{n - 1}{3n - 1}$
        \item $P(\text{same number}) = P(\text{2 chips of \#1}) + \cdots + P(\text{2 chips of \#n}) = n \cdot \frac{\binom{3}{2}}{\binom{3n}{2}} = \frac{n \cdot 3! (3n-2)! 2!}{2! 1! 3n!} = \frac{2}{3n - 1}$
        \item $P(\text{same color or same number}) = \frac{2+n-1}{3n-1} = \frac{n+1}{3n-1}$
    \end{itemize}
    \item What is the probability that at least two people in this room were born on the same day?
    \begin{itemize}
        \item Consider $k$ people in the room
        \item Line them up $1,2,\dots, k$, each has 365 possible birthdays
        \item So in total there are $365^k$ possible birthdays
        \item $A =$ at least 3 share the same birthday
        \item $A^c =$ all $k$ people have different birthdays
        \item Take (ordered) $k$ draws from 365 without replacement $=365 \times 364 \times \cdots \times (365-(k-1))$
        \item $P(\text{Born on same day}) = 1 - P^c = 1 - \frac{365 \times 364 \times \cdots \times (365 - k +1)}{365^k}$
        \item When $k=70, P(A)=.99$
    \end{itemize}
\subsection{Slide 2}
    \item Conditional Probability - for events $A,B$ and $P(B) > 0$, $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$
    \item A family has two children. What is the probability that both are girl given that at least one is girl?
    \begin{itemize}
        \item $P(\text{both girls} \mid \text{at least one girl}) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{4}}{\frac{3}{4}} =\frac{1}{3}$
    \end{itemize}
    \item We can rewrite the conditional probability formula to calculate $P(A \cap B) = P(A \mid B) P(B) = P(B \mid A) P(A)$
    \item Law of Total Probability - for a set of events $A_1,A_2,\dots,A_n$ which form a partition of the sample space, $P(B) = P(B \mid A_1)P(A_1)+ \cdots + P(B \mid A_n)P(A_n) = \sum_{i=1}^n P(B \mid A_i) P(A_i)$
    \begin{itemize}
        \item Since $A_1,\dots,A_n$ form a partition of $S$, $B = (B \cap A_1) \cup (B \cap A_2) \cup \dots \cup (B \cap A_n)$
        \item $P(B) = P(B \cap A_1) + \dots + P(B \cap A_n)$
        \item Each $P(B \cap A_i) = P(B \mid A_i)P(A_i)$
        \item Thus $P(B) = \sum_{i=1}^n P(B \mid A_i)P(A_i)$
    \end{itemize}
    \item A student is applying for a summer internship. If their interview goes well, they have a 70\% chance of getting an offer. If not, their chance drops to 20\%. The student estimates the likelihood of their interview going well is 10\%. What is the probability the student gets an offer?
    \begin{itemize}
        \item Using LOTP: $P(\text{offer})=P(\text{offer} \mid \text{good interview})P(\text{good interview}) + P(\text{offer} \mid \text{bad interview})P(\text{bad interview}) = 0.7 \cdot 0.1 + 0.2 \cdot 0.9 = 0.07 + 0.18 = 0.25$
    \end{itemize}
    \item Bayes' Rule - A set of events $A_1,A_2, \dots, A_n$ form a partition of the sample space. For any event $B$ where $P(B)>0$,
    $P(A_j \mid B) = \frac{P(B \mid A_j) P(A_j)}{\sum_{i=1}^n P(B \mid A_i) P(A_i)} = \frac{P(A_j \cap B)}{P(B)}$
    \item $P(A_j \mid B) = \frac{P(A_j \cap B)}{P(B)} = \frac{P(B \mid A_j)P(A_j)}{P(B)}$
    \begin{itemize}
        \item By Law of Total Probability, $P(B) = \sum_{i=1}^n P(B \mid A_i)P(A_i)$
        \item $P(A_j \mid B) = \frac{P(B \mid A_j)P(A_j)}{\sum_{i=1}^n P(B \mid A_i)P(A_i)}$
    \end{itemize}
    \item A biased coin, twice as likely to come up heads as tails, is tossed once. If it shows heads, a chip is drawn from urn I, which contains three white chips and four red chips; if it shows tails, a chip is drawn from urn II, which contains six white chips and three red chips. Given a white chip was drawn, what is the probability that the coin came up tails?
    \begin{itemize}
        \item $P(\text{tails} \mid \text{white chip}) = \frac{P(\text{white chip} \mid \text{tails}) P(\text{tails})}{\sum_{i=1}^n P(B \mid A_i) P(A_i)}= \frac{\frac{2}{3} \frac{1}{3}}{(\frac{2}{3} \frac{3}{7} + \frac{1}{3} \frac{2}{3})} = \frac{7}{16}$
    \end{itemize}
\end{itemize}

\section{Tutorial 2 (17/10/2025)}
\begin{itemize}
    \item Find out if order matters and if it is with or without replacement
    \item Bose-Einstein Formula for with replacement but can't distinguish order
    \item Law of Total Probability
    \begin{itemize}
        \item Suppose $B_1,B_2,B_3$ form partition of $S$
        \item By Axiom 3, $A = (A \cap B_1) \cup (A \cap B_2) \cup (A \cap B_3)$
        \item By the definition of conditional probability, we have $P(A) = P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + P(A \mid B_3)P(B_3)$
    \end{itemize}
    \item Suppose we have 3 students: $A,B,C$ and 2 tasks: cooking and cleaning
    \begin{itemize}
        \item In how many ways can we assign the 3 students to the 2 tasks?
        \begin{itemize}
            \item Order matters and we allow replacement: $3^2 = 9$
            \item Order matters but we do not replacement: $_3P_2 = \frac{3!}{1!} = 6$
            \item Order does not matter but we allow replacement: $\binom{3+2-1}{2}=\binom{4}{2} = \frac{4!}{2!2!} = 6$
            \begin{itemize}
                \item Ignore for first-year course in statistics
            \end{itemize}
            \item Order does not matter and we do not allow replacement: $\binom{3}{2} = \frac{3!}{2!1!} = 3$
        \end{itemize}
    \end{itemize}
\subsection{Problem Set 1}
    \item 2. Let $A$ and $B$ be two events defined on a sample space $S$ such that $P(A)=0.3, P(B)=0.5, P(A \cup B)=0.7$
    \begin{itemize}
        \item (a) $P(A \cap B) = P(A) + P(B) - P(A \cup B) = 0.3+0.5-0.7 = 0.1$
        \item (b) $P(A^c \cup B^c) = P((A \cap B)^c)$ by DeMorgan's Law
        \begin{itemize}
            \item $= 1 - P(A \cap B)$ by "Property 1"
            \item $= 1 - 0.1 = 0.9$
        \end{itemize}
        \item (c) Note that $B$ may be partitioned as $B = (B \cap A) \cup (B \cap A^c)$
        \begin{itemize}
            \item $P(B) = P(B \cap A) + P(B \cap A^c)$ by Axiom 3
            \item $P(B \cap A^c) = P(B) - P(B \cap A) = 0.5-0.1 = 0.4$
        \end{itemize}
    \end{itemize}
    \item 3a. How many ways are there to split a dozen people into 3 teams, where one team has 2 people, and the other two teams have 5 people each?
    \begin{itemize}
        \item No replacement and order is irrelevant
        \item $\binom{12}{2}$ ways of picking a team of 2 from 12 people, $\binom{10}{5}$ ways of picking a team of 5 from 10 remaining people, and $\binom{5}{5}$ ways of picking the last team of 5
        \item $\frac{\binom{12}{2}\binom{10}{5}\binom{5}{5}}{2} = 8316$ accounting for the double counting due to the two teams of 5 being indistinguishable
    \end{itemize}
    \item How many ways are there to split a dozen people into 3 teams, where each team has 4 people?
    \begin{itemize}
        \item Using the same logic as 3a, we get $\frac{\binom{12}{4}\binom{8}{4}\binom{4}{4}}{3!} = 5775$
    \end{itemize}
    \item 4. In the seventeenth century, Italian gamblers used to bet on the total number of spots rolled with three dice. They believed that the chance of rolling a total of 9 ought to equal the chance of rolling a total of 10. They noted that altogether there are six combinations to make 9: (1,2,6), (1,3,5), (1,4,4), (2,3,4), (2,2,5), and (3,3,3). Similarly, there are six combinations for 10: (1,4,5), (1,3,6), (2,2,6), (2,3,5), (2,4,4), (3,3,4). Thus, argued the gamblers, 9 and 10 should have the same chance. Empirically, they found this not to be true, however. Galileo solved the gamblers’ problem.
    \begin{itemize}
        \item (a) How many permutations of three dice are there that sum to 10?
        \begin{itemize}
            \item $\binom{3}{1}\binom{2}{1}\binom{1}{1} = 6$ ways to obtain: (1,2,6), (1,3,5), (2,3,4)
            \item $\frac{1}{2!}\binom{3}{1}\binom{2}{1}\binom{1}{1} = 3$ ways to obtain: (2,2,5) and (1,4,4)
            \item $\frac{1}{3!} \binom{3}{1}\binom{2}{1}\binom{1}{1} = 1$ way to obtain: (3,3,3)
            \item There are 25 possible equally likely ways to obtain 9
        \end{itemize}
        \item (b) How many permutations of three dice are there that sum to 10?
        \begin{itemize}
            \item $\binom{3}{1}\binom{2}{1}\binom{1}{1} = 6$ ways to obtain: (1,4,5), (1,3,6), (2,3,5)
            \item $\frac{1}{2!}\binom{3}{1}\binom{2}{1}\binom{1}{1} = 3$ ways to obtain: (2,2,6), (2,4,4), (3,3,4)
            \item There are 27 possible equally likely ways to obtain 10
        \end{itemize}
        \item (c) How many total permutations of three dice are there? What was Galileo's solution?
        \begin{itemize}
            \item There are $6^3=216$ total equally likely ways to roll 3 dice. Hence, the solution would have been: $P(\text{roll a sum of 9})=\frac{25}{216}$ and $P(\text{roll a sum of 10})=\frac{27}{216}$
        \end{itemize}
    \end{itemize}
    \item You and your friends just rented a car from Enterprise for an 8,000 mile cross-country road trip in the U.S. to see all of the sights from from Boston Harbor to the Golden Gate Bridge. Your rental car may be of three different types: brand new (and not a lemon), nearly 1 year old, or a lemon (bound to break down). That many miles can be demanding on a rental car. If the car you receive is brand new (New), it will break down with probability 0.05. If it is one year old (One), it will break down with probability 0.1. If it is just a lemon (Lemon), it will break down with probability 0.9. The probability that the car Enterprise gives you is New, One, or Lemon is 0.8, 0.1, and 0.1, respectively. Compute the probability that your car is going to break down on your road trip.
    \begin{itemize}
        \item $P(\text{New}) = 0.8$
        \item $P(\text{Break} \mid \text{New}) = 0.05$
        \item $P(\text{One}) = 0.1$
        \item $P(\text{Break} \mid \text{One}) = 0.1$
        \item $P(\text{Lemon}) = 0.1$
        \item $P(\text{Break} \mid \text{Lemon}) = 0.9$
        \item Using LOTP, $P(\text{Break}) = P(\text{Break} \mid \text{New})P(\text{New}) + P(\text{Break} \mid \text{One})P(\text{One}) + P(\text{Break} \mid \text{Lemon})P(\text{Lemon}) = (0.05)(0.8)+(0.1)(0.1)+(0.9)(0.1) = 0.14$
    \end{itemize}
\end{itemize}

\section{Lecture 3 (20/10/2025)}
\begin{itemize}
    \item Tests: Write as much details as I can under limited amount of time
    \begin{itemize}
        \item Lecture/Tutorials are good benchmarks (work through problem sets)
        \item Partial credit awarded
    \end{itemize}
    \item Independence
	\begin{itemize}
		\item Events $A$ and $B$ are independent if $P(A \cap B) = P(A) \cdot P(B)$
		\item If $P(A) > 0$ and $P(B) > 0$, then $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) \cdot P(B)}{P(B)} = P(A)$
		\item $P(A \mid B) = P(A)$ and $P(B \mid A) = P(B)$
	\end{itemize}
    \item Independence and mutually exclusiveness does not imply each other in general
    \begin{itemize}
        \item $A$ and $A^c$ are mutually exclusive, but one happening tells us the other
    \end{itemize}
    \item Independence of Multiple Events: happens only if all four equations hold
    \begin{itemize}
        \item $P(A \cap B) = P(A)(B)$
        \item $P(B \cap C) = P(B)(C)$
        \item $P(A \cap C) = P(A)(C)$
        \item $P(A \cap B \cap C) = P(A)P(B)P(C)$
    \end{itemize}
    \item A fair coin is tossed twice. Let event A be tail on first toss and event B be head on second toss. Are events A and B independent?
    \begin{itemize}
        \item $P(A) = \frac{1}{2}$, $P(B) = \frac{1}{2}$, $P(A \cap B) = \frac{1}{2} \cdot \frac{1}{2} = P(A) \cdot P(B)$
        \item Another way: $P(A \mid B) = P(\text{head on 2nd toss} \mid \text{tail on 1st toss}) = P(\text{head on 2nd toss}) = \frac{1}{2} = P(A)$
    \end{itemize}
    \item We roll a fair die once, event $A = \{2, 4, 6\}$, event $B = \{1, 2, 3, 4\}$, are $A$ and $B$ independent?
    \begin{itemize}
        \item $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{3}}{\frac{2}{3}} = \frac{1}{2} = P(A)$
    \end{itemize}
    \item Consider tossing a fair coin twice. Let $A$ be head on first toss, $B$ head on second toss, and $C$ both tosses have the same result
    \begin{itemize}
        \item $P(A)=\frac{1}{2}, P(B)=\frac{1}{2}, P(C)= \frac{1}{2}$
        \item $P(A \cap B) = \frac{1}{4} = P(A)P
        (B)$
        \item $P(A \cap C) = \frac{1}{4} = P(A)P(C)$
        \item $P(B \cap C) = \frac{1}{4} = P(B)P(C)$
        \item $P(A \cap B \cap C) = P(A \cap B) = \frac{1}{4} \neq P(A)P(B)P(C)$
    \end{itemize}
    \item If $P(A) = 0, P(A \cap B \cap C)=P(A)P(B)P(C)$ holds, but the pairwise independence of $B$ and $C$ is not confirmed
    \item Suppose you are decorating the lobby for an event with a string of lights which has 1000 light bulbs. Each bulb functions independently and has a 99.9\% probability of working. The whole string fails when at least one bulb fails because of the way the bulbs are connected on the string. What is the probability that the string fails?
    \begin{itemize}
        \item $P(\text{fail}) = 1 - P(A \cap B \cap C \dots) = 1 - P(\text{bulb working})^{1000} = 1 - 0.99^{1000} \approx 0.63$
    \end{itemize}
    \item Monty Hall Paradox
    \begin{itemize}
        \item 3 doors behind one of which is a prize
        \item Contestant chooses a door first
        \item Host, Monty Hall, opens one of the remaining doors that doesn't have the prize
        \item If both remaining doors have no prize, he opens each with probability $1/2$
        \item After Monty opens the door, the contestant is given the opportunity to switch their choice. Should the contestant switch?
        \item $C_i$ represents the car (prize) being behind door $i$
        \item $M_i$ represents Monty opening door $i$
        \item We assume the contestant opens door 1 and Monty opens door 2 without loss of generality
        \begin{flalign*}
            &P(C_1) = P(C_2) = P(C_3) = \frac{1}{3} \\
            &P(M_2 \mid C_1) = \frac{1}{2} \\
            &P(M_2 \mid C_2) = 0 \\
            &P(M_2 \mid C_3) = 1 \\
            &P(C_1 \mid M_2) = \frac{P(M_2 \mid C_1)P(C_1)}{P(M_2 \mid C_1)P(C_1) + P(M_2 \mid C_2) P(C_2) + P(M_2 \mid C_3) P (C_3)} = \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2} \times \frac{1}{3} + 0 \times \frac{1}{3} + 1 \times \frac{1}{3}} = \frac{1}{3} \\
            &P(C_3 \mid M_2) = 1 - P(C_1 \mid M_2) = \frac{2}{3}
        \end{flalign*}
        \item We see that switching doubles the probability of winning
    \end{itemize}
\subsection{Slide 3}
    \item For a coin toss, suppose we know the probability of heads is $p$. We are interested in the probability of having $k$ heads
    \begin{itemize}
        \item $P(\text{having } k \text{ heads}) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}$
    \end{itemize}
    \item Binomial Theorem - let $p$ be the probability of the success of a given trial and let us have $n$ independent trials. Then $P(k \text{ successes}) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}, k = 0,1,\dots,n$
    \begin{itemize}
        \item We say $k$ follows a binomial distribution
    \end{itemize}
    \item Random Variable - function that assigns numbers to outcomes $s \in S \rightarrow X(s)$
    \begin{itemize}
        \item Random variables allow us to speak about a particular aspect of possible outcomes and potentially redefining the sample space 
        \item In the previous example, reduced sample space from $2^n$ to $n+1$ by limiting question to number of heads
    \end{itemize}
    \item Capital letters denote random variables $X,Y$, etc.
    \begin{itemize}
        \item Lower-case letters used to denote specific or realized values of random variable
    \end{itemize}
    \item The probability function $P$ on the sample space $S$ induces a probability distribution for $X$ via $X$ is $P(X \in A) = P(\{s \in S : X(s) \in A \})$ for any event $A$ which is a subset of real numbers in the range of $X$
    \begin{itemize}
        \item Random variable $X$ maps each outcome $s \in S$ to $X(s)$
    \end{itemize} 
    \item Discrete Random Variable - variable with a discrete number of possible events (countable number of values $x_1,x_2,\dots,x_n$)
    \item Probability Density (Mass) Function (pdf/pmf) - the likelihood of different outcomes for a random variable
    \begin{itemize}
        \item  $p_X(x)=P(\{s \in S: X(s) = x\})$
        \item Note $p(x)=0$ for any $x$ not in the range of $X$
    \end{itemize}
    \item PDF Properties
    \begin{itemize}
        \item $0 \le p(x) \quad \forall x \in \mathbb{R}$
        \item $\sum_i p(x_i)=1,$ i.e., the sum of the probabilities of all possible values of $X$ (i.e., $X$ takes on values $x_1,x_2, \dots ,x_i$) is 1
        \item Mutually exclusive property given by the definition of pdf
        \item Note $p(x)$ is defined for all $x \in \mathbb{R}$ but $p(x)=0$ for $x$ not in the range of $X$
    \end{itemize}
    \item Cumulative Distribution Function - the probability that $X$ takes on a value $\le x$
    \begin{itemize}
        \item $F(x)=P(\{s \in S \mid X(s) \le x\})$
    \end{itemize}
    \item CDF Properties of a discrete random variable $X$
    \begin{itemize}
        \item $F(x)$ is a step function, remaining constant in all intervals between possible values of $X$
        \item At a possible value $x_i$ of $X$, $F(x)$ jumps up by the amount $p(x_i)=P(X=x_i)$
        \item At such an $x_i$, the value of $F(x_i)$ is the value at the top of the jump ($F_X$ is right-continuous)
    \end{itemize}
    \item Continuous Random Variable - random variable $X$ that can take on any values in some interval of the real line
    \begin{itemize}
        \item Can take more than countably many values, and therefore PDF becomes a little bit more involved
        \item Probability of any particular value typically has to be zero ($P(X=x)=0$)
        \item General idea: discretize the distribution by putting the possible values the random variable can take into "bins" ($P(x_1 \le X \le x_2$)
    \end{itemize}
    \item Probability Density Function (pdf) - non-negative function $f_X(x)$ defined as $[a,b] \subset S, P(X \in [a,b]) = \int_a^b f(x)dx$
    \item Histogram approximations to derive PDF is often used to estimate probability distribution in practice
    \item PDF Properties for continuous random variables
    \begin{itemize}
        \item $f(x) \ge 0 \quad \forall x \in \mathbb{R}$
        \item $\int_{-\infty}^\infty f(x)=1$
    \end{itemize}
\end{itemize}

\section{Tutorial 3 (24/10/2025)}
\begin{itemize}
\subsection{Problem Set 2}
    \item 1.  Two events $A$ and $B$ satisfy the following conditions: (1) the probability that $A$ occurs but $B$ does not occur is 0.2, (2) the probability that $B$ occurs but $A$ does not occur is 0.1, and (3) the probability that neither occurs is 0.6. What is $P(A \mid B)$?
    \begin{itemize}
        \item We are given $P(A \cap B^c) = 0.2, P(B \cap A^c) = 0.1, P(A^c \cap B^c) = 0.6$
        \item We want $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$
        \item Consider $P(A \cap B)$: we know that $P(A \cup B) = 1- P((A \cup B)^c) = 1 - P(A^c \cap B^c) = 1-0.6 = 0.4$ where we use De Morgan's Law and "Property 1"
        \begin{itemize}
            \item Further $P(A \cup B) = P(A \cap B^c) + P(A \cap B) + (B \cap A^c)$ due to Axiom 3 since $(A \cap B^c), (A \cap B), (B \cap A^c)$ form a partition of $(A \cup B)$
            \item It follows that $P(A \cap B) = 0.4-0.2-0.1=0.1$
        \end{itemize}
        \item Now consider $P(B)$: we know that $P(B) = P(B \cap A) + P(B \cap A^c)$ by Axiom 3 since $(B \cap A)$ and $(B \cap A^c)$ form a partition of $B$. It follows that $P(B) = 0.1+0.1 = 0.2$
        \item We thereby obtain $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{0.1}{0.2} = 0.5$
    \end{itemize}
    \item 2. Two events A and B are independent. Show the following pairs of events are also independent:
    \begin{itemize}
        \item (a) $A$ and $B^c$
        \begin{itemize}
            \item $P(A \cap B^c) = P(A) - P(A \cap B) = P(A) [1 - P(B)] = P(A)P(B^c)$ since $A \indep B$
            \item Thus $A \indep B^c$
        \end{itemize}
        \item (b) $A^c$ and $B$
        \begin{itemize}
            \item The proof of $A^c \indep B$ is obtained as the proof of $A \indep B^c$ given in part (a) with the notation for $A$ and $B$ interchanged
        \end{itemize}
        \item (c) $A^c$ and $B^c$
        \begin{itemize}
            \item $P(A^c \cap B^c) = P((A \cup B)^c)$ by De Morgan's Law
            \item $= 1- P(A \cup B)$ by "Property 1"
            \item $= 1 - P(A) - P(B) + P(A \cap B)$ by "Property 4"
            \item $= 1 - P(A) - P(B) + P(A)P(B)$ since $A \indep B$
            \item $= [1 - P(A)][1 - P(B)]$
            \item $= P(A^c)P(B^c)$ by "Property 1"
            \item Thus $A^c \indep B^c$
        \end{itemize}
    \end{itemize}
    \item 3. Mr. Patiente is testing for a rare medical condition which is known to affect 1\% of the population. The test is 95\% accurate which means two things in this particular case: 1) with probability 0.95 the test result is positive for someone with the condition; 2) with probability 0.95 the test result is negative for someone without the condition. What is the probability Mr. Patiente actually has the condition if his test result is positive?
    \begin{itemize}
        \item $D$ represents having the rare medical condition
        \item $+/-$ represents a positive/negative test result
        \item $P(D) = 0.01, P(D^c) = 0.99$
        \item $P(+ \mid D) = 0.95, P(+ \mid D^c) = 0.05$
        \item $P(- \mid D) = 0.05, P(- \mid D^c) = 0.95$
        \item We want to find $P(D \mid +) = \frac{P(+ \mid D) P(D)}{P(+)} = \frac{P(+ \mid D) P(D)}{P(+ \mid D)P(D) + P(+ \mid D^c)P(D^c)} = \frac{0.95 \cdot 0.01}{0.95 \cdot 0.01 + 0.05 \cdot 0.99} = \frac{95}{95 + (5 \times 99)} \approx 0.16$
    \end{itemize}
    \item Ragvir's Extra Practice Questions:
    \begin{itemize}
        \item 1. Establish whether the following claims are TRUE or FALSE:
        \begin{itemize}
            \item (a) "An event cannot be independent of itself"
            \begin{itemize}
                \item False since if $A \indep A$, $P(A \cap A) = P(A)P(A) = [P(A)]^2=P(A)$
                \item If $P(A)=0$ or $P(A)=1$, then $A \indep A$
            \end{itemize}
            \item "Mutually exclusive events are necessarily independent of each other"
            \begin{itemize}
                \item False since $0 = P(A \cap B) = P(A)P(B)$ which is only true if $P(A) = 0$ or $P(B) = 0$ or both
                \item In words, events $A$ and $B$ are mutually exclusive and independent only when either or both of the events occurring have a probability of $0$
            \end{itemize}
        \end{itemize}
        \item 2. Consider two coins, one fair and one biased. The fair coin shows heads with probability a half when tossed. The biased coin shows heads with probability 1/10 when tossed. Suppose two coins have equal probabilities of being picked. After one coin is picked, it was tossed twice.
        \begin{itemize}
            \item (a) What is the probability of heads on first toss?
            \begin{itemize}
                \item $P(H_1) = P(H_1 \mid F) P(F) + P(H_2 \mid B) P(B) = \frac{1}{2} \cdot \frac{1}{2} + \frac{1}{10} \cdot \frac{1}{2} = \frac{3}{10}$
            \end{itemize}
            \item (b) What is the probability of heads on both tosses?
            \begin{itemize}
                \item $P(H_1H_2) = P(H_1H_2 \mid F)P(F) + P(H_1H_2 \mid B)P(B) = \left(\frac{1}{2}\right)^2 \cdot \frac{1}{2} + \left(\frac{1}{10}\right)^2 \cdot \frac{1}{2} = \frac{13}{100}$
            \end{itemize}
            \item (c) What is the probability that the coin is the fair one given heads on both tosses?
            \begin{itemize}
                \item $P(F \mid H_1H_2) = \frac{P(H_1H_2 \mid F)P(F)}{P(H_1H_2)} = \frac{\left(\frac{1}{2}\right)^2 \cdot \frac{1}{2}}{\frac{13}{100}} = \frac{25}{26}$
            \end{itemize}
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Lecture 4 (27/10/2025)}
\begin{itemize}
    \item Uniform Distribution - distributed over $[a,b], a < b, f(x)= \left\{ \begin{array}{cc}
        \frac{1}{b-a} & \text{if } a \le x \le b  \\
        0 & \text{otherwise}
    \end{array} \right. $
    \begin{itemize}
        \item Write as $X \sim U(a,b)$
    \end{itemize}
    \item Suppose $X$ has pdf
    \[ f(x) = \left\{ \begin{array}{cc}
        ax^2 & \text{if } 0 < x < 3 \\
        0 & \text{otherwise}
    \end{array} \right. \]
    What is $a$?
    \begin{flalign*}
        \Rightarrow \int_{-\infty}^{\infty} f(x)dx = \int_0^3 ax^2 dx = a \left[ \frac{x^3}{3} \right]_0^3 = \frac{3^3a}{3} - 0 = 1 \Rightarrow a = \frac{1}{9}
    \end{flalign*}
    \item Definition of cdf for a continuous random variable $X$ is $F(x) = P(X \le x)$, same as for discrete random variables
    \begin{itemize}
        \item For continuous random variables in general, $P(X=x)=0$, $P(a < X < b) = P(a < x \le b)$, $P(a \le x < b) = P(a \le x \le b)$
    \end{itemize}
    \item We can relate the cdf and pdf of a continuous random variable using the following equations:
    \begin{itemize}
        \item $F(x)=\int_{-\infty}^x f(t)dt$
        \item $F'(x)=\frac{d}{dx} F(x)=f(x)$
    \end{itemize}
    \item If $X \sim U(a,b)$, what is its cdf?
    \begin{flalign*}
        F(x) &= \int_{-\infty}^x f(t)dt \\
        &= \left\{ \begin{array}{cl}
            \int_{-\infty}^x 0 dt & \text{if } x < a \\
            \int_a^x \frac{1}{b-a} dt & \text{if } a \le x < b \\
            \int_{a}^b \frac{1}{b-a} dt & \text{if } x \ge b
        \end{array} \right. \\
        &= \left\{ \begin{array}{cl}
            0 & \text{if } x < a \\
            \frac{t}{b-a} \vert_a^x & \text{if } a \le x < b \\
            \frac{t}{b-a} \vert_a^b & \text{if } x \ge b
        \end{array} \right. \\
        &= \left\{ \begin{array}{cl}
            0 & \text{if } x < a \\
            \frac{x-a}{b-a} & \text{if } a \le x < b \\
            1 & \text{if } x \ge b
        \end{array} \right. \\
    \end{flalign*}
    \item The following properties of cdf applies to both discrete and continuous random variables
    \begin{itemize}
        \item $0 \le F(x) \le 1$ for all $x \in \mathbb{R}$
        \item $P(X > x)=1-F(x)$
        \item $F(x_1) \le F(x_2)$ for $x_1 < x_2$
        \item $\lim_{x \rightarrow -\infty} F(x) = 0, \lim_{x \rightarrow +\infty} F(x) = 1$
        \item $F(x)$ is right continuous
    \end{itemize}
\subsection{Slide 4}
    \item Midterm Exam
    \begin{itemize}
        \item Wednesday Nov. 5th
        \item Exam covers topics from week 1 to today
        \item Closed book, no internet access
        \item Notes on one A4 size paper (double-sided) allowed
        \item College approved calculator allowed
        \item No need to prove properties proven in slides
        \item Point is to test on how to apply those tools in solving problems
    \end{itemize}
    \item Expected Value - of discrete random variable $X$ is
    \[ E(x) = x_1 \cdot p(x_1) + x_2 \cdot p(x_2) + \dots = \sum_{x_i} x_i \cdot p(x_i) = \mu_X, \]
    where $x_i$ are all possible values $X$ can take on
    \begin{itemize}
        \item For a continuous variable $Y$, the expected value is
        \[ E(Y) = \int_{-\infty}^\infty y \cdot f(y) dy = \mu_Y \]
    \end{itemize}
    \item Suppose $X$ is a binomial random variable with $p = \frac{5}{9}$ and $n=3$. What is the expected value of $X$?
    \begin{itemize}
        \item $p(x_i)=\binom{3}{x_i} \left( \frac{5}{9} \right)^{x_i} (1 - \frac{5}{9})^{(3-x_i)}, $ where $x_i= 0,1,2,3$
        \begin{flalign*}
            E(X) &= \sum_{x_i=0}^3 x_i \cdot \binom{3}{x_i} \left( \frac{5}{9} \right)^{x_i} (1 - \frac{5}{9})^{(3-x_i)} \\
            &= 0 \times \frac{64}{729} + 1 \times \frac{240}{729} + 2 \times \frac{300}{729} + 3 \times \frac{125}{729} = \frac{5}{3} = 3 \times \frac{5}{9}
        \end{flalign*}
    \end{itemize}
    \item In general, if $X$ is binomial with parameters $n$ and $p$, $E(X)=np$
    \item Suppose $X$ is a binomial random variable with parameters $p$ and $n$, the expectation $X, E(X) = \sum_{x_i=0}^n x_i \cdot \binom{n}{x_i} p^{x_i} (1-p)^{(n-x_i)}$
    \begin{flalign*}
        E[X] &= \sum_{x_i=0}^n x_i \cdot \frac{n!}{x_i! (n-x_i)!} p^{x_i} (1-p)^{(n-x_i)} \\
        &= \sum_{x_i=1}^n \frac{n!}{(x_i-1)! (n-x_i)!} p^{x_i} (1-p)^{(n-x_i)} \qquad \text{Case when } x_i=0 \text{ is zero}\\
        &= \sum_{x_i=1}^n \frac{n \cdot (n-1)!}{(x_i-1)! (n-x_i)!} p \cdot p^{x_i-1} (1-p)^{(n-x_i)}\\
        &= np \sum_{x_i=1}^n \frac{(n-1)!}{(x_i-1)! (n-1-(x_i-1))!} p^{x_i-1} (1-p)^{(n-x_i)} \\
        &= np \underbrace{\sum_{x_i=1}^n \binom{(n-1)}{(x_i-1)} p^{x_i-1} (1-p)^{[n-1-(x_i-1)]}}_{=1} \qquad \text{sum of all possible values of the binominal distribution is 1} \\
        &= np
    \end{flalign*}
    \item Let $Y = X-1$, $Y$ takes on value $y_j = x_i - 1 = 0,1, \dots, (n-1),$ then $E[X]=np \sum_{y_j=0}^{n-1} \binom{n-1}{y_j} p^{y_j} (1-p)^{[(n-1)-y_j]} = np \underbrace{\sum_{y_j=0}^{n-1} p(y_j)}_{=1}=np$,, where $Y$ is a binomial random variable with parameters $p$ and $n-1$
    \item Researchers studying unemployment often assumes the inter-arrival time $X$ of two consecutive job offers follows an exponential distribution with parameter $\lambda: f(x)=\lambda e^{-\lambda x}, x \ge 0; f(x)=0$ otherwise
    \begin{itemize}
        \item What is the expected value of $X$?
    \end{itemize}
    \begin{flalign*}
        E[X] &= \int_{-\infty}^\infty xf(x)dx=\int_0^{\infty} \lambda x e^{-\lambda x} dx = \qquad u = x, dv= \lambda e^{-\lambda x} dx, du = dx, v = -e^{-\lambda x} \\
        &= -xe^{-\lambda x} \vert_0^\infty - \int_0^\infty -e^{-\lambda x} dx = 0 - \frac{1}{\lambda}[e^{-\lambda x}]_0^\infty = -\frac{1}{\lambda} [0-1] = \frac{1}{\lambda}
    \end{flalign*}
    \item Often, we're not interested in $X$ itself, but in some function of it (e.g., $profit=g(demand)$)
    \item If $X$ is a discrete random variable with pdf $p(x_i)$, where $x_i$ represents all possible values $X$ takes on. Let $g(X)$ be a function of $X$. Then the expected value of the random variable $g(X)$ is given by $E[g(X)]=\sum_{x_i} g(x_i)p(x_i)$, if $\sum_{x_i} \vert g(x_i) \vert p(x_i) < \infty$
    \item If $Y$ is a continuous random variable with pdf $f(y)$, and if $g(Y)$ is a continuous function, then the expected value of the random variable $g(Y)$ is $\int_{-\infty}^\infty g(y)f(y)dy$, if $\int_{-\infty}^\infty \vert g(y) \vert f(y) dy < \infty$
    \item As a special case, for random variable $W$, discrete or continuous, $E(aW+b)=aE(W)+b$
    \begin{itemize}
        \item This is because $g(W)=aW+b$ is a linear function of $W$
    \end{itemize}
    \begin{flalign*}
        E(aW+b) &= \int_{-\infty}^\infty g(W)f(w)dw = \int_{-\infty}^\infty \left( aW + b \right)f(w)dw \\
        &= a\int_{-\infty}^\infty W f(w)dw + b \int_{-\infty}^\infty f(w)dw = aE(W) + b \cdot 1 = aE(W) + b
    \end{flalign*}
    \item In general, $E[g(W)] \neq g(E[W])$
    \item A fair coin is tossed until a head appears. You will be given $\frac{1}{2}^k$ dollars if the first head occurs on the $k$th toss. How much money can you expect to be paid?
    \begin{flalign*}
        &1+a+a^2+a^3+\cdots = \sum_{k=0}^\infty a^k = \frac{1}{1-a}, \text{if } \vert a \vert <1 \\
        &E[\text{amount won}] = E\left[\left(\frac{1}{2}\right)^X\right] = \sum_{i=1}^\infty \frac{1}{2}^{x_i} \frac{1}{2}^{x_i} = \sum_{i=1}^\infty \left( \frac{1}{2}^2 \right)^{x_i} = \sum_{k=1}^\infty \frac{1}{4}^{x_i} = \frac{1}{1-\frac{1}{4}} - \frac{1}{4}^0= \frac{4}{3} - 1 = \frac{1}{3}
    \end{flalign*}
    \item One reasonable way of measuring dispersion is to calculate the possible deviation from the mean $X-\mu$, where $\mu = E[X]$
    \item Since the deviation is a random variable, we can take the expected value of the squared deviation: variance
    \item Variance - for a discrete random variable $X$ with pdf $p(x)$ and expectation $\mu_X$, variance is 
    \[ Var(X) = E[(X-\mu_X)^2] = \sum_{x_i} (x_i - \mu_X)^2 p(x_i) = V(X) = \sigma_X^2 \]
    \begin{itemize}
        \item For a continuous random variable $Y$ with pdf $f(y)$ and expectation $\mu_Y$,
        \[ Var(Y) = E[(Y-\mu_Y)^2] = \int_{-\infty}^\infty (y - \mu_Y)^2 f(y)dy = V(Y) = \sigma_Y^2 \]
    \end{itemize}
    \item Using the definition of variance, for both discrete and continuous random variables, we can show $Var(X)=E[X^2] - E[X]^2$
    \begin{itemize}
        \item $E[(X-\mu_X)^2] = E[X^2 - 2\mu_X X + E[X]^2] = E[X^2] - E[2 \mu_X X] + E[E[X]^2] = E[X^2] - 2\mu_X E[X] + E[X]^2 = E[X^2] - 2E[X]^2 + E[X]^2 = E[X^2] - E[X]^2$
        \item We are implicitly assuming $E[X^2]$ is finite
    \end{itemize}
    \item Since variance is measured in the square of the units for the random variable, an alternative measure, standard deviation, is also commonly used: $\sigma = \sqrt{\text{variance}} = \sqrt{\sigma^2}$
    \item For linear functions of random variable $W$, we also know $Var(aW+b)=a^2Var(W)=a^2\sigma^2$
    \begin{flalign*}
        Var(aW+b) &= E[ \left((aW+b)-E[aW+b]\right)^2] \\
        &= E[ \left(aW+b-aE[W]+b\right)^2] \\
        &= E[ \left(aW-aE[W] \right)^2] \\
        &= E[ a^2 \left(W-E[W] \right)^2] \\
        &= a^2 E[(W-E[W])^2] = a^2 Var(W)
    \end{flalign*}
    \item If $X$ follows an exponential distribution with parameter $\lambda$, what is $Var(X)$?
    \begin{flalign*}
        E[X^2] &= \int_0^\infty x^2 \lambda e^{-\lambda x} dx = \int_0^{\infty} x^2 d(-e^{-\lambda x}) \\
        &= x^2 (-e^{-\lambda x}) \vert_0^\infty + 2 \int_0^\infty xe^{-\lambda x}dx \\
        &=0 + 2 \frac{1}{\lambda} \underbrace{\int_0^\infty \lambda x e^{-\lambda x} dx}_{=\frac{1}{\lambda}} = \frac{2}{\lambda^2} \\
        &\Rightarrow Var(X) = E[X^2] - E[X]^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
    \end{flalign*}
    \item Intuitively, when the arrival rate ($\lambda$) is high, the variability of inter-arrival time, $Var(X)$, will be low
\end{itemize}

\section{Tutorial 4 (31/10/2025)}
\begin{itemize}
    \item Random Variable is a function
    \item Probability Mass Function (for discrete random variables)
    \begin{itemize}
        \item To recover pdf, take differences of the cdf
    \end{itemize}
    \item Probability Density Function (for continuous random variables)
    \begin{itemize}
        \item To recover pdf, take derivative of cdf
        \item Sometimes, derivative doesn't exist
    \end{itemize}
    \item Validity conditions of pdf: nonnegative and sum is 1
    \item For a continuous random variable, $P(X=x)=0$
    \begin{itemize}
        \item You can ask for a probability over an interval: $P(a \le x \le b) = \int_a^b f_X(x)dx$
    \end{itemize}
    \item $k \sim \text{HyperGeometric}(N,r,k)$
    \begin{itemize}
        \item Calculates the probability of drawing a specific number of successes (\(x\)) in a sample (\(n\)) from a population (\(N\)) containing a known number of successes (\(k\)), without replacement
        \item Used in question 1
    \end{itemize}
\subsection{Problem Set 3}
    \item Always define your support
    \item 1. Suppose an urn contains $r$ red chips and $w$ white chips, where $r + w = N$. Imagine drawing $n$ chips from the urn without replacement. What is the probability that exactly $k$ red chips are included among the $n$ that are drawn?
    \begin{itemize}
        \item Order doesn't matter and no replacement
        \item There are $\binom{N}{n}$ ways to pick $n$ chips from $N$ total chips
        \item There are $\binom{r}{k}$ ways to pick $k$ red chips from $r$ total red chips
        \item There are $\binom{w}{n-k}$ ways to pick the remaining number, $n-k$ white chips, from $w$ total white chips
        \item Let random variable $K$ denote the number of red chips drawn. Then,
        \[
        P(K=k) = \frac{\binom{r}{k}\binom{w}{n-k}}{\binom{N}{n}} = \frac{\binom{r}{k}\binom{w}{n-k}}{\binom{r+w}{n}}
        \]
        \item Always define your support
        \begin{itemize}
            \item For Question 1, $k$ is bounded below by either 0 or $n-w$, the number of red chips after you have exhausted the white chips
            \item It is bounded above by either $n$ or $r$, the number of red chips
            \item Therefore, the support is $K = \max\{0,n-w\}, \dots, \min\{r,n\}$
        \end{itemize}
    \end{itemize}
    \item 2. Suppose that two fair dice are rolled. Let the random variable $X$ denote the larger of the two faces showing. Let $F(x)$ be the cdf of $X$.
    \begin{itemize}
        \item (a) Find $F(x)$ for $x = 1, 2, \dots, 6$
        \begin{itemize}
            \item Let $X_i$ denote the outcome of the roll of die $i$ for $i=1,2$
            \item Then, $P(X_i = x) = \left\{ \begin{array}{cl}
                \frac{1}{6} & \text{for } x = 1,\dots,6 \\
                0 & \text{otherwise} 
            \end{array} \right.$
            \item So $F_{X_i}(x) = P(X_i \le x) = \left\{ \begin{array}{cl}
                0 & x<1 \\
                \frac{1}{6} & 1 \le x \le 2 \\
                \frac{2}{6} & 2 \le x \le 3 \\
                \frac{3}{6} & 3 \le x \le 4 \\
                \frac{4}{6} & 4 \le x \le 5 \\
                \frac{5}{6} & 5 \le x \le 6 \\
                1 & x \ge 6 \\
            \end{array} \right.$ for $i=1,2$
            \item Consider $X := \max \{x_1,x_2\}$
            \begin{itemize}
                \item We have
                \begin{flalign*}
                    F_X(x) &= P(X \le x) \\
                    &= P(X_1 \le x \cap X_2 \le x) \\
                    &= P(X_1 \le x)P(X_2 \le x) \\
                    &= F_{X_1}(x) F_{X_2}(x) \\
                    &= [F_{X_i}(x)]^2
                \end{flalign*}
                for $x \in \mathbb{R}$ since $X_1 \indep X_2$
            \end{itemize}
            \item Thus, $F_{X}(x) = P(X \le x) = \left\{ \begin{array}{cl}
                0 & x<1 \\
                \frac{1}{36} & 1 \le x \le 2 \\
                \frac{4}{36} & 2 \le x \le 3 \\
                \frac{9}{36} & 3 \le x \le 4 \\
                \frac{16}{36} & 4 \le x \le 5 \\
                \frac{25}{36} & 5 \le x \le 6 \\
                1 & x \ge 6 \\
            \end{array} \right.$
        \end{itemize}
        \item (b) Find $F(2.5)$
        \begin{itemize}
            \item Using $F_X(x)$, we find easily that $F_X(2.5)=\frac{4}{36}=\frac{1}{9}$
        \end{itemize}
        \item If instead we were told that the random variable $Y$ denoted both die being at least some number $y$, we would do the following:
        \begin{itemize}
            \item Consider $Y := \min \{x_1,x_2\}$
            \begin{itemize}
                \item We have
                \begin{flalign*}
                    F_Y(y) &= P(Y \ge y) \\
                    &= P(X_1 \ge x \cap X_2 \ge x) \\
                    &= P(X_1 \ge x)P(X_2 \ge x) \\
                    &= (1-F_{X_1}(x)) (1-F_{X_2}(x))
                \end{flalign*}
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item 3. A continuous random variable Y has a cdf given by $F(y) = 0$ for $y < 0$; $F(y)=y^2$ for $0 \le y \le 1$; $F(y) = 1$ for $y \ge 1$. Find $P(\frac{1}{2} < Y \le \frac{3}{4})$ in two ways, first by using the cdf and second by using the pdf.
    \begin{itemize}
        \item We are given $F_Y(y) = \left\{ \begin{array}{cl}
            0 & y < 0 \\
            y^2 & 0 \le y < 1 \\
            1 & y \ge 1
        \end{array} \right.$
        \item Using the cdf, we have $P(\frac{1}{2} < y \le \frac{3}{4}) = F_Y(\frac{3}{4})-F_Y(\frac{1}{2}) = \frac{9}{16} - \frac{1}{4} = \frac{5}{16}$
        \item We recover the pdf by taking the derivative: $f_Y(y) = \left\{ \begin{array}{cl}
            0 & y < 0 \\
            2y & 0 \le y < 1 \\
            0 & y \ge 1
        \end{array} \right.$
        \item Hence, $P(\frac{1}{2} < y \le \frac{3}{4}) = \int_{\frac{1}{2}}^{\frac{3}{4}} f_Y(y) dy = \int_{\frac{1}{2}}^{\frac{3}{4}} 2y dy = \left[ y^2 \right]_{\frac{1}{2}}^{\frac{3}{4}} = \frac{9}{16} - \frac{1}{4} = \frac{5}{16}$
    \end{itemize}
    \item For a pdf $f(x)$ to be valid, two conditions must hold:
    \begin{itemize}
        \item $f(x) \ge 0$ for all $x \in \mathbb{R}$
        \item $\int_{-\infty}^\infty f(x)dx = 1$
    \end{itemize}
    \item 4. Let $F(x)$ be the cdf of a continuous random variable and $f(x)$ be the pdf
    \begin{itemize}
        \item (a) Show that $g(x)$ defined by $g(x) = 2F(x)f(x)$ is also a valif pdf
        \begin{itemize}
            \item Since $f(x) \ge 0$ and $F(x) \ge 0$ for all $x \in \mathbb{R}$, it must be true that $g(x) = 2F(x)f(x) \ge 0$ for all $x \in \mathbb{R}$ as well.
            \item Further, we need to show that $\int_{-\infty}^\infty g(x)dx = 1$.
            \item Solution 1:
            \begin{flalign*}
                \int_{-\infty}^\infty g(x)dx &= \int_{-\infty}^\infty 2F(x)f(x)dx \\
                &= \int_{-\infty}^\infty 2F(x)F'(x)dx \\
                &= \int_{-\infty}^\infty \left[ \frac{d}{dx} F^2(x) \right] dx \\
                &= \left[ F^2(x) \right]_{-\infty}^\infty  = 1
            \end{flalign*}
            \item Solution 2:
            \begin{flalign*}
                \int_{-\infty}^\infty g(x)dx &= \int_{-\infty}^\infty 2F(x)f(x)dx \\
                u = 2F(x), v &= F(x)dx \\
                du = 2f(x)dx, dv &= f(x)dx \\
                &= [2F^2(x)]_{-\infty}^\infty - \int_{-\infty}^\infty 2F(x)f(x)dx \\
                \Rightarrow \int_{-\infty}^\infty g(x)dx &= \frac{1}{2} [2F^2(x)]_{-\infty}^\infty = \frac{1}{2} [2 - 0] = 1
            \end{flalign*}
        \end{itemize}
        \item (b) Show that $h(x)$ defined by $h(x) = \frac{1}{2}f(-x) + \frac{1}{2} f(x)$ is also a valid pdf
        \begin{itemize}
            \item Since $f(x) \ge 0, \forall x \in \mathbb{R}$, $h(x) = \frac{1}{2} f(-x) + \frac{1}{2} f(x) \ge 0$ for all $x \in \mathbb{R}$
            \item Further, $\int_{-\infty}^\infty h(x)dx = \frac{1}{2} \int_{-\infty}^\infty f(-x)dx + \frac{1}{2} \int_{-\infty}^\infty f(x)dx = \frac{1}{2} \int_{-\infty}^\infty f(-x)dx + \frac{1}{2}$
            \item Let $y=-x, dy=-dx$
            \begin{flalign*}
                \int_{-\infty}^\infty f(-x)dx = \int_{y=\infty}^{y=-\infty} f(y)(-dy) = - \int_{y=\infty}^{y=-\infty} f(y)dy = \int_{y=-\infty}^{y=\infty} f(y)dy = 1
            \end{flalign*}
            \item Hence $\frac{1}{2} \int_{-\infty}^\infty f(-x)dx = \frac{1}{2}$ and $\int_{-\infty}^\infty h(x)dx = 1$
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Lecture 5 (3/11/2025)}
\begin{itemize}
    \item Moment generating functions are optional for the tests
    \item $r$th moment - $\mu_r = E[W^r]$ where $r$ is a positive integer and $\int_{-\infty}^\infty \vert w \vert^r f(w)dw < \infty$
    \item $r$th moment about the mean $\mu$ - $\mu_r' = E[(W - \mu)^r]$
    \item Moment Generating Function - mgf for random variable $W$ is denoted $M_W(t)$ and given by
    \[ M_W(t) = E[e^{tw}] = \left\{ \begin{array}{cl}
        \sum_w e^{tw} p(w) & \text{if } W \text{ is discrete} \\
        \int_{-\infty}^\infty e^{tw} f(w)dw & \text{if } W \text{ is continuous} 
    \end{array}\right. \]
    for all values of $t$ for which the expected value exists
    \begin{itemize}
        \item Note $M_W(t)$ is a function of real numbers $t$, not a random variable
    \end{itemize}
    \item The MGF is a convenient tool to derive means and variance:
    \begin{flalign*}
        M_X'(X) &= \frac{d}{dt} E[e^{tX}] = E\left[ \frac{d}{dt} e^{tX} \right] = E[Xe^{tX}] \\
        M_X'(0) &= E[Xe^{0 \cdot X}] = E[X] \\
        M_X''(X) &= \frac{d}{dt} E[Xe^{tX}] = E\left[ \frac{d}{dt} Xe^{tX} \right] = E[X^2e^{tX}] \\
        M_X''(0) &= E[X^2e^{0 \cdot X}] = E[X^2]
    \end{flalign*}
    and we have $Var(X) = E[X^2] - E[X]^2 =  M_X''(0) - (M_x'(0))^2$
    \item We can obtain other moments $M_X^{(k)}(0) = E[X^k], k \in \mathbb{N}$
    \item Suppose that $X$ and $Y$ are random variables for which $M_X(t) = M_Y(t)$ for some interval of $t$ containing $0$. Then $X$ and $Y$ have the same distribution.
    \item If $Y=aX+b$, $X$ is a random variable, $a$ and $b$ are constants, then $M_Y(t)=e^{bt}M_X(at)$
    \[
    M_Y(t) = E[e^{tY}] = E[e^{t(aX+b)}] = E[e^{atX}e^{bt}] = e^{bt} E[e^{(at)X}] = e^{bt} M_X(at)
    \]
    \begin{itemize}
        \item Can be used to prove Central Limit Theorem and compare to different types of distributions
    \end{itemize} 
    \item Example: Binomial
    \begin{itemize}
        \item Suppose $X$ is a binomial random variable with parameter $n$ and $p$, $p(x_i) = \binom{n}{x_i} p^{x_i} (1-p)^{(n-x_i)}$
        \item $M_X(t) = E[e^{tx}] = \sum_{x_i=0}^n e^{tx_i} \cdot \binom{n}{x_i}p^{x_i} (1-p)^{(n-x_i)} = \sum_{x_i=0}^n \binom{n}{x_i}(pe^t)^{x_i} (1-p)^{(n-x_i)}$
        \item Since we know $(x+y)^n = \sum_{k=0}^n \binom{n}{k} x^k y^{n-k}$, let $x = pe^t$, $y=1-p$, $k=x_i$, then $M_X(t) = (1-p+pe^t)^n$
        \item We get $M_X'(t) = n(1-p+pe^t)^{n-1}pe^t$ and $M_X''(t) = n(n-1)pe^t(1-p+pe^t)^{n-2}pe^t + n(1-p+pe^t)^{n-1}pe^t$
        \item Thus $E[X] = M_X'(0)=np$, $E[X^2] = M_X''(0)=n(n-1)p^2 + np$, and $Var(X) = n(n-1)p^2 + np - (np)^2 = np - np^2 = np(1-p)$
    \end{itemize}
    \item Example: Exponential
    \begin{itemize}
        \item If $X$ follows an exponential distribution with paramter $\lambda$, derive its mgf, expectation, and variance
        \item $M_X(t)=E[e^{tx}] = \int_0^\infty e^{tx}f(x)dx = \int_0^\infty e^{tx} \lambda e^{-\lambda x} dx = \int_0^\infty \lambda e^{-(\lambda - t)x} dx = \frac{\lambda}{\lambda - t} \underbrace{\int_0^\infty (\lambda - t)e^{-(\lambda - t)x} dx}_{=1} = \frac{\lambda}{\lambda - t}$
        \item Above equation holds when $t < \lambda$, and $M_X(t)$ is not defined when $t \ge \lambda$
        \item We get $M'_X(t) = \frac{\lambda}{(\lambda - t)^2}$ and $M_X''(t) = \frac{2\lambda}{(\lambda - t)^3}$, so $E[X] = M_X'(0) = \frac{1}{\lambda}$ and $E[X^2] = M_X''(0) = \frac{2}{\lambda^2}$, and $Var(X) = \frac{2}{\lambda^2} - (\frac{1}{\lambda})^2 = \frac{1}{\lambda^2}$
    \end{itemize}
\subsection{Slide 5 (Midterm Review)}
    \item Outcomes, events, and probability
    \begin{itemize}
        \item Sample space $S$ is the set of all possible outcomes $s \in S$
        \item Event $A$ is a collection of outcomes
        \item Probability function $P$ is a function from events to real numbers
        \item $P$ satisfies three axioms
        \begin{itemize}
            \item 1. $P(A) \ge 0$ for any event $A$
            \item 2. $P(S) = 1$
            \item 3. If events $A$ and $B$ are mutually exclusive, $P(A \cup B) = P(A) + P(B)$
        \end{itemize}
    \end{itemize}
    \item Counting and calculating probabilities
    \begin{itemize}
        \item Sampling an ordered set with replacement: take ordered $k$ draws from a group of size $n$ options, with replacement, the number of outcomes is $n \times n \times \cdots = n^k$
        \item Sampling an ordered set without replacement (permutation): take ordered $k$ draws from a group of size $n$ options, without replacement $(n \ge k)$, the number of outcomes is $\frac{n!}{(n-k)!}$
        \item Sampling an unordered set without replacement (combination): take unordered $k$ draws from a group of size $n$ options, without replacement $(n \ge k)$, the number of outcomes is $\binom{n}{k} = \frac{n!}{k!(n-k)!}$
    \end{itemize}
    \item Conditional probability, law of total probability, Bayes' rule, and independence
    \begin{itemize}
        \item The probability of $A \mid B$ is $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$
        \item Law of total probability: $A_1,\dots,A_n$ form a partition of the sample space, then $P(B) = \sum_{i=1}^n P(B \mid A_i)P(A_i)$
        \item Bayes' rule: $A_1,\dots,A_n$ form a partition of the sample space, for any event $B$ where $P(B) > 0$, $P(A_j \mid B) = \frac{P(B \mid A_j)P(A_j)}{P(B)} = \frac{P(B \mid A_j)P(A_j)}{\sum_{i=1}^n P(B \mid A_i)P(A_i)}$
        \item Events $A$ and $B$ are independent if $P(A \cap B) = P(A)P(B)$ or $P(A \mid B) = P(A)$ or $P(B \mid A)=P(B)$
        \begin{itemize}
            \item Independence and mutually exclusive: one does not imply the other
            \item Independence of multiple events: any subset of events are independent
        \end{itemize}
    \end{itemize}
    \item Discrete random variables
    \begin{itemize}
        \item Random variables $X$ takes on discrete values $x_1,x_2,\dots$
        \item Pdf (or pmf) $p(x_i) = P(X=x_i)$, where $p(x_i)$ satisfies two conditions:
        \begin{itemize}
            \item $p(x_i) \ge 0$ for any $x_i \in \mathbb{R}$
            \item $\int_{-\infty}^\infty p(x)dx = 1$ (i.e., the sum of the probabilities of all possible values of $X$ is 1)
        \end{itemize}
        \item Cdf $F(x) = P(X \le x)$
        \begin{itemize}
            \item $F(x)$ is a step function: it remains constant in all intervals between possible values of $X$
            \item At a possible value $x_i$, $F(x)$ jumps by the amount $p(x_i)=P(X=x_i)$
        \end{itemize}
    \end{itemize}
    \item Continuous random variables
    \begin{itemize}
        \item Random variables $X$ takes on any values in some interval of the real line
        \item Pdf $f(x)$ is defined such that for any interval $[a,b] \subset S, P(X \in [a,b]) = \int_a^b f(x)dx$ satisfies two conditions:
        \begin{itemize}
            \item $f(x) \ge 0$ for all $x \in \mathbb{R}$
            \item $\int_{-\infty}^\infty f(x)dx = 1$
        \end{itemize}
        \item Cdf $F(x) = P(X \le x)$
        \begin{itemize}
            \item $F(x) = \int_{-\infty}^x f(x)dx$
            \item $F'(x) = \frac{d}{dx}F(x)=f(x)$
        \end{itemize}
    \end{itemize}
    \item Expectation
    \begin{itemize}
        \item Discrete $X$: $E[X] = \sum_{i=1}^n x_i \cdot p(x_i)$
        \item Continuous $X$: $E[X] = \int_{-\infty}^\infty x \cdot f(x)dx$
        \item Expectation of functions of $X$
        \begin{itemize}
            \item If $X$ is discrete, the expected value of the random variable $g(X)$ is given by $E[g(X)] = \sum_{i=1}^n g(x_i) p(x_i)$ if $\sum_{x_i} \vert g(x_i) \vert p(x_i) < \infty$
            \item If $X$ is continuous and if $g(x)$ is a continuous function, then the expected value of $g(x)$ is $\int_{-\infty}^\infty g(x)f(x)dx$ if $\int_{-\infty}^\infty \vert g(x) \vert f(x) dx < \infty$
            \item Linear function of $X$: $E[aX+b] = aE[X]+b$
        \end{itemize}
    \end{itemize}
    \item Variance
    \begin{itemize}
        \item Discrete $X$ with pdf $p(x)$ and expectation $\mu$ is $Var(X) = E[(X-\mu)^2] = \sum_{x_i} (x_i -\mu)^2 p(x_i)$
        \item Continuous $X$ with pdf $f(x)$ and expectation $\mu$ is $Var(X) = E[(X - \mu)^2] = \int_{-\infty}^\infty (x-\mu)^2f(x)dx$
        \item For discrete or continuous $X$, its variance can be written as $Var(X) = E[X^2] - E[X]^2$
        \item Variance of linear functions of $X$: $Var(aX+b) = a^2Var(X)$
    \end{itemize}
\subsection{Problem Set 4 and Midterm Review}
    \item 1. Suppose you buy a laptop for 1,200 pounds which comes with a limited warranty for the first year. During that first year, there is a probability p = 10\% that you spill a cup of coffee over the laptop (or have a similar accident, which is all your own fault) and have to replace the entire motherboard, which will cost 1100 pounds. This repair is not covered by the limited warranty, but you may purchase an extended service plan for 115 pounds.
    \begin{itemize}
        \item (a) Should you buy this additional service plan if you only care about monetary value?
        \begin{itemize}
            \item Let $C$ be the monetary value of your spending. If you don't buy: $E[C] = (1200+1100)(0.1) + (1200)(0.9) = 1310$. If you buy, you spend $1200+115=1315$ for sure. Since $1310 < 1315$, you would not buy the additional service plan.
        \end{itemize}
        \item (b) Economists often model individual’s decision making using utility functions. Suppose your utility function depends on the cost you spend in the following way: $u(c) = \sqrt{4800 - c}$. Based on the expected value of your utility, should you buy the additional insurance?
        \begin{itemize}
            \item Now we will find $E[u(C)]$. If you don't buy: $E[u(C)] = (\sqrt{4800-(1200+1100)})(0.1) + (\sqrt{4800-1200})(0.9) = (50)(0.1) + (60)(0.9) = 59 = \sqrt{3481}$. If you buy, $E[u(C)] = \sqrt{4800-(1200+115)} = \sqrt{3485}$ for sure. Since $\sqrt{3485} > \sqrt{3481}$, you will buy the additional insurance.
        \end{itemize}
    \end{itemize}
    \item 2. A random variable X is said to follow a Pareto distribution if its pdf is
    \[
    f(x) = \left\{ \begin{array}{cl}
        \frac{\alpha k^\alpha}{x^{\alpha+1}} & \text{for } x \ge k \\
        0 & \text{otherwise} 
    \end{array} \right.
    \]
    where $\alpha > 0$ is a parameter, and $k > 0$ is a known number which is the smallest possible value of $X$. Pareto distribution is often used in practice to model the distribution of insurance claims, personal income, etc., where k is the deductible amount for an insurance policy or some low income benefits or universal credits.
    \begin{itemize}
        \item (a) Check the pdf is valid
        \begin{itemize}
            \item Since $\alpha > 0$, $k > 0$, and $x \ge k > 0$, $f(x) \ge 0$ for all $x$. Next, we check:
            \begin{flalign*}
                \int_{-\infty}^\infty f(x)dx &= \int_k^\infty \frac{\alpha k^\alpha}{x^{\alpha+1}} dx \\
                &= \alpha k^\alpha \int_k^\infty x^{-(\alpha + 1)} dx \\
                &= \alpha k^\alpha [\frac{-1}{\alpha} x^{-\alpha}]_k^\infty dx \\
                &= \alpha k^\alpha\frac{-1}{\alpha} [ x^{-\alpha}]_k^\infty dx \\
                &= -k^\alpha [0 - k^{-\alpha}] dx = 1\\
            \end{flalign*}
        \end{itemize}
        \item (b) Derive the cdf of $X$
        \begin{flalign*}
            F(x) &= \int_{-\infty}^x f(t)dt \\
            &= \int_{k}^x \frac{\alpha k^\alpha}{t^{\alpha+1}} dt \\
            &= \alpha k^\alpha [\frac{-1}{\alpha} t^{-\alpha}]_k^x \\
            &= -k^\alpha [x^{-\alpha} - k^{-\alpha}] \\
            &= 1-\left(\frac{k}{x}\right)^\alpha, \text{ for } x \ge k, F(x) = 0 \text{ for } x<k
        \end{flalign*}
        \item (c) Calculate the expected value and variance of $X$. When are they finite?
        \begin{itemize}
            \item Expected value of $X$:
            \begin{flalign*}
                E[X] &= \int_{-\infty}^\infty xf(x)dx \\
                &= \int_k^\infty x\frac{\alpha k^\alpha}{x^{\alpha+1}} dx \\
                &= \int_k^\infty \frac{\alpha k^\alpha}{x^{\alpha}} dx \\
                &= \frac{\alpha k}{\alpha - 1} \underset{=1}{\underbrace{\int_k^\infty \frac{(\alpha-1) k^{\alpha -1}}{x^{(\alpha-1)+1}} dx}} \\
                &= \frac{\alpha k}{\alpha -1}
            \end{flalign*}
            for $\alpha > 1$. The integral only converges when $\alpha > 1$, i.e. $E[X] = \infty$ if $\alpha \le 1$
            \item Variance of $X$
            \begin{flalign*}
                E[X^2] &= \int_{-\infty}^\infty x^2f(x)dx \\
                &= \int_k^\infty x^2\frac{\alpha k^\alpha}{x^{\alpha+1}} dx \\
                &= \int_k^\infty \frac{\alpha k^\alpha}{x^{\alpha-1}} dx \\
                &= \frac{\alpha k^2}{\alpha - 2} \underset{=1}{\underbrace{\int_k^\infty \frac{(\alpha-2) k^{\alpha -2}}{x^{(\alpha-2)+1}} dx}} \\
                &= \frac{\alpha k^2}{\alpha -2}
            \end{flalign*}
            for $\alpha > 2$. Therefore, $Var(X) = \frac{\alpha k^2}{\alpha -2} - \left( \frac{\alpha k}{\alpha -1}\right)^2 = \frac{\alpha k^2}{(\alpha -1)^2(\alpha-2)}$ for $\alpha > 2$ and $Var(X) = \infty$ for $1 < \alpha \le 2$
        \end{itemize}
    \end{itemize}
    \item 3.  Consider the Monty Hall problem, except that Monty enjoys opening Door 2 more than he enjoys opening Door 3, and if he has a choice between opening these two doors, he opens Door 2 with probability $p$, where $1/2 \le p \le 1$. To recap: there are three doors, behind one of which there is a car (which you want), and behind the other two of which there are goats (which you do not want). Initially, all possibilities are equally likely for where the car is. You choose a door, which for concreteness we assume is Door 1. Monty Hall then opens a door to reveal a goat, and offers you the option of switching. Assume that Monty Hall knows which door has the car, will always open a goat door and offer the option of switching, and as above assume that if Monty Hall has a choice between opening Door 2 and Door 3, he chooses Door 2 with probability $p$ (with $1/2 \le p \le 1$).
    \begin{itemize}
        \item (a) Find the unconditional probability that the strategy of always switching succeeds (unconditional in the sense that we do not condition on which of Doors 2,3 Monty opens)
        \begin{itemize}
            \item Let $C_j$ be the event that the car is hidden behind door $j$ and let $W$ be the event that we win using the switching strategy. Using the law of total probability, we can find the unconditional probability of winning like we did in class:
            \begin{flalign*}
                P(W) &= P(W \mid C_1)P(C_1) + P(W \mid C_2)P(C_2) + P(W \mid C_3)P(C_3) \\
                &= 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3} = \frac{2}{3}
            \end{flalign*}
        \end{itemize}
        \item (b) Find the probability that the strategy of always switching succeeds, given that Monty opens Door 2
        \begin{itemize}
            \item Let $M_i$ be the event that Monty opens Door $i$. Note that we are looking for $P(W \mid M_2)$, which is the same as $P(C_3 \mid M_2)$ as we first choose Door 1 and then switch to Door 3. Similar to the analysis we discussed in class, by Bayes' rule,
            \begin{flalign*}
                P(C_3 \mid M_2) &= \frac{P(M_2 \mid C_3)P(C_3)}{P(M_2)} \\
                &= \frac{P(M_2 \mid C_3)P(C_3)}{P(M_2 \mid C_1)P(C_1) + P(M_2 \mid C_2)P(C_2) + P(M_2 \mid C_3)P(C_3)} \\
                &= \frac{1 \cdot \frac{1}{3}}{p \cdot \frac{1}{3} + 0 \cdot \frac{1}{3} + 1 \cdot \frac{1}{3}} = \frac{1}{1+p}
            \end{flalign*}
        \end{itemize}
        \item (c) Find the probability that the strategy of always switching succeeds, given that Monty opens Door 3
        \begin{itemize}
            \item The structure of the problem is the same as part (b). Imagine repainting doors 2 and 3, reversing which is called which. By part (b) with $1-p$ in place of $p$, we get:
            \[
            P(C_2 \mid M_3) = \frac{1}{1+(1-p)} = \frac{1}{2-p}
            \]
        \end{itemize}
    \end{itemize}
    \item 4. Suppose that a deck of 52 cards containing four aces is shuffled thoroughly, and the cards are then distributed among four players so that each player receives 13 cards. What is the probability that each player will receive one ace?
    \begin{itemize}
        \item Solve through combinations:
        \begin{itemize}
            \item Number of possible combinations of four positions in deck occupied by four aces: $\binom{52}{4}$
            \item There must be exactly one ace out of 13 cards for each player
            \begin{itemize}
                \item 13 possible positions for ace for first player, 13 possible possitions for second player, etc.
            \end{itemize}
            \item Therefore, there are exactly $13^4$ possible combinations of each player receiving one ace
            \item The probability that each player will receive one ace is $p=\frac{13^4}{\binom{52}{4}}=0.1055$
            \begin{itemize}
                \item Works because we didn't specify which order the combination of aces are in nor the positions that the aces can occupy
            \end{itemize}
        \end{itemize}
        \item Solve through permutations:
        \begin{itemize}
            \item There are $52!$ possible outcomes considering ordering
            \item As before, there are $13^4$ ways to choose four positions for four aces
            \item Now there are $4!$ ways to arrange the aces in each position
            \item For the 48 other cards, there are $48!$ ways to arrange them
            \item $p=\frac{13^4 \times 4! \times 48!}{52!}=0.1055$
        \end{itemize}
    \end{itemize}
    \item 5. A venture capital fund has a portfolio of 15 startups, categorized as follows: 6 are in FinTech (F), 5 are in BioTech (B), 4 are in CleanTeach (C). The manager decides to randomly select 4 startups to review. What is the probability that the selected group contains at least 2 FinTech (F) startups?
    \begin{itemize}
        \item Total number of combinations: $\binom{15}{4} = 1365$
        \item Case 1: Exactly 2 FinTech startups
        \begin{itemize}
            \item $\binom{6}{2} \binom{9}{2} = 540$
        \end{itemize}
        \item Case 2: Exactly 3 FinTech startups
        \begin{itemize}
            \item $\binom{6}{3}\binom{9}{1} = 180$
        \end{itemize}
        \item Case 3: Exactly 4 FinTech startups
        \begin{itemize}
            \item $\binom{6}{4} = 15$
        \end{itemize}
        \item $P(\text{at least }2F) = \frac{540+180+15}{1365} = \frac{7}{13}$
    \end{itemize}
\end{itemize}

\section{Tutorial N/A (7/11/2025)}
\begin{itemize}
    \item Did not have tutorial (midterm exam)
\end{itemize}

\section{Lecture 6 (10/11/2025)}
\begin{itemize}
\subsection{Slide 6}
    \item Motivation for multivariate random variables
    \begin{itemize}
        \item Treat each observation in data as a random variable, then the size of the sample is the length of the multivariate random variable of interest
        \item Often interested in relationship between two or more variables
        \item Variable of interest might be transformations of other random variables
    \end{itemize}
    \item Joint Probability Density (Mass) Function - for random variables $X,Y$, $p_{X,Y}(x,y)=P(s \mid X(s) = x, Y(s) = y) = P(X=x, Y=y)$
    \item Similar to univariate pdfs, $p(x,y)$ satisfies two conditions
    \begin{itemize}
        \item $p(x,y) \ge 0$
        \item $\sum_x \sum_y p(x,y) = 1$
    \end{itemize}
    \item For multivariate random variables $X_1, X_2, \cdots , X_n$, the join pdf $p(x_1,x_2, \cdots, x_n) = P(X_1=x_1, X_2=x_2, \cdots, X_n=x_n)$
    \item Marginal PDF - Suppose that $p(x,y)$ is the joint pdf of the discrete random variables $X,Y$, then $p(x) = \sum_y p(x,y), p(y) = \sum_x p(x,y)$
    \begin{itemize}
        \item By summing over the other variable, we are ignoring it
    \end{itemize}
    \item Example: A supermarket has two express check-out lines. Let $X$ and $Y$ denote the number of customers in the first and in the second, respectively, at any given time. The joint pdf of $X$ and $Y$ is summarized by the following table:
    \[ \begin{array}{cc|cccc}
         & & & & X & \\
         & & 0 & 1 & 2 & 3 \\
        \hline
         & 0 & 0.1 & 0.2 & 0 & 0 \\
         & 1 & 0.2 & 0.25 & 0.05 & 0 \\
        Y & 2 & 0 & 0.05 & 0.05 & 0.025 \\
         & 3 & 0 & 0 & 0.025 & 0.05 
    \end{array}\]
    \begin{itemize}
        \item What is the probability that the two lines are of the same length?
        \begin{itemize}
            \item $p(x=y)=p(0,0)+p(1,1)+p(2,2)+p(3,3) = 0.1 + 0.25 + 0.05 + 0.05 = 0.45$
        \end{itemize}
        \item What is the probability that the lengths of the two lines differ by 1?
        \begin{itemize}
            \item $p(\vert x - y \vert = 1)=p(0,1)+p(1,0)+p(1,2)+p(2,1)+p(2,3)+p(3,2) = 0.2 + 0.2 + 0.05 + 0.05 + 0.025 + 0.025 = 0.55$
        \end{itemize}
        \item What is the probability $X \ge 2$?
        \begin{itemize}
            \item $P(X \ge 2) = \sum_{x \ge 2} \sum_y p(x,y) = 0.05+0.05+0.025+0.025+0.05 = 0.2$
        \end{itemize}
    \end{itemize}
    \item Joint PDF (Continuous) - $f(x,y) = P[(X,Y) \in \mathbb{R}]  = \int_R \int f(x,y) dxdy$
    \begin{itemize}
        \item $f(x,y) \ge 0$
        \item $\int_{-\infty}^\infty \int_{-\infty} ^\infty f(x,y) dxdy = 1$
    \end{itemize}
    \item Suppose the joint pdf of two continuous random variables $X,Y$ can be written as $f(x,y)=cxy, 0 < y < x < 1$
    \begin{itemize}
        \item $\int_{-\infty}^\infty \int_{-\infty} ^\infty f(x,y) dxdy =  \int_0^1 \int_0^x cxydydx = \int_0^1 \int_y^1 cxy dxdy = c \int_0^1 y[\frac{x^2}{2}]_y^1 dy = c \int_0^1 \frac{1}{2} y - \frac{y^3}{2} dy = c [\frac{y^2}{4} - \frac{y^4}{8}]_0^1 = [\frac{1}{4}-\frac{1}{8}] c = 1 \Rightarrow c = 8$
    \end{itemize}
    \item Marginal PDF (Continuous) - for continuous random variables $X,Y$, $f(x) = \int_{-\infty}^\infty f(x,y)dy, f(y) = \int_{-\infty}^\infty f(x,y)dx$
    \item Suppose two continuous random variables, $X,Y$ with joint uniform pdf $f(x,y)=\frac{1}{6}, 0 \le x \le 3, 0 \le y \le 2$. What is marginal pdf $f(x)$?
    \begin{itemize}
        \item $f(x)=\int_{-\infty}^\infty f(x,y)dy = \int_0^2 \frac{1}{6} dy = \left[ \frac{1}{6} y \right]_0^2 = \frac{1}{3}$ for $0 \le x \le 3$
    \end{itemize}
    \item Joint Cumulative Distribution Function (CDF) - for random variables $X,Y$, $F(x,y) = P(X \le x, Y \le y)$
    \item Conditional Probability Density Function (conditional pdf) - probability that $Y$ takes on the value $y$ given that $X=x$ is $p_{Y \mid X} (y) = P(Y=y,X=x) = \frac{p_{X,Y}(x,y)}{p_X(x)}, p_X(x) > 0$
    \item Conditional PDF (Continuous) - $f_{Y \mid X}(y) = \frac{f_{X,Y}(x,y)}
    {f_X(x)}, f_X(x) > 0$
    \item Conditional pdf properties
    \begin{itemize}
        \item $f_{Y \mid X}(y) \ge 0, p_{Y \mid X}(y) \ge 0$\
        \item $\int_{-\infty}^\infty f_{Y \mid X}(y) dy = 1, \sum_y p_{Y \mid X}(y) = 1$
    \end{itemize}
    \item Let $X$ and $Y$ be continuous random variables with joint pdf $f_{X,Y}(x,y) = \left\{ \begin{array}{cll}
        \frac{1}{8} (6-x-y), & 0 \le x \le 2, & 2 \le y \le 4 \\
        0, & \text{elsewhere}
    \end{array} \right. $. What is $P(2 < Y < 3 \mid X = 1)$?
    \begin{itemize}
        \item $P(2 < Y < 3 \mid X = 1) = \int_2^3 \frac{f_{X,Y}(1,y)}{f_X(1,\cdot)} dy = \int_2^3 \frac{\frac{1}{8}(6-1-y)}{\int_2^4 \frac{1}{8} (6-1-y)dy}dy = \int_2^3 \frac{\frac{1}{8}(5-y)}{\frac{1}{8} [5y-\frac{y^2}{2}]_2^4}dy = \frac{1}{4} \int_2^3 (5-y)dy = \frac{1}{4} [5y-\frac{y^2}{2}]_2^3 = \frac{1}{4} [\frac{5}{2}] = \frac{5}{8}$
        \item Another way (expanding out each individual function): 
        \begin{flalign*}
            f_{Y \mid X}(y) &= \frac{f_{X,Y}(x,y)}{f_X(x)}=\frac{\frac{1}{8}(6-x-y)}{\frac{1}{8}(6-2x)}= \frac{6-x-y}{6-2x}, 0 \le x \le 2, 2 \le y \le 4 \\
            f_X(x) &= \int_2^4 \frac{1}{8} (6-x-y)dy = \frac{1}{8} \left[ 6y-xy-\frac{y^2}{2} \right]_2^4 = \frac{1}{8} (6-2x), 0 \le x \le 2 \\
            \text{When } x=1, \ &f_{Y \mid X} (y) = \frac{5-y}{4} \\
            P(2 < Y < 3 \mid X = 1) &= \int_2^3 \frac{5-y}{4} dy = \frac{5}{8}
        \end{flalign*}
    \end{itemize}
    \item Independence of Random Variables - for discrete random variables $X,Y$, $p_{X,Y}(x,y)=p_X(x)p_Y(y)$ or $p_{Y \mid X}(y)=p_Y(y)$ or $p_{X \mid Y} (x) = p_X(x),  \forall x,y,p_X(x) > 0$
    \begin{itemize}
        \item For discrete random variables $X_1,X_2,\dots,X_n$, they are independent if $p(x_1,x_2, \cdots, x_n) = p(x_1)p(x_2) \cdots p(x_n)$
        \item For continuous case, replace $p(\cdot)$ with $f(\cdot)$ in the above definitions
    \end{itemize}
    \item Independent and Identically Distributed (i.i.d.) sample - set of $n$ independent random variables, all having the same pdf
    \begin{itemize}
        \item Said to also be a random sample of size $n$
    \end{itemize}
    \item Coin is tossed $n$ times
    \begin{itemize}
        \item Assume the tosses are independent
        \item The probability of head on each toss is $p$
        \item For $i \in \{1,\dots, n\},$ let $X_i$ be a random variable that equals 1 if the $i$th coin toss yields a head, and $0$ otherwise
        \item $X_i \sim Bernoulli(p)$ for each $i$
        \item $X_1,X_2,\dots, X_n$ are i.i.d.
        \item We are interested in the total number of heads in $n$ tosses, $Y = \sum_{i=1}^n X_i$, which is itself a discrete random variable with possible values $Y = \{0,1, \cdots, n\}$
        \item Since $x_1, \dots, x_n \in \{0,1\}$ and $\sum_{i=1}^n x_i = y, x_1, \dots, x_n$ is one sequence that contains exactly $y$ ones and $n-y$ zeros
        \item By independence, the probability of observing this one sequence is $P(X_1=x_1, X_2=x_2, \cdots, X_n = x_n) = P(X_1=x_1)P(X_2=x_2) \cdots P(X_n = x_n)$
        \item Since $P(X_i = 1) = p$ and $P(X_i = 0) = 1 - p$, $P(X_1=x_1)P(X_2=X_2)\cdots P(X_n=x_n) = p^y (1-p)^{n-y}$
        \item To compute $P(Y=y)=P(\sum_{i=1}^n X_i = y) = P\left( \bigcup_{\sum_{i=1}^n X_i=y} \{X_1=x_1, X_2=x_2, \cdots, X_n=x_n\} \right)$
        \item Since the different ways of observing $y$ heads in $n$ tosses are mutually exclusive, we can turn the multiplication into a sum:
        $P(Y=y)=\sum_{\sum_{i=1}^n X_i =y} P(X_1=x_1, X_2=x_2, \cdots, X_n=x_n) = \sum_{\sum_{i=1}^n X_i = y} p^y (1-p)^{n-y}$
        \item There are $\binom{n}{y}$ different ways of observing $y$ heads, so $P(Y=y)=\binom{n}{y} p^y (1-p)^{n-y}$
        \item $Y$ follows a binomial distribution with parameters $n$ and $p$ ($Y \sim Binom(n,p)$)
    \end{itemize}
\end{itemize}

\section{Lecture 7 (13/11/2025)}
\begin{itemize}
\subsection{Slide 7}
    \item For functions of multivariate random variables, their expectations have very similar expressions to the univariate random variable case
    \begin{itemize}
        \item $X$ and $Y$ are discrete random variables with joint pdf $p_{X,Y}(x,y)$ and $g(X,Y)$ is a function of $X$ and $Y$
        \[
        E[g(X,Y)] = \sum_x \sum_y g(x,y) p_{X,Y}(x,y)
        \]
        where $\sum_x \sum_y \vert g(x,y) \vert p_{X,Y}(x,y) < \infty$
        \item $X$ and $Y$ are continuous random variables with joint pdf $p_{X,Y}(x,y)$ and $g(X,Y)$ is a function of $X$ and $Y$
        \[
        E[g(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y) f_{X,Y}(x,y) dxdy
        \]
        where $\int_{-\infty}^\infty \int_{-\infty}^\infty \vert g(x,y) \vert f_{X,Y}(x,y) dxdy < \infty$
    \end{itemize}
    \item A supermarket has two express check-out lines. Let $X$ and $Y$ denote the number of customers in the first and in the second, respectively, at any given time. The joint pdf of $X$ and $Y$ is summarized by the following table:
    \[
    \begin{array}{cc|cccc}
         & & & & X & \\
         & & 0 & 1 & 2 & 3 \\
         \hline
         Y & 0 & 0.1 & 0.2 & 0 & 0 \\
         & 1 & 0.2 & 0.3 & 0.125 & 0.075
    \end{array}
    \]
    \begin{itemize}
        \item What is the expected value of $g(X,Y) = 3X - 2XY + Y$?
        \begin{flalign*}
            E[g(X,Y)] &= \sum_{x=0}^3 \sum_{y=0}^1 (3x - 2xy + y) p_{X,Y}(x,y) \\
            &= 0 + 0.2 \cdot 3 + 0 + 0 + 0.2 \cdot 1 + 0.3 \cdot 2 + 0.125 \cdot 3 + 0.075 \cdot 4 \\
            &= 0.6 + 0.2 + 0.6 + 0.375 + 0.3 \\
            &= 2.075
        \end{flalign*}
    \end{itemize}
    \item Let $X$ and $Y$ be any two random variables, discrete or continuous, dependent or indpeendent, and let $a$ and $b$ be two constants. Then
    \[
    E[aX+bY] = aE[X] + bE[Y]
    \]
    where $E[X]$ and $E[Y]$ are both finite
    \[
    E[\sum_{i=1}^n a_iX_i] = \sum_{i=1}^n a_i E[X_i]
    \]
    \item Prove this for the continuous case:
    \begin{flalign*}
        E[aX+bY] &= \int_{-\infty}^\infty \int_{-\infty}^\infty (ax+by)f_{X,Y}(x,y)dxdy \\
        &= a \int_{-\infty}^\infty \int_{-\infty}^\infty xf_{X,Y}(x,y)dxdy + b \int_{-\infty}^\infty \int_{-\infty}^\infty yf_{X,Y}(x,y)dxdy \\
        &= a \int_{-\infty}^\infty x \left[ \int_{-\infty}^\infty f_{X,Y}(x,y)dy \right] dx + b \int_{-\infty}^\infty y \left[ \int_{-\infty}^\infty f_{X,Y}(x,y)dx \right] dy \\
        &= a \int_{-\infty}^\infty xf_{X}(x) dx + b \int_{-\infty}^\infty yf_{Y}(y)dy \\
        &= a E[X] + bE[Y]
    \end{flalign*}
    \item Revisiting binomial example: $Y = \sum_{i=1}^n X_i$, where $X_1, X_2, \dots, X_n$ and i.i.d., $X_i \sim Bernoulli(p)$
    \begin{itemize}
        \item We know $E[X_i] = 1 \cdot p + 0 \cdot (1-p) = p$
        \item Since $X_i$s are identically distributed,
        \[
        E[Y] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = np
        \]
        \item Note we did not use the independent condition
    \end{itemize}
    \item Unlike the sum of random variables, there is no general formula for computing the expectation of the product of random variables (apart from treating it as a function of random variables)
    \begin{itemize}
        \item If $X,Y$ are independent,
        \[
        E[XY] = E[X]E[Y]
        \]
        where $E[X]$ and $E[Y]$ are finite
        \item More generally, if $X,Y$ are independent,
        \[
        E[g(X)h(Y)] = E[g(X)] \cdot E[h(Y)]
        \]
    \end{itemize}
    \begin{flalign*}
        E[XY] &= \int_{-\infty}^\infty \int_{-\infty}^\infty xyf_{X,Y}(x,y) dxdy \\
        &= \int_{-\infty}^\infty \int_{-\infty}^\infty xyf_X(x)f_Y(y) dxdy \quad f_{X,Y}(x,y) = f_X(x)\cdot f_Y(y) \leftarrow \text{Independence Condition} \\
        &= \int_{-\infty}^\infty xf_X(x) \left[ \int_{-\infty}^\infty yf_Y(y) dy \right] dx \\
        &= \int_{-\infty}^\infty xf_X(x) E[Y] dx \\
        &= E[Y] \int_{-\infty}^\infty xf_X(x) dx \\
        &= E[X] \cdot E[Y]
    \end{flalign*}
    \item We need a measure of the relationship between two random variables
    \begin{itemize}
        \item If two random variables are not independent, we would like to measure how strong their association is
        \item Covariance and correlation is one way to measure association and is also a key part of the motivation and interpretation of regression analysis and many other statistical analyses
        \item $X$ and $Y$ are random variables with finite variances, the covariance of $X$ and $Y$ is
        \[
        Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]
        \]
    \end{itemize}
    \begin{flalign*}
        E[(X-E[X])(Y-E[Y])] &= E[XY - E[X] \cdot Y - E[Y] \cdot X + E[X] \cdot E[Y]] \\
        &= E[XY] - E[X]E[Y] - E[Y]E[X] + E[X]E[Y] \\
        &= E[XY] - E[X]E[Y]
    \end{flalign*}
    \item Properties of Covariance
    \begin{itemize}
        \item If $X$ is a random variable and $a$ is a constant,
        \[
        Cov(a,X) = E[aX] - E[a]E[X] = aE[X] - aE[X] = 0
        \]
        \item If $X,Y$ are random variables,
        \[
        Cov(X,Y) = Cov(Y,X)
        \]
        \item If $X,Y,Z$ are random variables, and $a,b$ are constants,
        \[
        Cov(X,aY+
        bZ) = aCov(X,Y) + bCov(X,Z)
        \]
        \begin{flalign*}
            Cov(X,aY+bZ) &= E[X(aY+bZ)] - E[X]E[aY+bZ] \\
            &= aE[XY] + bE[XZ] - aE[X]E[Y] - bE[X]E[Z] \\
            &= a Cov(X,Y) + bCov(X,Z)
        \end{flalign*}
        \begin{itemize}
            \item This property reveals one disadvantage of covariance measure: it is not "scale-free"
        \end{itemize}
        \item If $X,Y$ are independent, $Cov(X,Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0$
        \item Note $Cov(X,Y) = 0 \centernot \Longleftrightarrow X,Y$ are independent
        \begin{itemize}
            \item Example: $Y = X^2, X \sim U(-1,1) \Rightarrow E[XY] = E[X] = 0 \Rightarrow Cov(X,Y) = 0 $
            \item $X$ and $Y$ are not independent
        \end{itemize}
    \end{itemize}
    \item For random variables $X$ and $Y$, their correlation is defined as
    \[
    Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
    \]
    \begin{itemize}
        \item We can see from the above equation that correlation is the "scale-free" version of covariance: it is covariance scaled by the standard deviation of $X$ and the standard deviation of $Y$ (always between +1 and -1)
        \begin{flalign*}
            Corr(aX+b,cY+d) &= \frac{Cov(aX+b,cY+d)}{\sqrt{Var(aX+b)}\sqrt{Var(cY+d})} \\
            &= \frac{acCov(X,Y)}{a\sigma_{X} c \sigma_{Y}} \\
            &=\frac{Cov(X,Y)}{\sigma_X \sigma_Y} \\
            &= Corr(X,Y)
        \end{flalign*}
    \end{itemize}
    \item Correlation and covariance only measures linear associations of $X$ and $Y$
    \begin{itemize}
        \item If $Y=aX+b$, $Corr(X,Y) = \left\{ \begin{array}{cc}
            -1 & a>0 \\
            1 & a<0
        \end{array} \right.$
    \end{itemize}
    \item For univariate random variables, $Var(aX)=a^2Var(X)$
    \item $X$ and $Y$ are random variables with finite variances,
    \begin{flalign*}
        Var(aX+bY) &= E[(aX+bY)^2] - E[aX+bY]^2 \\
        &= E[a^2X^2 + b^2Y^2 + 2abXY] - (aE[X] + bE[Y])^2 \\
        &= a^2E[X^2] + b^2 E[Y^2] + 2abE[XY] - a^2E[X]^2 - b^2E[Y]^2 -2abE[X]E[Y] \\
        &= a^2(E[X^2] - E[X]^2) + b^2(E[Y^2] - E[Y]^2) +2ab(E[XY] - E[X]E[Y]) \\
        &= a^2 Var(X) + b^2 Var(Y) + 2abCov(X,Y)
    \end{flalign*}
    \begin{itemize}
        \item When variables move together (positive covariance), total variance increases
        \item When variables offset each other (negative covariance), total variance decreases
    \end{itemize}
    \item If $X_1,X_2, \dots, X_n$ are random variables with finite variances
    \[
    Var\left( \sum_{i=1}^n a_iX_i \right) = \sum_{i=1}^n a_i^2Var(X_i) + 2\sum_{i<j} a_ia_jCov(X_i,X_j)
    \]
    \item If $X_1,X_2,\dots,X_n$ are independent random variables with finite variances, $Var \left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)$
    \item Revisiting our binomial example: $Y = \sum_{i=1}^n X_i$, where $X_1,X_2, \dots, X_n$ are i.i.d., $X_i \sim Bernoulli(p)$
    \begin{itemize}
        \item We know $E[X_i] = 1 \cdot p + 0 \cdot (1-p) = p$
        \item We can compute $E[X_i^2] = 1 \cdot p + 0 \cdot (1-p)$, so $Var(X_i) = E(X_i^2) - [E(X_i)]^2 = p - p^2 = p(1-p)$
        \item Since $X_1,X_2, \dots, X_n$ are i.i.d.,
        \[
        Var(Y) = Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) = np(1-p)
        \]
        \item If the variables are only independent and not identically distributed,
        \[
        Var(Y) = Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) = \sum_{i=1}^n p_i(1-p_i)
        \]
    \end{itemize}
\end{itemize}

\section{Tutorial 5 (14/11/2025)}
\begin{itemize}
    \item Tips:
    \begin{itemize}
        \item Understand the logic
        \item Recognize particular patterns and use them
        \item Understanding and processing the question is time well spent
        \begin{itemize}
            \item Like going to war without knowing who you are fighting
        \end{itemize}
    \end{itemize}
\subsection{Midterm Exam}
    \item 1. Ten fair dice are rolled: four blue and six yellow. Write down numerical expression for the following probabilities. If your answer involves factorials, binomial coefficients, or fractions ($n!$, $\binom{n}{k}$, or $\frac{k}{n}$), you do not need to calculate them, simply leave them in your answer.
    \begin{itemize}
        \item (a) The probability that exactly one blue die shows a six, and exactly two yellow dice show even numbers. \textit{(10 points)}: $p(A) = \binom{4}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)^3 \binom{6}{2} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^4$
        \begin{itemize}
            \item Probability that two die are rolled and first shows a six while the second doesn't: $\left( \frac{1}{6} \right) \left( \frac{5}{6} \right)$
            \item Probability that two die are rolled and second shows a six while the first doesn't: $\left( \frac{5}{6} \right) \left( \frac{1}{6} \right)$
            \item Probability that two die are rolled and one shows a six: $\binom{2}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)$
            \item Probability that any one of four blue die rolled shows a six:
            $\binom{4}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)^3$
            \item Probability that two die are rolled and first shows a even number and the second doesn't:
            $\left( \frac{1}{2} \right) \left( \frac{1}{2} \right)$
            \item Probability that two die are rolled and one shows a even number: $\binom{2}{1} \left( \frac{1}{2} \right) \left( \frac{1}{2} \right)$
            \item Probability that exactly two yellow die show even numbers: $\binom{6}{2} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^4$
        \end{itemize}
        \item (b) The probability that exactly three of the ten dice show a six, regardless of color. \textit{(10 points)}: $\binom{10}{3} \left( \frac{1}{6} \right)^3 \left( \frac{5}{6} \right)^7$
    \end{itemize}
    \item 2. A box contains 998 black balls and 2 white balls. Let $X$ be the number of white balls in 500 random draws with replacement from this box. Calculate the probability that $X$ equals to 1 given we already know $X$ is either 1 or 2. \textit{(15 points)}
    \begin{itemize}
        \item White ball is a success (counting total number of sucessess)
        \item $p=\frac{2}{1000}$
        \item $n=500$
        \item $X \sim Binom(n,p)$
        \item Apply Binomial: $P(X=1) = \binom{500}{1} \left( \frac{2}{1000} \right)^1 \left( \frac{998}{1000} \right)^{2-1}$
        \item $P(X=2) = \binom{500}{2} \left( \frac{2}{1000} \right)^2 \left( \frac{998}{1000} \right)^{2-2}$
        \item $P(X=1 \mid X=1 \cup X=2) = \frac{P(X=1 \cap \left( X=1 \cup X=2 \right))}{P(X=1 \cup X=2)} = \frac{P(X=1)}{P(X=1)+P(X=2)}$
        \begin{itemize}
            \item If $A \subseteq B$, then $P(A \cap B) = P(A)$
        \end{itemize}
        \begin{itemize}
            \item $\frac{P(X=1)}{P(X=2)} = \frac{\binom{n}{1} p (1-p)^{n-1}}{\binom{n}{2} p^2 (1-p)^{n-2}}= \frac{\frac{n!}{(n-1)!} (1-p)}{\frac{n!}{(n-2)!2!} p} = \frac{2(1-p)}{(n-1)p}$
        \end{itemize}
        \item $\frac{\frac{P(X=1)}{P(X=2)}}{\frac{P(X=1)}{{(X=2)}} + 1} = \frac{\frac{2(1-p)}{(n-1)p}}{\frac{2(1-p)}{(n-1)p}+1} = \frac{\frac{2}{499}\frac{\frac{998}{1000}}{\frac{2}{1000}}}{\frac{2}{499}\frac{\frac{998}{1000}}{\frac{2}{1000}} + 1} = \frac{2}{3}$
    \end{itemize}
    \item 3. A random variable $X$ takes value $0,1,2$ with probabilities proportional to $1:2:3$. Compute $E[X^2+1]$. \textit{(15 points)}
    \begin{itemize}
        \item Method 1: Calculate $X^2+1$ first
        \begin{itemize}
            \item $X^2+1= \left\{ \begin{array}{cc}
                0^2+1 = 1 & P(0)=\frac{1}{6} \\
                1^2 + 1 = 2 & P(1) = \frac{1}{3} \\
                2^2 + 1 = 5 & P(2) = \frac{1}{2}
            \end{array} \right.$
            \item $E[X^2 + 1] = \frac{1}{6} \cdot 1 + \frac{1}{3} \cdot 2 + \frac{1}{2} \cdot 5 = \frac{1}{6} + \frac{2}{3} + \frac{5}{2} = \frac{1+4+15}{6} = \frac{20}{6} = \frac{10}{3}$
        \end{itemize}
        \item Method 2: Use properties of expectations
        \begin{itemize}
            \item $X = \left\{ \begin{array}{cc}
                0 & P(0)=\frac{1}{6} \\
                1 & P(1) = \frac{1}{3} \\
                2 & P(2) = \frac{1}{2}
            \end{array} \right.$
            \item $E[X^2 + 1] = E[X^2] + 1 = 0^2 \frac{1}{6} + 1^2 \frac{2}{6} + 2^2 \frac{3}{6} + 1 = \frac{7}{3} + 1 = \frac{10}{3}$
        \end{itemize}
    \end{itemize}
    \item 4. A random variable $X$ has pdf
    \[
    f(x) = \left\{ \begin{array}{rl}
        k(1-x)^2, & 0 \le x \le 1 \\
        0, & \text{otherwise}
    \end{array} \right.
    \]
    \begin{itemize}
        \item (a) Find $k$. \textit{(10 points)}
        \begin{itemize}
            \item By the property of pdfs, we know that $\int_{-\infty}^\infty f(x)dx = \int_0^1 k(1-x)^2 dx = \frac{-1}{3}k [(1-x)^3]_0^1 = \frac{-k}{3}(0-1) = \frac{k}{3} = 1 \Rightarrow k = 3$
        \end{itemize}
        \item (b) Find the cumulative distribution function $F(x)$. \textit{(10 points)}
        \begin{itemize}
            \item $F(x) = \int_{-\infty}^x f(x)dx = \int_0^x 3(1-t)^2dt = [-(1-t)^3]_0^x = -(1-x)^3 + 1$
            \[
            F(x) = \left\{ \begin{array}{rl}
                0, & x < 0 \\
                1-(1-x)^3, & 0 \le x \le 1 \\
                1, & x > 1
            \end{array} \right.
            \]
        \end{itemize}
        \item (c) Compute $E[X]$ and $Var(x)$. \textit{(15 points)}
        \begin{itemize}
            \item $E[X] = \int_{-\infty}^\infty xf(x)dx = \int_0^1 3x(1-x)^3 dx = 3 \int_0^1 (x-2x^2+x^3)dx = 3\left[ \frac{x^2}{2} - \frac{2x^3}{3} + \frac{x^4}{4} \right]_0^1 = 3 \left[\frac{1}{2} - \frac{2}{3} + \frac{1}{4} \right] = \frac{1}{4}$
            \item $E[X] = \int_{-\infty}^\infty x^2f(x)dx = \int_0^1 3x^2(1-x)^3 dx = 3 \int_0^1 (x^2-2x^3+x^4)dx = 3\left[ \frac{x^3}{3} - \frac{x^4}{2} + \frac{x^5}{5} \right]_0^1 = 3 \left[\frac{1}{3} - \frac{1}{2} + \frac{1}{5} \right] = \frac{1}{10}$
            \item $Var(X) = E[X^2] - E[X]^2 = \frac{1}{10} - \left( \frac{1}{3} \right)^2 = \frac{3}{80}$
        \end{itemize}
    \end{itemize}
    \item 5. A factory has three suppliers: $A,B,C$. Supplier $A$ provides 50\% of all parts, supplier $B$ provides 30\% of all parts, and supplier $C$ provides 20\% of all parts. The defect rate is the probability that a supplier provides a defective part. The defect rates for suppliers $A$, $B$, and $C$ are $0.01$, $0.03$, and $0.05$, respectively. A randomly chosen part is found defective, what is the probability that it came from supplier $C$? \textit{(15 points)}
    \begin{itemize}
        \item $P(A) = 0.5, P(D \mid A)= 0.01$
        \item $P(B) = 0.3, P(D \mid B) = 0.03$
        \item $P(C) = 0.2, P(D \mid C) = 0.05$
        \item $P(C \mid D) = \frac{P(D \mid C) P(C)}{P(D)} = \frac{P(D \mid C)P(C)}{P(D \mid A)P(A) + P(D \mid B)P(B) + P(D \mid C)P(C)} = \frac{0.05 \cdot 0.2}{0.01 \cdot 0.5 + 0.03 \cdot 0.3 + 0.05 \cdot 0.2} = \frac{0.1}{0.005+0.009+0.01} = \frac{5}{12}$
    \end{itemize}
\end{itemize}

\section{Lecture 8 (17/11/2025)}
\begin{itemize}
    \item $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$, or $X \sim N(\mu,\sigma^2)$
    \begin{itemize}
        \item The pdf of $X$ is $f(x)=\frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}$
        \item We often need to transform $X$ into a variable with standard normal distribution, $\frac{X-\mu}{\sigma} \sim N(0,1)$
        \begin{itemize}
            \item $E\left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma} E[X-\mu] = \frac{1}{\sigma} [E[X] - \mu] = 0$
            \item $Var\left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma^2}Var(X) = 1$
        \end{itemize}
    \end{itemize}
    \item Suppose $X_1,X_2,$ are normally distributed random variables with $X_i \sim N(\mu_i,\sigma_i^2)$ for $i=1,2,\dots,n$ and $a_1,a_2,\dots,a_n$ and $b$ are constants, then
    \[
    \sum_{i=1}^n a_iX_i + b \sim N(\mu,\sigma^2), \quad \mu = \sum_{i=1}^n a_i \mu_i + b, \quad \sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2 + 2\sum_{i<j} a_ia_j Cov(X_i,X_j)
    \]
    \begin{itemize}
        \item If $X_i$s are independent or just uncorrelated, the variance simplifies to $\sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2$
    \end{itemize}
    \item Normal Distribution - Mean $\mu$ and variance $\sigma^2$ ($X \sim \mathcal{N} (\mu, \sigma^2)$)
    \begin{itemize}
        \item Pdf of $X$ is $f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}}$
        \item Often normalize $X$ by transforming $\frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)$
        \[
        E \left[ \frac{X - \mu}{\sigma} \right] = \frac{1}{\sigma} E[X - \mu] = \frac{1}{\sigma} [E(X) - \mu] = 0
        \]
        \[
        Var \left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma^2} Var(X) = 1
        \]
    \end{itemize}
    \item Suppose $X_1, X_2, \dots, X_n$ are normally distribute random variables, with $X_i \sim \mathcal{N}(\mu_i,\sigma_i^2)$ for $i=1,2,\dots, n$ and $a_1, a_2, \dots, a_n$ and $b$ are constants, then
    \begin{flalign*}
        \sum_{i=1}^n a_i X_i + b \sim \mathcal{N}(\mu,\sigma^2) \\
        \mu = \sum_{i=1}^n a_i \mu_i + b, \quad \sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2 + 2 \sum_{i < j} a_i a_j Cov(X_i,X_j) 
    \end{flalign*}
    \item Suppose that in the population of English people aged 16 or over:
    \begin{itemize}
        \item the heights of men (in cm) follow a normal distribution with mean 174.9 and standard deviation 7.39
        \item the heights of women (in cm) follow a normal distribution with mean 161.3 and standard deviation 6.85
        \item Suppose we select one man and one woman at random and independently of each other. Denote the man’s height by X and the woman’s height by Y. What is the probability that the man is at most 10 cm taller than the woman?
        \begin{itemize}
            \item $W = X - Y, \ W \sim \mathcal{N}(\mu_W,\sigma_W^2)$
            \item $\mu_W = (1)174.9 + (-1) 161.3 = 13.6$
            \item $\sigma_W^2 = (1)^2 7.39^2 + (-1)^2 6.85^2 = 10.08^2$
            \item $p(W \le 10) = p\left(\frac{W - 13.6}{10.08} \le \frac{10-13.6}{10.08}\right) = p(z \le -0.36)$
            \begin{itemize}
                \item $z= \frac{w - 13.6}{10.08} \sim \mathcal{N}(0,1)$
            \end{itemize}
            \item Looking up in a z-table, we find that $P(W \le 10) = 0.36$
        \end{itemize}
    \end{itemize}
    \item Conditional Expectation - conditional expectation of $Y$ given $X=x$ is defined as
    \[
    E[Y \mid X = x] = \left\{ \begin{array}{cc}
        \sum_{\text{all y}} y \cdot p_{Y \mid X} (y) & Y \text{ is discrete} \\
        \sum_{-\infty}^\infty y \cdot f_{Y \mid X} (y) dy & Y \text{ is continuous}
    \end{array} \\
    \right.
    \]
    \begin{itemize}
        \item We can interpret it as the average value of $Y$ given $X=x$
    \end{itemize}
    \item $E[Y \mid X=x]$ also provides us with a useful way of summarizing the relationship, or dependence, between $X$ and $Y$
    \begin{itemize}
        \item If $Y$ tends to take on higher values when $X$ also takes on higher values, the mapping $x$ to $E[Y \mid X=x]$ will trace out an increasing function
        \item $E[Y \mid X=x]$ can also be shown to give the best possible prediction that we can make about $Y$, using (only) the information that $X=x$
    \end{itemize}
    \item Conditional Expectation Properties
    \begin{itemize}
        \item If $X$ and $Y$ are independent, $E[Y \mid X=x] = E[Y]$
        \item Conditional expectation has similar property as unconditional expectation when it comes to linear functions of random variables:
        \[ E[aY+bZ + c \mid X=x] = aE[Y \mid X=x] + bE[Z \mid X=x] +c
        \]
        \item Law of iterated expectations - $E[E[Y \mid X]] = E[Y]$, where the inner expectation is taken over $Y$ conditional on $X=x$, the outer expectation is taken over $X$
        \begin{itemize}
            \item If we make our best guess of $Y$ when $X=x$ and average over all possible values of $X$, then we recove the guess we would have make about $Y$, which is $E[Y]$
        \end{itemize}
    \end{itemize}
\subsection{Slide 8}
    \item Toss a coin $n$ times where each toss has probability $p$ of yielding a head, and we derived the total number of heads $Y \sim Binomial(n,p)$
    \begin{itemize}
        \item In statistics, we invert the problem: we might have data on the total, number of successes from independent experimental trials and we would like to use this data to learn about $p$ (can we estimate $p$?)
    \end{itemize}
    \item Statistics is the science of using data to learn about the world
    \begin{itemize}
        \item How can we come up with a good estimator? What properties does the estimator have? What properties do we want the estimated have
    \end{itemize}
    \item Suppose we are studying the impact of current inflation on people's spending patterns in the UK
    \begin{itemize}
        \item To conduct the analysis, we need to factor in individual's income level and perhaps estimate the income distribution
        \item The size of the UK population is about 67 million
        \item One reasonable thing to do is to draw a random sample of size $n$ people, and collect data about their income $x_1,x_2, \dots, x_n$
        \item We typically assume $X_1, X_2, \dots, X_n$ are i.i.d. random variables
        \item For the first observation or data point in our sample, we randomly select an individual to record their income level
        \item There is randomness to who the particular individual we select is, therefore we treat $X_1$ as a random variable
        \item After the first observation has been selected, there is no more randomness, and $X_1=x_1$ is the realized value of $X_1$
        \begin{itemize}
            \item Data are realizations of random variables drawn from an underlying population distribution
        \end{itemize}
        \item $X_1,X_2,\dots, X_n$ are i.i.d. for the following reasons
        \begin{itemize}
            \item Because $X_1, X_2, \dots, X_n$ are randomly drawn from the same population, the marginal distribution of $X_i$ is the same for $i=1,2,\dots,n$
            \item Under simple random sampling, knowing the value of $X_1$ provides no information about $X_2$
        \end{itemize}
    \end{itemize}
    \item Suppose $X_1, X_2, \dots, X_n$ are i.i.d. with mean $\mu$ and variance $\sigma^2$
    \begin{itemize}
        \item $\mu, \sigma^2$ are the unknown parameters we are interested in estimating using the data
        \item If $X$ is continuous, $\mu = \int_{-\infty}^\infty x \cdot f(x)dx$
        \item Sample mean $\hat{u} = \frac{1}{n} \sum_{i=1}^n X_i$ is one estimator
    \end{itemize}
    \item More generally, an estimator $\hat{\theta}$ (for unknown parameter $\theta$) is a function of the random sample $X_1, X_2, \dots, X_n$ with distribution $f(x_1, x_2, \dots, x_n; \theta)$
    \begin{itemize}
        \item $\hat{u} = \frac{1}{n} \sum_{i=1}^n X_i$ is an estimator for $\mu = \int_{-\infty}^\infty x \cdot f(x)dx$ 
        \item Here we are looking at the first moment, but moment estimator can be constructed for higher order moments, covariance of two random variables, etc.
    \end{itemize}
    \item Since $\hat{\theta}$ is a random variable itself, it is reasonable to consider $E[\hat{\theta}] =\theta$ a desirable property
    \begin{itemize}
        \item If $\hat{\theta}$ is evaluated many times with many samples, on average, we would get the right answer
        \item Formally, if $E[\hat{\theta}]=\theta$, $\hat{\theta}$ is an unbiased estimator
        \item Note that the unbiasedness of an estimator does not depend on the size of the sample $n$
    \end{itemize}
    \item $\tilde{\mu} = X_1$ is an estimator of $\mu$ and it is unbiased because $E[X_1] = \mu$. Is $\tilde{\mu}$ a desirable estimator?
    \begin{itemize}
        \item Throwing away $X_2, \dots, X_n$ observations
        \item Variance increases with less observations
        \item Perhaps unbiasedness alone is not sufficient for an estimator to be desireable
    \end{itemize}
    \item We might also want to compute the variance of the estimator, which tells us how reliable the estimator is
    \begin{itemize}
        \item If $\hat{\theta}$ and $\tilde{\theta}$ are two estimators for $\theta$, and $\hat{\theta}$ has a smaller variance than $\tilde{\theta}$, then $\hat{\theta}$ is said to be more efficient than $\tilde{\theta}$
        \item Going back to our example: $Var(\hat{\mu}) = Var\left( \frac{1}{n} \sum_{i=1}^n X_i \right)= \frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n} < \sigma^2 = Var(X_1) = Var(\tilde{\mu})$
        \item Standard Error - the estimated standard deviation of the estimator (estimator for $\sqrt{Var(\hat{\theta})}$)
    \end{itemize}
    \item Consider a third estimator for $\mu$: $\overline{\mu} = \frac{1}{n} (\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n)$
    \begin{itemize}
        \item For convenience, assume $n$ is even, then $E[\overline{\mu}] = E[\frac{1}{n} (\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n)] = \frac{1}{n} \left(\frac{1}{2} E[X_1] + \frac{3}{2} E[X_2] + \frac{1}{2} E[X_3] + \frac{3}{2} E[X_4] + \cdots + \frac{1}{2} E[X_{n-1}] + \frac{3}{2} E[X_n]\right) = \frac{1}{n} \left( \frac{1}{2}\frac{3}{2} \mu + \frac{1}{2} \frac{1}{2} \mu \right)  = \mu$
        \item $Var(\overline{\mu})= \frac{1}{n^2} Var(\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n) \\ = \frac{1}{n^2} \left( \frac{1}{4} Var[X_1] + \frac{9}{4} Var[X_2] + \cdots + \frac{1}{4} Var[X_{n-1}] + \frac{9}{4} Var[X_n] \right) = \frac{1}{n^2} \frac{5}{2} \frac{n}{2}z \sigma^2 = 1.25 \frac{\sigma^2}{n} > Var(\hat{\mu})$
        \item Note that all three estimators $\hat{\mu}, \tilde{mu}, \overline{\mu}$ are unbiased and linear functions of $X_1,X_2,\dots,X_n$ 
    \end{itemize}
    \item In the class of unbiased estimators which can be written as $\frac{1}{n} \sum_{i=1}^n a_iX_i$, where $a_i$s are constants, $\hat{\mu}$ has the smallest variance, and is called the best linear unbiased estimator (BLUE)
    \item How do we compare two estimators if one is biased and the other is not?
    \begin{itemize}
        \item We can use mean squared error: $MSE(\hat{\theta}) =  E[(\hat{\theta} - \theta)^2]$
        \item $MSE(\hat{\theta}) = \left[ bias(\hat{\theta}) \right]^2 + Var(\hat{\theta}^2)$ where $bias(\hat{\theta}) = E[ \hat{\theta}] - \theta$
        \begin{flalign*}
            MSE(\hat{\theta}) &=  E[(\hat{\theta} - \theta)^2] \\
            &= E[(\hat{\theta} - E[\hat{\theta}] + E[\hat{\theta}] - \theta)^2] \\
            &= E[(\hat{\theta} - E[\hat{\theta}])^2 + 2(\hat{\theta} - E[\hat{\theta}])(E[\hat{\theta}] - \hat{\theta}) + (E[\hat{\theta}] - \theta)^2] \\
            &= \underset{Var(\hat{\theta})^2}{\underbrace{E[(\hat{\theta} - E[\hat{\theta}])^2]}} + \underset{E[\hat{\theta}]E[\hat{\theta}] - E[\hat{\theta}]E[\hat{\theta}] - E[\hat{\theta}]\theta + E[\hat{\theta}]\theta = 0}{\underbrace{2E[\hat{\theta}E[\hat{\theta}] - E[\hat{\theta}]E[\hat{\theta}] - \hat{\theta}\theta + E[\hat{\theta}]\theta]}} + \underset{bias(\hat{\theta})^2}{\underbrace{E[(E[\hat{\theta}] - \theta)^2]}} \\
            &= \left[ bias(\hat{\theta}) \right]^2 + Var(\hat{\theta}^2)
        \end{flalign*}
    \end{itemize}
    \item For an estimator to have low MSE, both bias and variance has to be small
    \begin{itemize}
        \item For many estimators, there is often a bias-variance tradeoff
        \item For this reason, it is more meaningful to discuss efficiency for two unbiased estimators, as we only need to compare variance
    \end{itemize}
    \item So far we have discussed properties of estimators regardless of the sample size $n$
    \begin{itemize}
        \item In practice, we know it is often the case that the larger the sample size is, the more reliable our evidence is
        \item Large sample properties provide the mathematical reasoning behind such intuition
        \item In addition, as we typically do not have knowledge about the distribution of even a simple estimator $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i$, unless $X_i$s are all normally distributed, we need some tools to analyze the distribution of our estimators
        \item Large sample properties provide us with the tool: analyze the distribution of our estimators when $n$ is large
    \end{itemize}
\end{itemize}

\section{Tutorial 6 (21/11/2025)}
\begin{itemize}
    \item Remember that $f_X \cdot f_Y = f_{X,Y}$ if $X$ and $Y$ are independent
    \begin{itemize}
        \item $f_X(x,y) = \int_{-\infty}^\infty f_{X,Y}(x,y) dy$
        \item $f_Y(x,y) = \int_{-\infty}^\infty f_{X,Y}(x,y) dx$
    \end{itemize}
\subsection{Problem Set 5}
    \item 1.  An urn contains four red chips, three white chips, and two blue chips. A random sample of size 3 is drawn without replacement. Let $X$ denote the number of white chips in the sample and $Y$ the number of blue chips. Write a formula for the joint pdf of $X$ and $Y$.
    \begin{itemize}
        \item There are $\binom{9}{3}$ ways to choose three chips from nine
        \item $\binom{3}{X}$ ways of choosing $X$ white chips for $X = 0,1,2,3$
        \item $\binom{2}{Y}$ ways of choosing $Y$ blue chips for $Y=0,1,2$
        \item $\binom{4}{3-(X+Y)}$ ways of choosing $3-(X+Y)$ red chips for $(X+Y)=0,1,2,3$
        \item Let us define set $A := \left\{ (x,y): x \in \{0,1,2,3\}, y \in \{0,1,2\}, x+y \le 3 \right\}$
        \item Then $P(X=x, Y=y) = \left\{ \begin{array}{rl}
            \frac{\binom{3}{x}\binom{2}{y}\binom{4}{3-(x+y)}}{\binom{9}{3}}, & \text{for } (x,y) \in A \\
            0, & \text{otherwise} 
        \end{array} \right.$
    \end{itemize}
    \item 2. A study claims that the daily number of hours $X$ a teenager watches television and the daily number of hours they work on their homework are approximted by the joint pdf $f_{X,Y}(x,y) = xye^{-(x+y)}, x>0, y>0$
    \begin{itemize}
        \item (a) What is the probability that a teenager chosen at random spends at least twice as much time watching television as they do working on their homework?
        \begin{flalign*}
            f(x \ge 2y) &= \int_0^\infty \int_{2y}^\infty xye^{-(x+y)} dxdy \\
            &= \int_0^\infty ye^{-y} \int_{2y}^\infty xe^{-x}dxdy \\
            u = x, v &= -e^{-x} \\
            du = dx, dv &= e^{-x}dx \\
            &= \int_0^\infty ye^{-y}\left[ -xe^{-x} \vert_{2y}^\infty + \int_{2y}^\infty e^{-x}dx \right] dy \\
            &= \int_0^\infty ye^{-y} \left[ 0 + 2ye^{-2y} - e^{-x} \vert_{2y}^\infty \right] dy \\
            &= \int_0^\infty ye^{-y} \left[ 2ye^{-2y} +e^{-2y} \right] dy \\
            &= \int_0^\infty e^{-3y} \left[ 2y^2 + y \right] dy \\
            &= 2 \int_0^\infty y^2e^{-3y}dy + \int_0^\infty ye^{-3y}dy \\
            u = y^2, v &= -\frac{1}{3}e^{-3y} \\
            du = 2ydy, dv &= e^{-3y}dy \\
            &= 2 \left[ -\frac{y^2}{3}e^{-3y} \vert_0^\infty + \int_0^\infty \frac{2y}{3} e^{-3y}dy \right] + \int_0^\infty ye^{-3y}dy \\
            u = y, v &= -\frac{1}{3}e^{-3y} \\
            du = dy, dv &= e^{-3y}dy \\
            &= 2 \left[ 0 + \frac{2}{3}  \left[ -\frac{y}{3} e^{-3y} \vert_0^\infty + \int_0^\infty \frac{1}{3}e^{-3y}dy \right] \right] + \int_0^\infty ye^{-3y}dy \\
            &= 2 \left[ \frac{2}{3}  \left[ -\frac{1}{9}e^{-3y} \vert_0^\infty \right] \right] + \int_0^\infty ye^{-3y}dy \\
            &= 2 \left[ \frac{2}{3} \cdot \frac{1}{9} \right] + \int_0^\infty ye^{-3y}dy \\
            &= \frac{4}{27} + \int_0^\infty ye^{-3y}dy \\
            u = y, v &= -\frac{1}{3} e^{-3y} \\
            du = dy, dv &= e^{-3y}dy \\
            &= \frac{4}{27} + \left[ -\frac{y}{3}e^{-3y} \vert_0^\infty + \int_0^\infty \frac{1}{3}e^{-3y}dy \right] \\
            &= \frac{4}{27} + \left[ 0 + -\frac{1}{9}e^{-3y} \vert_0^\infty \right] \\
            &= \frac{4}{27} + \frac{1}{9} = \frac{7}{27} \\
        \end{flalign*}
        \item (b) Are $X$ and $Y$ independent?
        \begin{itemize}
            \item Notice that
            \begin{flalign*}
                \int_0^\infty xye^{-(x+y)}dy &= xe^{-x} \int_0^\infty ye^{-y}dy \\
                u = y, v &= -e^{-y} \\
                du = dy, dv &= e^{-y}dy \\
                &= xe^{-x} \left[ -ye^{-y} \vert_0^\infty + \int_0^\infty e^{-y}dy \right] \\
                &= xe^{-x} \left[ 0 - e^{-y} \vert_0^\infty \right] \\
                &= xe^{-x}
            \end{flalign*}
            \item Therefore, $f_X(x) = \int_0^\infty f(x,y)dy = \left\{ \begin{array}{rl}
                xe^{-x}, & x>0 \\
                0, & \text{otherwise}
            \end{array}\right.$
            \item Analogously, $f_Y(y) = \left\{ \begin{array}{rl}
                ye^{-y}, & y>0 \\
                0, & \text{otherwise}
            \end{array}\right.$
            \item If $f_{X,Y}(x,y) = f_x(x)f_Y(y)$, we can say random variables $X$ and $Y$ are independent of each other
            \item $f_{X,Y}(x,y) = \left\{ \begin{array}{rl}
                f_X(x)f_Y(y) = xe^{-x}ye^{-y} = xye^{-(x+y)}, & x>0,y>0 \\
                f_X(x)f_Y(y) = 0 \cdot 0 = 0, & \text{otherwise}
            \end{array}\right.$
            \item Thus $X \indep Y$
        \end{itemize}
    \end{itemize}
    \item 3. Suppose the joint pdf of $X$ and $Y$ is $f_{X,Y}(x,y) = 2, x \ge 0, y \ge 0, x+y = 1$. Is the condition distribution of $Y$ given $X = x$ uniform?
    \begin{flalign*}
        f_X(x) = \int_0^{1-x} 2 dy = [2y]_0^{1-x} = 2-2x, 0 \le x \le 1 \\
        f_{Y \mid X}(y) = \frac{f_{X,Y}(x,y)}{f_X(x)} = \frac{2}{2-2x} = \frac{1}{1-x}, 0 \le y \le 1-x
    \end{flalign*}
    \begin{itemize}
        \item Since $f_{Y \mid X}(y)$ is independent of $y$, it is a constant so it is uniform.
    \end{itemize}
    \item 4. A fair coin is tossed five times. Let the random variable $Y$ denote the total number of heads that occur, and let $X$ denote the number of heads occurring on the last two tosses. Find the conditional pmf $p_{Y \mid X}(y)$ for $X = 1$.
    \begin{itemize}
        \item Support for $P(Y=y \mid X=1)$ is clearly $y=\{1,2,3,4\}$
        \item $Y = X_1+X_2+X_3+X_4+X_5$ and $X = X_4 + X_5 = 1$
        \begin{itemize}
            \item $\Rightarrow Y = X + (X_1+X_2+X_3) = 1 + (X_1+X_2+X_3)$
        \end{itemize}
        \item Utilizing the binomial formula, we get $P_{Y \mid X}(y) = \left\{ \begin{array}{rl}
            \binom{3}{y-1} (0.5)^{y-1}(0.5)^{3-(y-1)} & y=1,2,3,4 \\
            0, & \text{otherwise}
        \end{array} \right.$
    \end{itemize}
\end{itemize}

\section{Lecture 9 (24/11/2025)}
\begin{itemize}
    \item A sequence of random variables $Y_1, Y_2, \dots, Y_n$ converges in probability to a constant $c$, denoted $Y_n \overset{p}{\rightarrow} c$, if for any small $\epsilon > 0$,
    \[
    P(\vert Y_n - c \vert > \epsilon) \rightarrow 0
    \]
    as $n \rightarrow \infty$
    \begin{itemize}
        \item As $n$ increases, the distribution of $Y_n$ collapses to a constant $c$, or grows more and more concentrated around the constant $c$
        \begin{itemize}
            \item Probability that $c - \epsilon \le Y_n \le c + \epsilon$ is close to 1
        \end{itemize}
        \item $\hat{\theta}$ is a consistent estimator for $\theta$, if $\hat{\theta} \overset{p}{\rightarrow} \theta$ (as sample size $n \rightarrow \infty$)
    \end{itemize}
    \item Law of Large Numbers (LLN) - suppose $X_1, X_2, \dots, X_n$ are i.i.d. with $E(X_i) = \mu$ and $Var(X_i) < \infty$, then
    \[
    \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i = \hat{\mu} \overset{p}{\rightarrow} \mu
    \]
    \begin{itemize}
        \item If the data is collected by random sampling, the i.i.d. assumption is likely to hold
        \item The finite variance assumptions can be interpreted as outliers are unlikely or observed infrequently
        \item Intuition: as we collect more and more data, our estimators constructed using sample average gets more and more concentrated around the true parameter value
        \item Applies to any random variable that takes on positive values
    \end{itemize}
    \item Proof of the Law of Large Numbers:
    \begin{flalign*}
        E[x] &= \int_0^\epsilon xf(x)dx + \int_\epsilon^\infty xf(x)dx \\
        &\ge \int_\epsilon^\infty xf(x)dx, \quad x \ge \epsilon > 0 \\
        &\ge \epsilon \int_\epsilon^\infty f(x)dx = \epsilon P(x > \epsilon) \\
        \Rightarrow P(x > \epsilon) &\le \frac{E[X]}{\epsilon}, \quad x >0, \epsilon>0 \\ \\
        P(\vert \overline{x} - \mu \vert > \epsilon) &= P((\overline{x}-\mu)^2 >. \epsilon^2) \\
        &\le \frac{E\left[(\overline{x}-\mu)^2\right]}{\epsilon^2} \\
        &= \frac{Var(\overline{x})}{\epsilon^2} \\
        &= \frac{\sigma^2}{n\epsilon^2} \\
        \lim_{n \rightarrow 0} \frac{\sigma^2}{n\epsilon^2} &= 0
    \end{flalign*}
    \item Optional: Chebyshev's Inequality - $P(\vert X - \mu \vert \ge k \sigma) \le \frac{1}{k^2}$
    \item Consistency tells us the distribution of an $\hat{\theta}$ collapses to a constant $\theta$, but we would like to know more about its shape
    \item Sequence of random variables $Y_1,Y_2, \dots, Y_n$ converges in distribution to a random variable $Y$ with cdf $F(y)$, denoted $Y_n \overset{d}{\rightarrow} Y$, if for all $y$, 
    \[
    F_{Y_n}(y) = P(Y_n \le y) \rightarrow F(y)
    \]
    as $n \rightarrow \infty$
    \begin{itemize}
        \item Intuition: convergence in distribution tells us we can treat $Y_n$ as if it has the same distribution as $Y$ when $n$ is large
    \end{itemize}
    \item Central Limit Theorem (CLT) - Suppose $X_1,X_2, \dots, X_n$ are i.i.d. with $E[X_i]=\mu$ and $Var(X_i)=\sigma^2$, then
    \[
        \frac{\overline{X}_n - E[\overline{X}_n]}{\sqrt{Var(\overline{X}_n)}} = \frac{\sqrt{n}(\overline{X}_n - \mu)}{\sigma} \overset{d}{\rightarrow} N(0,1)
    \]
    where $X_n = \frac{1}{n} \sum_{i=1}^n X_i = \hat{\mu}$
    \begin{itemize}
        \item Note $E[\overline{X}_n]=\mu, Var(\overline{X}_n) = \frac{\sigma^2}{n}$, so we can treat the left-hand side as the standardization of a nomrally distributed random variable: $\frac{\overline{X}_n - E[\overline{X}_n]}{\sqrt{Var(\overline{{X}}_n)}}$
        \item Intuition: as we collect more data, estimator fluctuates normally around true parameter value
    \end{itemize}
\subsection{Slide 9}
    \item CLT provides one way to approximate the average or sum of a random sample using normal distribution
    \item Suppose a flight from London to New York has 168 economy class seats. The arline has estimated that each ticket holder has a probability 90\% of showing up at gate. If the arline has sold 178 tickets, what is the probability that the flight is overbooked?
    \begin{itemize}
        \item We often use $\phi(z)$ to denote the cdf of standard normal distribution, $\phi(z) = P(Z \le z)$, where $Z \sim N(0,1)$
        \begin{itemize}
            \item $\phi(z)$ can be looked up in the standard normal distribution table or Z-table
            \item Exact calculation is possible but inconvenient for large $n$
        \end{itemize}
        \begin{flalign*}
            x &= \text{\# of } \text{people } \text{showing} \\
            x &\sim Binom(178,0.9) \\
            P(X > 168) &= P\left(\frac{x-np}{\sqrt{np(1-p)}} \ge  \frac{169-178 \cdot 0.9}{\sqrt{178 \cdot 0.9 (1-0.9)}} \right) \\
            &= P(z \ge 2.1999999) \\
             &= 1 - \phi(2.199) = 0.014
        \end{flalign*}
    \end{itemize}
    \item We have looked at sample mean as an estimator for $\mu$
    \begin{itemize}
         \item Next, we will look at estimators and their properties for $\sigma^2$
        \item Recall our setting: $\{ X_1, \dots, X_n\}$ is a random sample drawn from a distribution with negative mean $\mu$ and variance $\sigma^2$
        \item Applying our methods of moments to construct an estimator by replacing expectations with averages of the sample:
        \[
        \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2
        \]
        \item Is $\hat{\sigma}^2$ an unbiased estimator for $\sigma^2$?
        \begin{flalign*}
            E[\hat{\sigma}^2] &= E[\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2] \\
            &= E[\frac{1}{n} \sum_{i=1}^n (X_i^2 - 2\overline{X}X_i + \overline{X}^2)] \\
            &= E[\frac{1}{n}  \left(\sum_{i=1}^nX_i^2 - 2\overline{X} \sum_{i=1}^nX_i + n\overline{X}^2\right)] \\
            &= E[\frac{1}{n}  \left(\sum_{i=1}^nX_i^2 - 2\overline{X} n \overline{X} + n\overline{X}^2\right)], \quad \sum_{i=1}^n X_i = n\overline{X} \\
            &= E[\frac{1}{n} \sum_{i=1}^nX_i^2 - \frac{1}{n}n\overline{X}^2] \\
            &= E[X_i^2] - E[\overline{X}^2] \\
            &= E[X_i^2] - E[X_i]^2 + E[X_i]^2 - E[\overline{X}^2] \\
            &=\sigma^2 + E[X_i]^2 - E[\overline{X}^2] \\
            &=\sigma^2 + E[X_i]^2 - E[\left(\frac{X_1+X_2+\cdots+X_n}{n}\right)^2] \\
            &=\sigma^2 + E[X_i]^2 - \frac{1}{n^2} E[\sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_iX_j], \quad 2 \cdot \binom{n}{2} = n(n-1) \\
            &=\sigma^2 + E[X_i]^2 - \frac{1}{n^2} n E[X_i^2] -\frac{1}{n^2} n(n-1) E[X_i]^2, \quad E[X_iX_j] = E[X_i]E[X_j] = E[X_i]^2 \\
            &= \sigma^2 + \frac{1}{n} E[X_i]^2 - \frac{1}{n} E[X_i^2] \\
            &= \sigma^2 - \frac{1}{n}\sigma^2 = \frac{n-1}{n} \sigma^2
        \end{flalign*}
        \[
        E[\frac{n}{n-1}\hat{\sigma}^2] = \sigma^2, \quad \frac{n}{n-1}\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2
        \]
        \item $\hat{\sigma}^2$ is not an unbiased estimator for $\sigma^2$ but we can construct the sample variance to be an unbiased estimator
    \end{itemize}
    \item Sample Variance - $\tilde{\sigma}^2 = s_X^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$ and is an unbiased estimator for $\sigma^2$
    \begin{itemize}
        \item As $n$ increases, the difference between $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ becomes smaller and smaller, so in large samples, $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ should have very similar properties
        \item We can show that $\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}^2$
        \begin{flalign*}
            \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \\
            &= \frac{1}{n} \sum_{i=1}^n (X_i^2 - 2X_i\overline{X} + \overline{X}^2) \\
            &= \frac{1}{n} \left( \sum_{i=1}^n X_i^2 - 2\overline{X}(\sum_{i=1}^n X_i) + n\overline{X}^2 \right) \\
            &= \frac{1}{n} \left( \sum_{i=1}^n X_i^2 - 2n\overline{X}^2 + n\overline{X}^2 \right) \\
            &= \frac{1}{n} \sum_{i=1}^n (X_i^2) - \overline{X}^2 \\
        \end{flalign*}
        \item By LLN, $\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \overset{p}{\rightarrow} \sigma^2$, so $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ are both consistent estimators for $\sigma^2$
        \item Sketch of proof for consistency of $\hat{\sigma}^2$
        \begin{itemize}
            \item Functions of independent variables are independent, so $\{X_1^2,X_2^2, \dots, X_n^2\}$ is a random sample
            \item By LLN, $\frac{1}{n} \sum_{i=1}^n X_i^2 \overset{p}{\rightarrow} E[X^2]$
            \item By LLN, $\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \overset{p}{\rightarrow} E[X]$
            \item Convergences in probability is preserved by continuous operations such as addition, multiplication, etc.
            \begin{itemize}
                \item As a result, $\overline{X}_n^2 \overset{p}{\rightarrow} E[X]^2$
                \item And $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}_n^2 \overset{p}{\rightarrow} E[X^2] - E[X]^2 = \sigma^2$
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item We would like to test hypotheses about unknown parameters of interest $\theta$
    \begin{itemize}
        \item For example, we might want to test $H_0: \mu = 3, \quad H_1 : \mu \neq 3$
    \end{itemize}
    \item There are two types of errors
    \begin{itemize}
        \item Type I Error: null hypothesis $(H_0)$ is true, but we reject it
        \item Type II Error: alternative hypothesis $(H_1)$ is true, but we accept the null
    \end{itemize}
    \item The significance level of the test is defined as the probability of Type I error, $\alpha$
    \begin{itemize}
        \item Testing procedure: suppose $H_0$ is true, we want to check if $\overline{X}_n$ is far from 3 or not $(\vert \overline{X}_n - 3 \vert > c)$
    \end{itemize}
    \item Suppose $H_0$ is true, $\mu = 3$, then we know $\overline{X}_n \sim N(3,\frac{\sigma^2}{n})$ approximately, or $\frac{\overline{X}_n - 3}{\sqrt{\frac{\sigma^2}{n}}} \sim N(0,1)$
    \begin{itemize}
        \item If we set significance level at $\alpha = 10\%$ or 0.1, we would like
        \[
        P\left( \frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\sigma^2}{n}}} \ge k \right) = 0.1
        \]
        where $k$ is called critical value, and we know $k = 1.645$
        \item We reject $H_0$ if $\frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\sigma^2}{n}}} \ge 1.645$
        \item Hypothesis testing assesses whether sample evidence is too inconsistent with the null
        \item Upper critical value for two-tailed test found by $1-\frac{\alpha}{2} = (1-\alpha) + \frac{\alpha}{2}$
    \end{itemize}
    \item In practice, since $\sigma^2$ is often unknown, we replace it by its estimator $s_X^2$:
    \[
    \frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\tilde{\sigma}^2}{n}}} \ge 1.645
    \]
    or equivalently, $\overline{X} \ge 3 + 1.645\sqrt{\frac{s_X^2}{n}}$ or  $\overline{X} \le 3 - 1.645\sqrt{\frac{s_X^2}{n}}$
    \begin{itemize}
        \item $\frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{s_X^2}{n}}}$ is referred to as t-statistic, and in some references, $\sqrt{\frac{s_X^2}{n}}$ is referred to as the standard error of $\overline{X}_n$ (estimate of the standard deviation of $\overline{X}_n$)
    \end{itemize}
\end{itemize}

\section{Tutorial 7 (28/10/2025)}
\begin{itemize}
\subsection{Problem Set 6}
    \item 1. Suppose random variables $X$ and $Y$ have joint pdf $f_{X,Y}(x,y) = \lambda^2 e^{-\lambda(x+y)}, 0 \le x, 0 \le y$. Find $E[X+Y]$.
    \begin{itemize}
        \item $E[X+Y]=E[X]+E[Y]$
        \item $\int_0^\infty \lambda^2 e^{-\lambda(x+y)} dy = \lambda^{-\lambda x}$
        \item Hence, $f_X(x) = \left\{ \begin{array}{rl}
            \lambda e^{-\lambda x}, & x \ge 0  \\
            0, & \text{otherwise}
        \end{array}\right.$ and $f_Y(y) = f_X(y)$ for all $y \in \mathbb{R}$
        \item It follows that $E[X] = \int_0^\infty x \lambda e^{-\lambda x} dx = \frac{1}{\lambda} = E[Y]$
        \item Hence $E[X+Y] = E[X]+E[Y] = \frac{1}{\lambda} + \frac{1}{\lambda} = \frac{2}{\lambda}$
    \end{itemize}
    \item 2. $Y_1,Y_2,\dots,Y_n$ is a random sample (i.i.d.) from the uniform pdf over [0,1]. The geometric mean of the numbers is the random variable $(Y_1 \cdot Y_2 \cdot, \cdots, \cdot Y_n)^{\frac{1}{n}}$. Compute the expected value of the geometric mean.
    \begin{itemize}
        \item $Y_i \sim Uniform([0,1])$ for $i=1,\dots,n$
        \item Let $Y = \left( \prod_{i=1}^n Y_i \right)^{\frac{1}{n}}$. What is $E[Y]$?
        \begin{flalign*}
            E[Y] &= E\left[ \left( \prod_{i=1}^n Y_i \right)^{\frac{1}{n}} \right] \\
            &= E\left[ \left( \prod_{i=1}^n Y_i^{\frac{1}{n}}  \right) \right] \\
            &= \prod_{i=1}^n E \left[ Y_i^{\frac{1}{n}} \right], \qquad \because \text{Independence} \\
            &= E\left[ Y_i^{\frac{1}{n}} \right]^n, \qquad \because \text{Identically Distributed} \\
            &= \left[ \int_0^1 y_i^{\frac{1}{n}} \cdot f(y_i) dy_i  \right]^n \\
            &= \left[ \int_0^1 y_i^{\frac{1}{n}} \cdot 1 dy_i  \right]^n \\
            &= \left[ \frac{1}{\left( 1+ \frac{1}{n} \right)} y_i^{1 + \frac{1}{n}} \vert_0^1  \right]^n \\
            &= \left[ \frac{n}{n+1} y_i^{\frac{(n+1)}{n}} \vert_0^1  \right]^n = \left[ \frac{n}{1+n} \right]^n \\
        \end{flalign*}
    \end{itemize}
    \item 3. Let $X$ and $Y$ be random variables with
    \[
    f_{X,Y}(x,y) = \left\{ \begin{array}{rl}
        1, & -y < x < y, 0 < y < 1 \\
        0, & \text{otherwise}
    \end{array}\right.
    \]
    Show that $Cov(X,Y) = 0$.
    \begin{itemize}
        \item $Cov(X,Y) = E[XY] - E[X]E[Y]$
        \item $E[XY] = \int_0^1 \int_{-y}^y xy \cdot 1 dxdy = \int_0^1 y[\frac{x^2}{2}]_{-y}^y dy = 0$
        \item $f_Y(y) = \left\{ \begin{array}{rl}
            \int_{-y}^y 1 dx = [x]_{-y}^y = 2y, & 0 < y < 1 \\
            0, & \text{otherwise}
        \end{array}\right.$
        \item Hence, $E[Y] = \int_0^1 2y^2 dy = [\frac{2}{3}y^3]_0^1 = \frac{2}{3}$
         \item $f_X(y) = \left\{ \begin{array}{rl}
            \int_0^1 1 dy = [y]_0^1 = 1, & -1 < x < 1 \\
            0, & \text{otherwise}
        \end{array}\right.$
        \item Hence, $E[X] = \int_{-1}^1 x dx = [\frac{1}{2}x^2]_{-1}^1 = 0$
        \item It follows that $Cov(X,Y) = E[XY] - E[X]E[Y] = 0 - \frac{2}{3} \cdot 0 = 0$
    \end{itemize}
    \item 4. The random variable $X$ is said to have a Poisson distribution if $p_X(k) = P(X = k) = \frac{e^{-\lambda}\lambda^k}{k!}, k = 0, 1, 2,\dots$, and $\lambda$ is a positive integer. We know for any Poisson random variables, $E[X] = Var(X) = \lambda$, which can be shown using its moment generating function. Poisson distribution is often used to model the number of occurrences of a certain event during a time period. For example, the number of traffic accidents in a week. Consider the following application of Poisson distribution. \\
    Let the Poisson random variable $U$ be the number of calls for technical assistance received by a computer company during the firm’s nine normal workday hours. Suppose the average number of calls per hour is 7.0 and that each call costs the company 50 pounds. Let $V$ be a Poisson random variable representing the number of calls for technical assistance received during a day’s remaining fifteen hours. Suppose the average number of calls per hour is 4.0 for that time period and that each such call costs the company 60 pounds due to overtime cost. Find the expected cost and the variance of the cost associated with the calls received during a twenty-four-hour day.
    \begin{itemize}
        \item $U \sim Poisson(9 \cdot 7) = Poisson(63)$
        \item $V \sim Poisson(15 \cdot 4) = Poisson(60)$
        \item $C = 50U + 60V$ then $E[C] = 50 E[U] + 60 E[V] = 50 \cdot 9 \cdot 7 + 60 \cdot 15 \cdot 4 = 6750$
        \item $Var(C) = Var(50U + 60V) = 50^2Var(U) + 60^2Var(V) = 2500 \cdot 63 + 3600 \cdot 60 = 373500$
    \end{itemize}
    \item Extra Topic: Show that for $X \sim Poisson(\lambda)$, $E[X]=Var(X)=\lambda$ using the moment generating function $\mu_X(t)$
    \begin{flalign*}
        \mu_X(t) = E[e^{tX}] &= \sum_{x=0}^\infty (e^{tx})\frac{e^{-\lambda}\lambda^x}{x!} \\
        &= e^{-\lambda} \sum_{x=0}^\infty \frac{(\lambda e^t)^x}{x!} \\
        &= e^{-\lambda}e^{\lambda e^t}, \qquad \because \text{Maclaurin series for } e^z: \sum_{x=0}^\infty \frac{z^x}{x!} \\
        &= e^{\lambda(e^t-1)} \\
        M_X'(t) &= (\lambda e^t)(e^{\lambda(e^t - 1)}) \\
        E[X] = M_X'(0) &= (\lambda e^0)(e^{\lambda(e^0-1)}) = \lambda \\
        M_X''(t) &= (\lambda e^t)(e^{\lambda(e^t - 1)}) + (\lambda e^t)^2(e^{\lambda(e^t - 1)})\\
        E[X^2] &= M_X''(0) (\lambda e^0)(e^{\lambda(e^0 - 1)}) + (\lambda e^0)^2(e^{\lambda(e^0 - 1)}) = \lambda^2 + \lambda \\
        Var(X) &= E[X^2] - E[X]^2 = \lambda^2 + \lambda - \lambda^2 = \lambda
    \end{flalign*}
\end{itemize}

\section{Lecture 10 (01/12/2025)}
\begin{itemize}
    \item In hypothesis testing, we also often compute the p-value
    \begin{itemize}
        \item Specify $H_0$ and $H_1$
        \item Obtain data and compute $\overline{X}_n = \overline{x}_n^{actual}$
        \item Use $\overline{x}_n^{actual}$ to calculate the critical value
        \item Ask what is the smallest significance level to reject $H_0$?
    \end{itemize}
    \item In our example $H_0: \mu = 3$ vs $H_1: \mu \neq 3$
    \[
    p\text{-value} = P\left( \frac{\vert \overline{X}_n - 3 \vert}{\sqrt{\sigma^2 / n}} \ge \frac{\vert \overline{x}_n^{actual} - 3 \vert}{\sqrt{\sigma^2 / n}} \right)
    \]
    \item Since $\frac{\vert \overline{X}_n - 3\vert}{\sqrt{\sigma^2 /n}} \sim N(0,1)$, if $\frac{\vert \overline{x}_n^{actual} - 3 \vert}{\sqrt{\sigma^2 / n}} \ge 1.645$, p-value $\le 0.1$
    \begin{itemize}
        \item We reject $H_0$ if the significance level is 10%
    \end{itemize}
    \item In some cases, we might be interested in conducting one-sided tests
    \item For example, when studying whether additional years of education change earnings after graduation, the alternative hypothesis might be that people with higher education degrees earn more, since we believe it is unlikely that additional years of education lead to lower earnings
    \item In this case, we might want to test $H_0: \mu = 3$ vs $H_1: \mu > 3$
    \begin{itemize}
        \item Sometimes we write $H_0: \mu \le 3$ or $H_0: \mu < 3$
    \end{itemize}
    \item If the significance level is set at 0.1, we have $P \left( \frac{\vert \overline{X}_n - 3\vert}{\sqrt{\sigma^2 /n}} \ge 1.28 \right) = 0.1$
    \item Small sample size - Optional Material
    \begin{itemize}
        \item We can also conduct both one-sided and two-sided test for population mean using the $t$-statistic when the sample size is small
        \item In this case, the $t$-statistic $= \frac{Z}{\sqrt{W/(n-1)}}$ follows student $t$ distribution with $(n-1)$ degrees of freedom, $Z \sim N(0,1), W \sim \chi_{n-1}^2$, $Z$ and $W$ are independent
        \item We can compute critical values based on the student $t$ distribution
    \end{itemize}
    \item Difference between two means
    \begin{itemize}
        \item Suppose we continue our study about returns to education
        \item Suppose we have two random samples, one for population age 40 and above, and one for population below 40, the two populations have similar education levels (know if average earnings are different)
        \item Let $\hat{d} = \overline{Y}_{old} - \overline{Y}_{young}$
        \item We would like to test $H_0: d=0, H_1: d \neq 0$
        \item At 5\% significance level, $P\left( \frac{\vert \hat{d} - 0 \vert}{\sqrt{Var(\hat{d})}} \ge k \right) = 0.05, k = 1.96$
        \item We need an estimator of $Var(\hat{d})$
        \item First note $Var(\hat{d}) = Var(\overline{Y}_{old} - \overline{Y}_{young}) = Var(\overline{Y}_{old}) + Var(\overline{Y}_{young})$ since both sample are drawn randomly
        \item Then, $Var(\hat{d}) = \frac{\sigma_{old}^2}{n_{old}} + \frac{\sigma_{young}^2}{n_{young}}$, and we can find an estimator easily:
        \[
        \hat{Var}(\hat{d}) = \frac{s_{old}^2}{n_{old}} + \frac{s_{young}^2}{n_{young}}
        \]
    \end{itemize}
    \item Confidence intervals
    \begin{itemize}
        \item Since we know the distribution of $\hat{d}$, we can assess the uncertainty around the estimated difference in mean directly
        \item Given $\hat{d} \sim N(d, Var(\hat{d}))$, we have
        \begin{flalign*}
            0.95 &= P\left( \frac{\vert \hat{d} - d \vert}{\sqrt{Var(\hat{d})}} \le 1.96 \right) \\
            &= P \left(-1.96 \le \frac{\hat{d} - d}{\sqrt{Var(\hat{d})}} \le 1.96 \right) \\
            &= P \left(-1.96 \sqrt{Var(\hat{d})} - \hat{d} \le - d \le 1.96\sqrt{Var(\hat{d})} - \hat{d} \right) \\
            &= P \left( \hat{d} - 1.96 \sqrt{Var(\hat{d})} \le d \le \hat{d} + 1.96\sqrt{Var(\hat{d})} \right) \\
        \end{flalign*}
        \item We can replace $Var(\hat{d})$ with its estimator $\hat{Var}(\hat{d})$
        \item With probability 95\%, the true parameter $d$ falls in the interval $\left( \hat{d} - 1.96 \sqrt{Var(\hat{d})}, \hat{d} + 1.96\sqrt{Var(\hat{d})} \right)$
        \item We call this interval the 95\% confidence interval for $d$
        \item Intuition: if we draw our sample repeatedly, the confidence interval contains the true value $d$ 95\% of the time
    \end{itemize}
\subsection{Slide 10}
    \item We start with the simplest case: linear regression with single regressor, $Y$ on $X$ 
    \item Linear regression with single regressor lets us measure the slope, $\Delta Y / \Delta X$
    \begin{itemize}
        \item Interpretation: what is the effect on $Y$ when $X$ changes by one unit
    \end{itemize}
    \item Take test scores and student-teacher ratio (STR) as an example
    \begin{itemize}
        \item We collect data on students' test scores and STRs $(Y_i,X_i)$
        \item Try to find a line that runs through the middle of the observed variables
        \item Answer: $\underset{b_0,b_1}{\text{argmin}} \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2$
        \begin{itemize}
            \item We choose the line with the smallest total vertical squared distance from the points
            \item We minimize the vertical distance because $Y_i$ is the variable whose variation we are interested in explaining using $X_i$
        \end{itemize}
    \end{itemize}
    \item The ordinary least squares (OLS) estimator solves
    \[
    \underset{b_0,b_1}{\text{argmin}} \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2
    \]
    \item We can show the solution to this problem is
    \[
    \hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}, \ \hat{\beta}_0 = \overline{Y} - \overline{X} \hat{\beta}_1
    \]
    \item We compute the derivative and set it equal to zero:
    \begin{flalign*}
        \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2 &= (Y_1 - (b_0 + b_1X_1))^2 \\
        + (Y_2 - (b_0 + b_1X_2))^2 + \dots &+ (Y_n - (b_0 + b_1X_n))^2 \\
        \text{First Order Condition 1: } -2 \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i) &= 0 \\
        \Rightarrow \sum_{i=1}^n Y_i - n\hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n X_i &= 0 \\
        \Rightarrow \hat{\beta}_0 &= \overline{Y} - \overline{X} \hat{\beta}_1 \\
         \text{First Order Condition 2: } -2 \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i)X_i &= 0 \\
        \Rightarrow \sum_{i=1}^n X_iY_i - \hat{\beta}_0 \sum_{i=1}^n X_i  - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0 \\
        \Rightarrow \sum_{i=1}^n X_iY_i - \overline{Y} \underset{n\overline{X}}{\underbrace{\sum_{i=1}^n X_i}} + \hat{\beta}_1 \overline{X} \underset{n\overline{X}}{\underbrace{\sum_{i=1}^n X_i}}  - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0, \qquad \because \text{Substitute in } \hat{\beta}_0\\ 
        \Rightarrow \sum_{i=1}^n X_iY_i - n \overline{Y} \overline{X} + n \hat{\beta}_1 \overline{X} \overline{X} - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0 \\ 
        \Rightarrow \hat{\beta}_1 \left( n \overline{X}^2 - \sum_{i=1}^n X_i^2 \right) &= n \overline{X} \overline{Y} - \sum_{i=1}^n X_iY_i \\
        \Rightarrow \hat{\beta}_1 \left( \sum_{i=1}^n X_i^2 - n \overline{X}^2 \right) &= \sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y} \\
        \Rightarrow \hat{\beta}_1 &= \frac{\sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y}}{\sum_{i=1}^n X_i^2 - n \overline{X}^2} \\
        \sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y} &= \sum_{i=1}^n \left( X_iY_i - \overline{X} \overline{Y} \right) \\
        &= \sum_{i=1}^n \left( X_iY_i - \overline{X}Y_i - \overline{Y}X_i + \overline{X} \overline{Y} \right) \\
        &\because \sum_{i=1}^n \overline{X}Y_i = \sum_{i=1}^n \overline{Y}X_i = n \overline{XY} \\
        &= \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y}) \\
        \sum_{i=1}^n X_i^2 - n \overline{X}^2 &= \sum_{i=1}^n (X_i^2 - \overline{X}^2) \\
        &= \sum_{i=1}^n (X_i^2 - 2X_i\overline{X} + \overline{X}^2) \qquad \because \sum_{i=1}^n X_i\overline{X} = n \overline{X}^2 \\
        &= \sum_{i=1}^n (X_i - \overline{X})^2 \\
        \Rightarrow \hat{\beta}_1 &= \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
    \end{flalign*}
    \item Two observations given the expressions of the OLS estimator $\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}$
    \begin{itemize}
        \item Little variation in $X_i$ can create problems, as the denominator can not be too close to zero
        \item Larger variation in $X_i$ leads to more precisely estimated $\beta_1$
        \begin{itemize}
            \item In plain language measn that more horizontal spread leads to easier detection of trends
        \end{itemize}
    \end{itemize}
    \item In theory, we can also estimate $X = \delta_0 + \delta_1 Y$ by solving $\underset{\delta_0,\delta_1}{\text{argmin}} \sum_{i=1}^n (X_i - \delta_0 - \delta_1 Y_i)^2$, but $\hat{\beta} \neq \frac{1}{\hat{\delta}}$
    \begin{itemize}
        \item This is because regression of $Y$ on $X$ minimizes vertical errors while regression of $X$ on $Y$ minimizes horizontal errors
    \end{itemize}
    \item Population linear regression model:
    \[
    Y_i = \beta_0 + \beta_1 X_i + u_i
    \]
    $i = 1,2,\dots,n$
    \begin{itemize}
        \item $Y_i$ is the dependent variable
        \item $X_i$ is the independent variable (or regressor or covariate)
        \item $u_i$ is the error term and captures other factors which affect $Y_i$
        \item $\beta_0$ is the intercept, $\beta_1$ is the slope
    \end{itemize}
    \item There are three assumptions we are making:
    \begin{itemize}
        \item Assumption 1: $E[u_1 \mid X_i] = 0$
        \begin{itemize}
            \item Equivalent to $E[Y_i \mid X_i] = \beta_0 + \beta_1 X_i$
            \begin{flalign*}
                E[Y_i \mid X_i] &= E[\beta_0 + \beta_1X_i + u_i \mid X_i] \\
                &= \beta_0 + \beta_1 E[X_i \mid X_i] + E[u_i \mid X_i] \\
                &= \beta_0 + \beta_1 X_i \qquad \text{if } E[u_i \mid X_i] = 0
            \end{flalign*}
            \item Assumes the conditional expectation is always a linear function (lots of cases when this is not true)
            \item For a given value of $X_i$, the expectation taken of $u_i$ needs to average to zero and $Y_i$ needs to average to the linear regression line
        \end{itemize}
        \item Assumption 2: $(Y_i,X_i), i=1,\dots,n$ are i.i.d.
        \begin{itemize}
            \item Equivalent to assuming data are randomly sampled
            \item Some cases when this assumption may not hold are things like temporal correlation, spatial correlation, etc.
        \end{itemize}
        \item Assumption 3: $0 < E[u_i^4] < \infty$ and $0 < E[X_i^4] < \infty$
        \begin{itemize}
            \item Can be interpreted as large outliers $(X_i, Y_i)$ are unlikely
            \item OLS can be sensitive to even a single outlier
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Tutorial 9 (05/12/2025)}
\begin{itemize}
    \item $\hat{\theta}$ is a consistent estimator for $\theta$ if $\hat{\theta}$ converges in probability to $\theta$
    \begin{itemize}
        \item Check for asymptotic unbiasedness ($\lim_{n\rightarrow \infty} E[\hat{\theta}] = \theta$) and $\lim_{n \rightarrow \infty} Var(\hat{\theta}) = 0$
    \end{itemize}
    \item You never accept a null. You either "reject" or "fail-to-reject"
    \begin{itemize}
        \item When you "reject" or "fail-to-reject", you must say: "... at the $x\%$ significance level"
    \end{itemize}
\subsection{Problem Set 7}
    \item 1. We showed in lecture that, for a random sample $\{ X_1, \dots, X_n\}$, $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2$ is a biased estimator for $\sigma^2 = Var(X_i)$. If $\mu = E[X_i]$ is known, show that $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2$ then becomes an unbiased estimator for $\sigma^2$.
    \begin{flalign*}
        \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 \\
        E[\hat{\sigma}^2] &= \frac{1}{n} \sum_{i=1}^n E[(X_i - \mu)^2] \\
        &= \frac{1}{n} \sum_{i=1}^n E[(X_i - E[X_i])^2] \\
        &= \frac{1}{n} n Var(X) \qquad \text{i.i.d. condition} \\
        &= Var(X) = \sigma^2
    \end{flalign*}
    \item 2. Five hundred adults are asked whether they favor a public finance reform bill. If the true proportion of the electorate in favor of the legislation is 52\%, what are the chances that fewer than half of those in the sample support the proposal? Use a normal distribution to approximate the answer.
    \begin{itemize}
        \item Suppose $X_i \overset{i.i.d.}{\sim} Bernoulli(p)$ for $i=1,\dots,n$ where $n=500$ and $p = 0.52$
        \item Then $X = \sum_{i=1}^{500} X_i \sim Binomial(n,p)$
        \item Nevertheless, we will use the $\mathcal{N}(np,np(1-p)$ approximation to answer this question (as instructed)
        \item Suppose $Y \sim \mathcal{N}(260,124.8)$. We use $Y$ to model the number of adults that support the public finance reform bill.
        \[
        P(Y \le 250) = P(\frac{Y-260}{\sqrt{124.8}} \le -\frac{10}{\sqrt{124.8}}) \approx P(Z \le -0.90)
        \]
        \item From a z-table, we obtain $P(Y \le 250) \approx 0.1841$
    \end{itemize}
    \item 3. An estimator $\hat{\theta}_n = h(W_1,\dots,W_n)$ is said to be asymptotically unbiased for $\theta$ if $\lim_{n \rightarrow \infty} E[\hat{\theta}_n] = \theta$. Suppose $W_1, \dots,W_n$ is a random sample with mean $\mu$ and variance $\sigma^2$. Let $\overline{W} = \frac{1}{n} \sum_{i=1}^n W_i$. Show that $\overline{W}^2$ is an asymptotically unbiased estimator for $\mu^2$.
    \begin{flalign*}
        E[\overline{W}^2] &= Var(\overline{W}) + E[\overline{W}]^2 \\
        E[\overline{W}] &= \frac{1}{n} \sum_{i=1}^n E[W_i] \\
        &= \frac{1}{n} n\mu = \mu \qquad \text{i.i.d. assumption} \\
        Var(\overline{W}) &= Var\left( \frac{1}{n} \sum_{i=1}^n W_i \right) \\
        &= \frac{1}{n^2} Var(\sum_{i=1}^n W_i) \\
        &= \frac{1}{n^2} \sum_{i=1}^n Var(W_i) \qquad \text{independence assumption} \\
        &= \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n} \qquad \text{identical assumption} \\
        E[\overline{W}^2] &= \frac{\sigma^2}{n} + \mu^2 \\
        \lim_{n \rightarrow \infty} E[\overline{W}^2] = \mu^2
    \end{flalign*}
    \item 4. In lecture, we showed that $\hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2$ is a consistent estimator for $\sigma^2$, and we said the sample variance $s_X^2$ must also be consistent given it has similar large sample properties as $\hat{\sigma}^2$. Show directly $s_X^2$ is consistent for $\sigma^2$, when $\{X_1,\dots,X_n\}$ is a random sample with finite fourth moments.
    \begin{flalign*}
        s_X^2 &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \\
        &= \frac{1}{n-1} \sum_{i=1}^n ((X_i - \mu) - (\overline{X} - \mu))^2 \\
        &=\frac{1}{n-1} \sum_{i=1}^n \left[(X_i - \mu)^2 + (\overline{X} - \mu)^2 - 2(X_i - \mu)(\overline{X} - \mu)\right] \\
        &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \mu)^2 + \frac{1}{n-1} n(\overline{X} - \mu)^2 - \frac{2n}{n-1} (\overline{X} - \mu)(\overline{X} - \mu) \qquad \sum_{i=1}^n X_i = n \overline{X} \\
        &= \frac{n}{n-1}\frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - \frac{n}{n-1} (\overline{X}-\mu)^2 \\
        &= \frac{n}{n-1} \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - (\overline{X} - \mu)^2 \right] \\
        &= \left( \frac{n}{n-1} \right) (A - B), \qquad A = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2, \qquad B = (\overline{X} - \mu)^2
    \end{flalign*}
    \begin{itemize}
        \item Consider $A := \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2$, for which $E[A] = \sigma^2$ and $ Var(A) = \frac{1}{n} Var(\tilde{X}_i^2) = \frac{1}{n} \left[ E[\tilde{X}_i^4] - E[\tilde{X}_i^2]^2 \right]$
        \begin{itemize}
            \item Since $E[\tilde{X}^4] < \infty$, where $\tilde{X}_i = X_i - \mu$, $\frac{1}{n} \left[ E[\tilde{X}_i^4] - E[\tilde{X}_i^2]^2 \right] \rightarrow 0$
            \item Hence, $A \overset{p}{\rightarrow} \sigma^2$ as $n \rightarrow \infty$ due to the LLN
        \end{itemize}
        \item Further, we know that $B := (\overline{X} - \mu) \overset{p}{\rightarrow} 0$ as $n \rightarrow \infty$ due to the LLN, meaning that $(\overline{X} - \mu)^2 \overset{p}{\rightarrow} 0$ as $n \rightarrow \infty$
        \item Finally, $\frac{n}{n-1} \rightarrow 1$ as $n \rightarrow \infty$
        \item Putting these together, we get that $\lim_{n \rightarrow \infty} E[s_X^2] = \sigma^2$ and $\lim_{n \rightarrow} Var(s_X^2) = 0$.
    \end{itemize}
    \item 5. To investigate possible gender discrimination in a firm, a sample of 100 men and 64 women with similar job descriptions are selected at random. A summary of the resulting monthly salaries follows:
    \[
    \begin{array}{cccc}
        0 & \text{Average Salary} & \text{Standard Deviation} & n  \\
        \hline 
        \text{Men} & 3100 & 200 & 100 \\
        \text{Women} & 2900 & 320 & 64
    \end{array}
    \]
    What does the data suggest about wage differences in the firm? Are wages of men and women different?
    \item Ragvir's hypothesis testing recipe
        \begin{itemize}
            \item (1) Assumptions
            \begin{itemize}
                \item Assume $X_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu_X,\sigma_X^2)$ for $i = 1,\dots,n_X$ and $Y_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu_Y,\sigma_Y^2)$ for $i = 1,\dots,n_Y$, where $\sigma_X^2$ and $\sigma_Y^2$ are finite and $X_i \perp Y_i$ (independence) for all $i,j$, with $\mu_X,\mu_Y,\sigma_X^2,\sigma_Y^2$ unknown
            \end{itemize}
            \item (2) Hypotheses
            \begin{itemize}
                \item $H_0: \mu_X - \mu_Y = 0$
                \item $H_1: \mu_X - \mu_y \neq 0$
            \end{itemize}
            \item (3) Test statistic
            \begin{itemize}
                \item $t := \frac{(\overline{X} - \overline{Y})}{\left[ \frac{S_X^2}{n_X} + \frac{S_Y^2}{n_Y}\right]^\frac{1}{2}}$
            \end{itemize}
            \item (4) Distribution of test statistic under $H_0$ (and our assumptions)
            \begin{itemize}
                \item $t \overset{approx}{\sim} \mathcal{N}(0,1)$
            \end{itemize}
            \item (5) Significance level
            \begin{itemize}
                \item $\alpha = 0.05$
            \end{itemize}
            \item (6) Critical value(s)
            \begin{itemize}
                \item Reject $H_0$ if and only if $\vert t \vert > 1.96$
            \end{itemize}
            \item (7) Decision rule
            \begin{itemize}
                \item Reject $H_0$ if and only if $\vert t \vert > 1.96$
            \end{itemize}
            \item (8) Compute value of $t$
            \begin{itemize}
                \item $t = \frac{3100-2900}{\left[ \frac{200^2}{100} + \frac{320^2}{64} \right]^\frac{1}{2}} \approx 4.472$
            \end{itemize}
            \item (9) Test result
            \begin{itemize}
                \item Since $4.472 > 1.96$, we reject $H_0$ at the $5\%$ significance level
            \end{itemize}
            \item Interpretation
            \begin{itemize}
                \item There is strong evidence against the hypothesis that there is no gender discrimination in earnings.
            \end{itemize}
        \end{itemize}
\end{itemize}

\section{Lecture 11 (08/12/2025)}
\begin{itemize}
    \item Probability framework for OLS
    \begin{itemize}
        \item We can show that OLS estimators of $\hat{\beta}_0. \hat{\beta}_1$ are indeed reasonable estimators for $\beta_0$ and $\beta_1$
        \item $\beta_0$ and $\beta_1$ are true parameter values (constants) in the population model
        \item $\hat{\beta}_0$ and $\hat{\beta}_1$ are the parameter estimates (random variables)
        \item Similarly, $u_i = Y_i - \beta_0 - \beta_1 X_i$ is the error term defined in the population model
        \item $\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$ is the estimated error we get by using estimates $\hat{\beta}_0$ and $\hat{\beta}_1$
        \item We start from assumption 1 ($E[u_i \mid X_i] = 0$) and apply the law of iterated expectations
        \begin{flalign*}
            E[u_i] = E[E[u_i \mid X_i]] = E[0] &= 0 \\
            \text{Sample analog: } \frac{1}{n} \sum_{i=1}^n \hat{u}_i &= 0 \\
            \text{Thus } \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) &= 0 \\
            \Rightarrow \overline{Y} - \hat{\beta}_0 - \hat{\beta}_1 \overline{X} &= 0 \\
            \Rightarrow \hat{\beta}_0 = \overline{Y} - \overline{X} \hat{\beta}_1 \\
            E[u_i X_i] = E[E[u_iX_i \mid X_i]] = E[E[u_i \mid X_i]X_i] = E[0 \cdot X_i] &= 0 \\
            \text{Sample analog: } \frac{1}{n} \sum_{i=1}^n \hat{u}_i X_i &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) X_i &= 0 \\
            \because \hat{\beta}_0 = \overline{Y} - \overline{X} \hat{\beta}_1, \qquad \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} + \hat{\beta}_1 \overline{X} - \hat{\beta}_1 X_i) X_i &= 0 \\
            \text{Note } \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y}) \overline{X} = 0 \text{ and } \frac{1}{n} \sum_{i=1}^n (\overline{X} \hat{\beta}_1 - X_i \hat{\beta}_1)\overline{X} &= 0 \\
            (1) \because \frac{1}{n} n \overline{Y} \overline{X} - \frac{1}{n} n \overline{Y} \overline{X} &= 0 \\
            (2) \because \frac{\hat{\beta}_1}{n} n\overline{XX} - \frac{\hat{\beta}_1}{n} n \overline{XX} &= 0 \\
            \text{Subtract by zero: }
            \Rightarrow \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} + \hat{\beta}_1 \overline{X} - \hat{\beta}_1 X_i) (X_i- \overline{X}) &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} - \hat{\beta}_1 (X_i - \overline{X})) (X_i- \overline{X}) &= 0 \\
            \sum_{i=1}^n (Y_i - \overline{Y})(X_i - \overline{X}) - \hat{\beta}_1 \sum_{i=1}^n (X_i - \overline{X})^2 &= 0 \\
            \Rightarrow \hat{\beta}_1 = \frac{\sum_{i=1}^n (Y_i-\overline{Y})(X_i - \overline{X}))}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
            \hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X}
        \end{flalign*}
        \item We use the above two equations and method of moments to construct estimators for $\beta_0$ and $\beta_1$
        \item We can show the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ have the same expressions as those obtained by solving the problem $\min_{b_0,b_1} \sum_{i=1}^n (Y_i - b_0 - b_1X_i)^2$
    \end{itemize}
    \item Properties of OLS estimators: unbiasedness
    \begin{flalign*}
        Y_i &= \beta_0 + \beta_1 X_i + U_i \\
        \overline{Y} &= \beta_0 + \beta_1 \overline{X} + \overline{U} \\
        Y_i - \overline{Y} &= \beta_1 (X_i - \overline{X}) + (U_i - \overline{U}) \\
        \hat{\beta_1} &= \underset{\beta_1}{\underbrace{\frac{\sum_{i=1}^n \beta_1 (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i -\overline{X})^2}}} + \frac{\sum_{i=1}^n \beta_1 (X_i - \overline{X})(U_i - \overline{U})}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
        \hat{\beta_1} &= \beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2} - \frac{\sum_{i=1}^n (X_i - \overline{X})\overline{U}}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
        \sum_{i=1}^n (X_i - \overline{X})\overline{U} &= \frac{1}{n} n \overline{X}\overline{U} - \frac{1}{n} n \overline{X} \overline{U} = 0 \\
        \Rightarrow \hat{\beta}_1 &=\beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2}
    \end{flalign*}
    \item By the law of iterated expectations, since $E[U_i \mid X_1, \dots, X_n] = 0$,
    \[
    E[\hat{\beta}_1] = E\left[ \beta_1 + \frac{\sum_{i=1}^n (X_i-\overline{X})E[U_i \mid X_1, \dots, X_n]}{\sum_{i=1}^n (X_i - \overline{X})^2} \right] = \beta_1
    \]
    \item We can also show $E[\hat{\beta}_0] = \beta_0$ similarly
    \item The OLS estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are unbiased
\subsection{Slide 11}
    \item Final exam
    \begin{itemize}
        \item Two-hour long
        \item Similar format as midterm test
        \item Exam covers all topics in lectures
        \begin{itemize}
            \item Conceptual questions only for OLS estimation
            \item Moment generating function not included
        \end{itemize}
        \item Closed book, no internet access
        \item Notes on two sheets of A4 paper (double-sided) allowed
        \item Calculator allowed
        \item Standard normal distribution table will be provided
        \item Show your work!
        \item Review lecture notes, tutorial notes, problem sets, practice exam
        \item Review session on Friday
    \end{itemize}
    \item Properties of OLS estimators: consistency
    \begin{itemize}
        \item Intuition comes from LLN
        \item We can show the true parameter $\beta_1 = \frac{Cov(X_i,Y_i)}{Var(X_i)}$
        \item We have shown $\hat{\beta}_1 = \frac{\sum_{i=1}^n (Y_i-\overline{Y})(X_i - \overline{X}))}{\sum_{i=1}^n (X_i -\overline{X})^2} = \frac{\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2}$
        \item We can see the numerator of $\hat{\beta}_1$ approximates $Cov(X_i,Y_i)$ and the denominator approximates $Var(X_i)$
    \end{itemize}
    \item Properties of OLS estimators: asymptotic distribution
    \begin{itemize}
        \item We can also use the central limit theorem to derive the asymptotic (large sample) distribution of the OLS estimates
        \item $\hat{\beta}_1 \sim \mathcal{N}(\beta_1,\sigma_{\hat{\beta}_!}^2), \hat{\beta}_0 \sim \mathcal{N}(\beta_0, \sigma_{\hat{\beta}_0}^2)$ approximately in large sample
        \item We take $\hat{\beta}_1$ as an example to describe the intuition
        \begin{flalign*}
            \hat{\beta}_1 &= \frac{\sum_{i=1}^n (Y_i-\overline{Y})(X_i - \overline{X}))}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
            Y_i &= \beta_0 + \beta_1 X_i + U_i \\
            \hat{\beta}_1 &= \frac{\sum_{i=1}^n (\beta_0 + \beta_1 X_i + U_i - \overline{Y})(X_i - \overline{X})}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
            &= \frac{1}{\sum_{i=1}^n (X_i -\overline{X})^2} \left[ \sum_{i=1}^n \beta_0(X_i - \overline{X}) + \sum_{i=1}^n \beta_1 X_i (X_i - \overline{X}) + \sum_{i=1}^n U_i (X_i - \overline{X}) - \sum_{i=1}^n \overline{Y} (X_i - \overline{X}) \right] \\
            &= \frac{1}{\sum_{i=1}^n (X_i -\overline{X})^2} [  \underset{=0}{\underbrace{(\beta_0 \sum_{i=1}^n X_i - \beta_0 \sum_{i=1}^n \overline{X})}} + \underset{= \beta_1\sum_{i=1}^n (X_i -\overline{X})^2}{\underbrace{(\beta_1 \sum_{i=1}^n X_i^2 - \beta_1 \sum_{i=1}^n X_i \overline{X})}} \\
            &- \underset{=0}{\underbrace{(\sum_{i=1}^n \overline{Y} X_i - \sum_{i=1}^n \overline{YX})}} + \sum_{i=1}^n U_i (X_i - \overline{X})] \\
            &= \frac{\beta_1\sum_{i=1}^n (X_i -\overline{X})^2 + \sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
            &= \beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2}
        \end{flalign*}
        \begin{itemize}
            \item Since $\hat{\beta}_1 = \beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2} = \beta_1 + \frac{\frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})U_i}{\frac{1}{n} \sum_{i=1}^n (X_i -\overline{X})^2}$
            \item By LLN, $\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \overset{p}{\rightarrow} Var(X_i)$, meaning that in large samples, the denominator behaves like a constant
            \item Assuming the terms are i.i.d. with finite variance, $\frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})U_i \approx \frac{1}{n} \sum_{i=1}^n (X_i - E[X_i])U_i$
            \begin{itemize}
                \item We can also assume that $E[(X_i - E[X_i])U_i] = 0$ (expected value is zero)
                \item By the central limit theorem, $\sqrt{n} \left( \frac{1}{n} \sum_{i=1}^n (X_i - E[X_i])U_i \right) \overset{d}{\rightarrow} \mathcal{N}(0,Var[(X_i - E[X_i])U_i])$
            \end{itemize}
            \item Putting everything together, we get $\hat{\beta}_1 \approx \mathcal{N}\left( \beta_1, \frac{Var[(X_i - E[X_i])U_i]}{n[Var(X_i)]^2} \right)$
            \item Then $\sigma_{\hat{\beta}_1}^2 = \frac{Var[(X_i - E[X_i])U_i]}{n[Var(X_i)]^2}$
        \end{itemize}
        \item $\sigma_{\hat{\beta}_0}^2$ has a similar expression of $\frac{constant}{n}$
    \end{itemize}
    \item Standard errors of $\hat{\beta}_0$ and $\hat{\beta}_1$, $SE(\hat{\beta}_0)$ and $SE(\hat{\beta}_1)$ are the estimates of $\sigma_{\hat{\beta}_0}$ and $\sigma_{\hat{\beta}_1}$
    \begin{itemize}
        \item Standard errors tell us how reliable our estimates are
    \end{itemize}
    \item Confidence intervals (CI): 95\% confidence intervals cover the true parameter value with probability 95\%
    \item Given the large sample distribution of $\hat{\beta}_1$, 95\% $CI = [\hat{\beta}_1 - 1.96 SE[\hat{\beta}_1], \hat{\beta}_1 + 1.96 SE[\hat{\beta}_1]]$
    \begin{itemize}
        \item The same CI expression appies to $\hat{\beta}_0$
    \end{itemize}
    \item We might be interested in testing for specific values of $\beta_1$
    \begin{itemize}
        \item $H_0 : \beta_1 = \gamma$ and $H_1: \beta_1 \neq \gamma$
        \item The procedure is the same as the population mean test we discussed before
        \begin{itemize}
            \item Set $t$-statistic $= \frac{\hat{\beta}_1 - \gamma}{SE[\hat{\beta}_1]} \sim \mathcal{N}(0,1)$ under $H_0$
            \item For 5\% significance level, reject $H_0$ if $\vert t\text{-statistic} \vert > 1.96$
        \end{itemize}
    \end{itemize}
    \item The predicted value of the dependent variable is $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$
    \begin{itemize}
        \item We can write predicted error $\hat{u}_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$, which can also be called the residual
        \item Sum of square residuals, $SSR = \sum_{i=1}^n \hat{u}_i^2$
        \item Note that SSR is what we were minimizing when solving for $\hat{\beta}_0$ and $\hat{\beta}_1$, or how far the fitted line is to the data points
    \end{itemize}
    \item Standard error of the regression $SER = \sqrt{S_{\hat{u}}^2}$, where $S_{\hat{u}}^2 = \frac{1}{n-2} \sum_{i=1}^n \hat{u}_i^2 = \frac{SSR}{n-2}$, which measures the average size of the regression residual or "mistake" made by OLS line
    \item Total sum of squares $TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2$ measures the total variation in the dependent variable which is what we are trying to explain using the regression model
    \item Explained sum of squares $ESS = \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2$ measures the variation we are able to explain using the regression model
    \item $R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}$, the fraction of variation in the dependent variable explained by the regression model, $0 \le R^2 \le 1$
    \begin{itemize}
        \item  If $(\forall i) \ \hat{u}_i = 0$, then $Y_i = \hat{Y}_i, R^2 = 1$, we have a perfect fit
        \item If $\hat{\beta}_1 = 0, \hat{Y}_i = \hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X} = \overline{Y}$, then $ESS = 0, R^2 = 0$, which can be interpreted as $X_i$ does not explain any variation in $Y_i$
    \end{itemize}
    \item Note $R^2$ increases with the number of independent variables, so when comparing models with different numbers of covariates, $R^2$ needs to be adjusted
    \item Going back to the test score vs STR example, we can conduct the following hypothesis test: $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$
    \begin{itemize}
        \item 95\% confidence interval $CI=[-2.28-1.96 \times 0.52, -2.28 + 1.96 \times 0.52] = [-3.30,-1.26]$, does not cover 0
        \item We say $\hat{\beta}_1$ is statistically significantly different from 0 at the 5\% level, or simple, statistically significant at the 5\% level
        \item This shows STR is a relevant factor for test score, however, given the low $R^2$, there might be other factors we are missing in the regression model, which may or may not change the relevance of STR
    \end{itemize}
\end{itemize}

\section{Tutorial 10 (12/12/2025)}
\begin{itemize}
    \item Variance of a linear combination
    \begin{itemize}
        \item $Var(a_1Y_1+a_2Y_2) = a_1^2Var(Y_1)+a_2^2Var(Y_2)+2a_1a_2Cov(Y_1,Y_2)$
        \item We can rewrite this expression as $Var(a_1Y_1+a_2Y_2) = a_1a_1 Cov(Y_1,Y_1) + a_1a_2Cov(Y_1,Y_2) + a_2a_1Cov(Y_2,Y_1) + a_2a_2 Cov(Y_2,Y_2)$
        \item $Var(a_1Y_1 + a_2Y_2) = \sum_{t=1}^2 \sum_{s=1}^2 a_ta_sCov(Y_t,Y_s)$
        \item Therefore, $Var(a_1Y_1+\dots+a_nY_n) = Var\left( \sum_{i=1}^n a_iY_i \right) = \sum_{t=1}^n \sum_{s=1}^n a_ta_s Cov(Y_t,Y_s)$
        \item Obviously, if we have $Y_i \indep Y_j$ for $i \neq j$ then $Cov(Y_i,Y_j) = 0$ when $i \neq j$ for $i,j = 1,\dots,n$
        \item Thus, under independence, $Var\left( \sum_{i=1}^n a_iY_i \right) = \sum_{i=1}^n a_i^2Var(Y_i)$
    \end{itemize}
\subsection{Problem Set 8}
    \item 1. We derived in class the slope estimate $\hat{\beta}_1$ by solving the minimization problem $\min_{b_0,b_1} \sum_{i=1}^n (Y_i - b_0 - b_1X_i)^2$. We showed $\hat{\beta}_1 = \frac{\sum_{i=1}^n X_iY_i - n\overline{XY}}{\sum_{i=1}^n X_i^2 - n\overline{X}^2}$. Complete the derivation by showing $\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}$.
    \begin{flalign*}
        \text{Numerator} &= \sum_{i=1}^n X_iY_i - n\overline{XY} - n\overline{XY} + n\overline{XY} \\
        &= \sum_{i=1}^n X_iY_i - \sum_{i=1}^n X_i\overline{Y} - \sum_{i=1}^n Y_i\overline{X} +  \sum_{i=1}^n \overline{XY} \\
        &= \sum_{i=1}^n Y_i (X_i - \overline{X}) - \sum_{i=1}^n \overline{Y}(X_i - \overline{X}) \\
        &= \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{X}) \\
        \text{Denominator} &= \text{Replace $Y_i$ with $X_i$ and $\overline{Y}$ with $\overline{X}$} \\
        &= \sum_{i=1}^n (X_i-\overline{X})(X_i-\overline{X}) \\
        &= \sum_{i=1}^n (X_i - \overline{X})^2
    \end{flalign*}
    \item 2. We showed in class that the OLS estimator $\hat{\beta}_1$ is unbiased. Show $\hat{\beta}_0$ is also unbiased.
    \begin{itemize}
        \item We can rewrite $\hat{\beta}_1$ as follows:
        \begin{flalign*}
            \hat{\beta}_1 &= \frac{\sum_{i=1}^n (X_i -\overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2} \\
            &= \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i - (X_i -\overline{X})\overline{Y}}{\sum_{i=1}^n (X_i - \overline{X})X_i - (X_i - \overline{X})\overline{X}} \\
            &= \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i - \overline{Y} \sum_{i=1}^n (X_i -\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i - \overline{X} \sum_{i=1}^n (X_i - \overline{X})} \\
            &= \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i - \overline{Y} (n\overline{X} - n\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i - \overline{X} (n\overline{X} - n\overline{X})} \\
            &= \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i}{\sum_{i=1}^n (X_i - \overline{X})X_i}
        \end{flalign*}
        \item $Y_i = \beta_0 + \beta_1 X_i + u_i$
        \item Consider $\hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X}$ where $\hat{\beta}_1 := \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i}{\sum_{i=1}^n (X_i - \overline{X})X_i}$
        \item We want to find $E[\hat{\beta}_0] = E[\overline{Y}] - E[\hat{\beta}_1\overline{X}]$
        \item Then by the Law of Iterated Expectations, $E[\overline{Y}] = E[\beta_0 + \beta_1 \overline{X} + E[\overline{U} \mid X]]$ and $E[\hat{\beta}_1 \overline{X}] = E[\overline{X} E[\hat{\beta}_1 \mid X]]$
        \item We define $X$ to be a specific set of $X_1,\dots,X_n$
        \begin{flalign*}
            E[\hat{\beta}_1 \mid X] &= E\left[ \frac{\sum_{i=1}^n (X_i -\overline{X})Y_i}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X \right] \\
            &= E\left[ \frac{\sum_{i=1}^n (X_i -\overline{X})(\beta_0 + \beta_1X_i + u_i)}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X \right] \\
            &= E\left[ \beta_0\frac{\sum_{i=1}^n (X_i -\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i} + \beta_1 \frac{\sum_{i=1}^n (X_i -\overline{X})X_i}{\sum_{i=1}^n (X_i - \overline{X})X_i}  + \frac{\sum_{i=1}^n (X_i -\overline{X})u_i}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X \right] \\
            &= \beta_0 E\left[ \frac{\sum_{i=1}^n (X_i -\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X\right] + E[\beta_1 \mid X] + E\left[ \frac{\sum_{i=1}^n (X_i -\overline{X})u_i}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X \right] \\
            &= \underset{=0}{\underbrace{\beta_0 E\left[ \frac{(n\overline{X} -n\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i} \mid X\right]}} + \beta_1 + \underset{\text{Assumption 1}}{\underbrace{E\left[ \frac{\sum_{i=1}^n (X_i -\overline{X})}{\sum_{i=1}^n (X_i - \overline{X})X_i} E\left[ u_i \mid X \right] \right]}} \\
            &= \beta_1
        \end{flalign*}
        \item This means that $E[\hat{\beta}_1 \overline{X}] = E[\overline{X} E[\hat{\beta}_1 \mid X]] = E[\overline{X}\beta_1] = \beta_1 \mu$
        \item We also know that Assumption 1 makes $E[\overline{Y}] = E[\beta_0 + \beta_1 \overline{X} + E[\overline{U} \mid X]] = E[\beta_0 + \beta_1 \overline{X} + 0] = \beta_0 + \beta_1\mu$
        \item Therefore, $E[\hat{\beta}_0] = E[\overline{Y}] - E[\hat{\beta}_1 \overline{X}] = \beta_0 + \beta_1\mu - \beta_1\mu = \beta_0$
    \end{itemize}
    \item 3. Show that under the three OLS assumptions, $\beta_1 = \frac{Cov(X_i,Y_i)}{Var(X_i)}$
    \begin{flalign*}
        Y_i &= \beta_0 + \beta_1 X_i + u_i \\
        Cov(X_i,Y_i) &= \underset{=0}{\underbrace{Cov(X_i,\beta_0)}} + \beta_1 Cov(X_i,X_i) + Cov(X_i,u_i) \\
        &= \beta_1 Var(X_i) + E[X_iu_i] - E[X_i]E[u_i] \\
        &= \beta_1 Var(X_i) + E[E[X_iu_i \mid X_i]] - E[X_i]\underset{=0}{\underbrace{E[E[u_i \mid X_i]]}} \qquad \because \text{Assumption 1 and LIE} \\
        &= \beta_1 Var(X_i) + E[X_i \underset{=0}{\underbrace{E[u_i \mid X_i]}}]\\
        &=\beta_1 Var(X_i) \\
        \Rightarrow \beta_1 &= \frac{Cov(X_i,Y_i)}{Var(X_i)}
    \end{flalign*}
    \item 4. Suppose $Y_i = \beta_0 + \beta_1X_i + u_i$, where $(X_i,u_i)$ are i.i.d., and $X_i$ is a Bernoulli random variable with $P(X_i = 1) = 0.2$. When $X_i = 1, u_i \sim \mathcal{N}(0,4)$; when $X_i = 0, u_i \sum \mathcal{N}(0,1)$.
    \begin{itemize}
        \item (a) Show that the three OLS regression assumptions are satisfied in this setting. (Hint: if $X \sim \mathcal{N}(0,\sigma^2), E[X^4] = 3\sigma^4$)
        \begin{itemize}
            \item Assumption 1: $E[u_i \mid X_i = 1] = 0$ and $E[u_i \mid X_i = 0] = 0$ and $X_i$ can take no other values
            \item Assumption 2: For any $i,j=1,\dots,n$ and $i \neq j$, we have
            \begin{flalign*}
                P((X_i \le x_i, Y_i \le y_i), (X_j \le x_j, Y_j \le y_j)) &= P((X_i \le x_i, \beta_0 + \beta_1 X_i + u_i \le y_i), (X_j \le x_j, \beta_0 + \beta_1 X_j + u_j \le y_j)) \\
                &= P(X_i \le x_i, \beta_0 + \beta_1 X_i + u_i \le y_i) \cdot P(X_j \le x_j, \beta_0 + \beta_1 X_j + u_j \le y_j) \\
                &\because (X_i,u_i) \indep (X_j,u_j) \\
                &= \left[ P(X_i \le x_i, \beta_0 + \beta_1X_i +u_i \le y_i) \right]^2 \\
                &\because P(X_i \le x, u_i \le u) = P(X_j \le x, u_j \le u) \\
                &= \left[ P(X_i \le x_i, Y_i \le y_i) \right]^2 \\
                \Rightarrow &\text{All pairs $(X_i,Y_i)$ are independent and indentically distributed}
            \end{flalign*}
            \item Assumption 3: finite fourth moments
            \begin{flalign*}
                E[u_i^4] &= E[u_i^4 \mid X_i=1]P(X_i=1) + E[u_i^4 \mid X_i = 0 ]P(X_i = 0) \\
                &= (3 \times 16)(0.2) + (3 \times 1)(0.8) \qquad \because \text{Hint} \\
                &= 9.6 + 2.4 \\
                &= 12 < \infty \\
                E[X_i^4] &= 1^4 \times 0.2 + 0^4 \times 0.8 \\
                &= 0.2 < \infty
            \end{flalign*}
        \end{itemize}
        \item (b) Derive the expression for the large sample variance of the OLS estimate $\hat{\beta}_1$.
        \begin{itemize}
            \item We know that since $X_i \overset{i.i.d.}{\sim} Bernoulli(0.2)$ for $i=1,\dots,n$, $E[X_i]=0.2$ and $Var(X_i) = E[X_i^2] - E[X_i]E[X_i] = 0.2 -  0.2^2 = 0.16$
            \item We have derived that $\sigma_{\hat{\beta}_1}^2 = \frac{Var[(X_i - E[X_i])u_i]}{n[Var(X_i)]^2}$
            \item We now need to find $Var[(X_i-E[X_i])u_i] = Var[(X_i - 0.2)u_i]$
            \begin{flalign*}
                Var[(X_i - 0.2)u_i] &= E[((X_i - 0.2)u_i)^2] - E[(X_i - 0.2)u_i]^2 \\
                &= E[(X_i-0.2)^2u_i^2] - E[X_iu_i]^2 + E[0.2u_i]^2 \\
                &= E[(X_i-0.2)^2u_i^2] - E[E[X_iu_i \mid X_i]]^2 + E[E[0.2u_i \mid X_i]]^2 \qquad \because \text{LIE} \\
                &= E[(X_i-0.2)^2u_i^2] \underset{=0}{\underbrace{- E[X_iE[u_i \mid X_i]]^2 + 0.2E[E[u_i \mid X_i]]^2}} \qquad \because \text{Assumption 1} \\
                &= E[(X_i-0.2)^2u_i^2] \\
                &= E[(X_i - 0.2)^2 u_i^2 \mid X_i = 1] P(X_i = 1) + E[(X_i - 0.2)^2 u_i^2 \mid X_i = 0] P(X_i = 0) \qquad \because \text{LOTP} \\
                &= (1-0.2)^2 E[u_i^2 \mid X_i = 1] (0.2) + (-0.2)^2 E[u_i^2 \mid X_i = 0] (0.8) \\
                &= (0.64)(4)(0.2) + (0.04)(1)(0.8) \\
                &= 0.544 \\
                Var(\hat{\beta}_1) &= \frac{0.544}{n(0.16)^2}
            \end{flalign*}
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Textbook Material}
\begin{itemize}
    \item Slide 1: LM 2.1-2.3, 2.6-2.7
    \item Slide 2: LM 2.4-2.5
    \item Slide 3: LM 3.1-3.4
    \item Slide 4: LM 3.5, 3.6, 3.9, 3.12
    \item Slide 5: Midterm Review
    \item Slide 6: LM 3.7, 3.8, 3.11
    \item Slide 7: LM 3.8, 3.9, 5.1, 5.2, 4.3, 11.4
    \item Slide 8: LM 4.3, 5.2, 5.4, 5.7
    \begin{itemize}
        \item SW 2.6, 3.1
    \end{itemize}
    \item Slide 9: SW 2.4, 3.2-3.4
    \item Slide 10: SW 4
    '\item Slide 11: SW4
\end{itemize}

\end{document}
