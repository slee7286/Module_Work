\documentclass{article}
\usepackage{graphicx} % Required for inserting images
\usepackage[a4paper, left=2cm, right=2cm, top=3cm, bottom=3cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{centernot}
\allowdisplaybreaks

\title{Probability and Statistics Notes}
\author{Siheon Lee}
\date{October 2025}

\begin{document}

\maketitle

\section{Lecture 1 (6/10/2025)}
\begin{itemize}
    \item Dr Fanyin Zheng
    \begin{itemize}
        \item Market places and online platforms
        \item Hospital data
        \item PhD in economics from Harvard
        \item Three years ago faculty at Columbia University Business School
    \end{itemize}
    \item Office Hours
    \begin{itemize}
        \item 14:00-15:00 on Monday or by Appointment
        \item Tutorial leader TBD
    \end{itemize}
    \item Textbooks
    \begin{itemize}
        \item o	Larsen, R., and M. Marx. Introduction to Mathematical Statistics and Its Applications. (LM in lecture slides)
        \item o	Stock, J. H., and M. W. Watson. Introduction to Econometrics. (SW in lecture slides)
    \end{itemize}
    \item 	Experiment – repeatable procedure and set of well-defined possible outcomes
	\item Sample space S – set of all possible outcomes
    \begin{itemize}
        \item $s \in S$ is one outcome
    \end{itemize}
    \item Event A is any collection of outcomes
    \begin{itemize}
        \item Any individual outcome, any subset of or the entire sample space
	   \item Said to occur if the realized outcome of the experiment is in A
    \end{itemize}
    \item Sample space is $S={HH,HT,TH,TT}$
    \begin{itemize}
        \item Event $A$ is at least one head, $A={HH,HT,TH}$
	   \item Event $B$ is exactly two heads $B={HH}$
    \end{itemize}
    \item Set - collection of elements
    \begin{itemize}
        \item $s \in A$ if $s$ is an element of $A$
        \item $s \notin A$ if $s$ is not an element of $A$
        \item $\emptyset$ is the empty/null set without any elements
        \item $A \subseteq S$ if all elements of $A$ are also elements of $S$
    \end{itemize}
    \item Intersection - $A \cap B$ is event whose outcomes belong to both A and B
    \item Union - $A \cup B$ is event whose outcomes belong to either A or B or both
    \item Mutually exclusive – no events in common $A \cap B=\emptyset$
    \item Compliment – $A^c$ is event consisting of all the outcomes in $S$ which are not in $A$
    \item Partition - if $\bigcup_{i=1}^n A_i=S$ and $\forall i\neq j, i,j \in \{ 1,2, \dots , n \} A_i \cap A_j = \emptyset$
    \item Commutative - $A \cap B = B \cap A, A \cup B = B \cup A$
    \item Associative - $(A \cap B)\cap C = A \cap (B \cap C), (A \cup B) \cup C = A \cup (B \cup C)$
    \item Distributive - $A \cup (B \cap C) = (A \cup B) \cap (A \cup C), A \cap (B \cup C) = (A \cap B) \cup (A \cap C)$
    \item De Morgan's theorem - $(A \cap B)^c = A^c \cup B^c, (A \cup B)^c = A^c \cap B^c$
    \item Probability function - $P$ function from events to real numbers
    \begin{itemize}
        \item Probability of event $A$ is $P(A)$
        \item Axiom 1 - $P(a) \ge 0$ for any event $A$
        \item Axiom 2 - $P(S)=1$
        \item Axiom 3 - If $A$ and $B$ are mutually exclusive ($A_i \cap A_j = \emptyset$ for $i \neq j$), $P(A \cup B) = P(A) + P(B)$
        \begin{itemize}
            \item If $A_i$ and $A_j$ are pairwise mutually exclusive, $P(\bigcup_{i \ge 1} A_i) = \sum_{i \ge 1} P(A_i), i=1,2,\dots$
            \item Pairwise mutually exclusive means that all pairs of events are mutually exclusive, while a mutually exclusive relationship between $n$ events just means that the $n$ events can't occur simultaneously
        \end{itemize}
        \item $P(\emptyset)=0$
        \begin{itemize}
            \item $\emptyset = S^c$
            \item $\emptyset \cap S = 0$
            \item $P(\emptyset)+P(S)=P(\emptyset)+1=1 \Rightarrow P(\emptyset)=0$
        \end{itemize}
        \item $P(A^c)=1-P(A)$
        \begin{itemize}
            \item $P(S) = P(A \cup A^c) = 1$
            \item Since $A \cap A^c = 0$, $P(A \cup A^c) = P(A) + P(A^c) = 1$ and $P(A^c) = 1 - P(A)$
        \end{itemize}
        \item $P(A) \le 1$
        \begin{itemize}
            \item $P(S) = 1 = P(A) + P(A^c)$
            \item $P(A^c) \ge 0$ by Axiom 1, meaning $P(A) = 1 - P(A^c) \le 1$
        \end{itemize}
        \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        \begin{itemize}
            \item $P(A) = P(A \cap B) + P(A \cap B^c)$
            \item $P(B) = P(A \cap B) + P(A^c \cap B)$
            \item $P(A) + P(B) = P(A \cap B) + P(A \cap B^c) + P(A^c \cap B) + P(A \cap B) = P(A \cup B) + P(A \cap B)$
            \item $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
        \end{itemize}
    \end{itemize}
    \item $P(A) = \frac{\text{number of outcomes in } A}{\text{total number of outcomes in } S}$
    \item $m \times n$ outcomes if first part has $m$ possible outcomes and second part has $n$ possible outcomes
    \item If a password is required to have 8 characters (letters or numbers) and is case-sensitive, how many possible outcomes are there?
    \begin{itemize}
        \item $(26 \cdot 2 + 10)^8$
    \end{itemize}
    \item 10 runners compete in a race. Assume ties are not possible. All 10 completes the race. How many possible outcomes are there for the first, second, and third winners?
    \begin{itemize}
        \item ${}_{10}P_3 = \frac{10!}{(10-3)!} = 10 \cdot 9 \cdot 8$
    \end{itemize}
    \item Sampling With Replacement - take $k$ draws from group of size $n$ options
    \begin{itemize}
        \item $n \times n \times \cdots = n^k$
    \end{itemize}
    \item Sampling Without Replacement - take $k$ draws from a group of size $n$ options
    \begin{itemize}
        \item $n \times (n-1) \times (n-2) \times \cdots \times (n - (k-1)) = \frac{n!}{(n-k)!}$
    \end{itemize}
    \item Consider our runners example. Suppose we only care about which 3 runners won the top 3 places, but not which one of the three is first, second, or third
    \begin{itemize}
        \item $\binom{10}{3} = \frac{10!}{(10-3)!3!} = \frac{10 \cdot 9 \cdot 8}{3 \cdot 2} = 10 \cdot 3 \cdot 4$
    \end{itemize}
    \item Combination - Unordered collection of $k$ elements
    \begin{itemize}
        \item $\binom{n}{k} = \frac{n!}{k!(n-k)!}$
    \end{itemize}
\end{itemize}


\section{Tutorial 1 (10/10/2025)}
\begin{itemize}
    \item Probability is a social science because it deals with the world of uncertainty
    \item $A \cap B = \emptyset \Rightarrow $ mutually exclusive (ME)
    \item $A \cup B = S \Rightarrow $ collectively exhaustive (CE)
    \item If $A$ and $B$ are ME and CE, $A$ and $B$ form a partition of $S$
    \item Theorem 1 - $P(A^c)=1-P(A)$
    \begin{itemize}
        \item Since $A \cap A^c = \emptyset$, it holds by Axiom 3. Further, since $A \cup A^c = S$, it holds by Axiom 3. Putting these two results together, 
    \end{itemize}
    \item Theorem 4 - $P(A \cup B)=P(A)+P(B)-P(A \cap B)$
    \begin{itemize}
        \item Note that $A=(A \cap B) \cup (A \cap B^c)$ and $B=(A \cap B) \cup (A^c \cap B)$
        \item Also note that $(A \cup B)=(A \cap B^c) \cup (A \cap B) \cup (A^c \cap B)$
        \item So by Axiom 3, we have $P(A \cap B^c) = P(A) - P
        (A \cap B)$, $P(A^c \cap B) = P(B) - P
        (A \cap B)$, and $P(A \cup B) = P(A \cap B^c) + P(A \cap B) + P(A^c \cap B) = P(A) - P
        (A \cap B) + P(A \cap B) + P(B) - P
        (A \cap B) = P(A) + P(B) - P(A \cap B)$
    \end{itemize}
    \item Integration by Parts
    \begin{itemize}
        \item $\int udv = uv - \int vdu$
    \end{itemize}
    \item Find $\int_0^\infty x \lambda e^{-\lambda x} dx$
    \begin{itemize}
        \item $\int_0^\infty x \lambda e^{-\lambda x} dx = -x e^{-\lambda x} \vert_0^\infty - \int_0^\infty e^{-\lambda x} dx = 0 + \frac{e^{-\lambda x}}{-\lambda} \vert_0^\infty = \frac{1}{\lambda}$
    \end{itemize}
    \item Double Integrals
    \begin{itemize}
        \item
        \begin{flalign*}
            f(x,y)= \left\{ \begin{array}{rcl}
            x+y \quad , 0<x<1 \ \& \ 0<y<1 \\
            0 \quad , \text{ otherwise.}
            \end{array} \right.
        \end{flalign*}
        \item
        \begin{flalign*}
            &\int_{-\infty}^\infty f_{X,Y}(x,y)dxdy \\
            &= \int_0^1 \int_0^1 (x+y)dxdy = \int_0^1 [\frac{x^2}{2} + xy]_0^1 dy \\
            &= \int_0^1 (\frac{1}{2} + y)dy = [\frac{1}{2}y+y^2]_0^1 = 1
        \end{flalign*}
        \item Suppose we only want to integrate over the triangular region given by $\{(x,y): 0<x<1, 0<y<1, y>2x\}$
        \begin{flalign*}
            \int_0^{\frac{1}{2}} \int_{2x}^1 (x+y)dydx = \int_0^{1} \int_0^{\frac{y}{2}} (x+y) dxdy = \int_0^{1} [\frac{x^2}{2} + xy]_0^{\frac{y}{2}} dy = \int_0^{1} (\frac{5y^2}{8}) dy = [\frac{5y^3}{24}]_0^{1} = \frac{5}{24} 
        \end{flalign*}
    \end{itemize}
    \item Support - region of domain that takes nonzero values
\end{itemize}

\section{Lecture 2 (13/10/2025)}
\begin{itemize}
    \item Permutation with replacement - $n^k$
    \item Permutation without replacement - $\frac{n!}{(n-k)!}$
    \item Combination without replacement - $\binom{n}{k} = \frac{n!}{(n-k)!k!}$
    \item Combination with replacement - $\binom{n+k-1}{k} = \frac{(n+k-1)!}{k!(n-1)!}$
    \item An urn contains eight chips, numbered 1 through 8. Three chips are drawn without replacement. What is the probability that the largest chip in the sample is a 5?
    \begin{itemize}
        \item $\frac{\binom{1}{1}\binom{4}{2}}{\binom{8}{3}}$
    \end{itemize}
    \item An urn contains eight chips, numbered 1 through 8. Three chips are drawn without replacement. What is the probability that the largest chip in the sample is a 5?
    \begin{itemize}
        \item $\frac{\binom{1}{1} \binom{4}{2}}{\binom{8}{3}} = \frac{4 \cdot 3 \cdot 3! \cdot 5!}{8! c\dot 2} = \frac{4 \cdot 2 \cdot 3 \cdot 3}{8 \cdot 7 \cdot 6 \cdot 2} = \frac{3}{28}$
    \end{itemize}
    \item  An urn contains n red chips numbered 1 to n, n white chips numbered 1 to n, and n blue chips numbered 1 to n. Two are drawn at random and without replacement. What is the probability that the two drawn are either the same color or the same number?
    \begin{itemize}
        \item $P(\text{same color or same number}) = P(\text{same color}) + P(\text{same number}) + P(\text{same color and number}) = P(\text{same color}) + P(\text{same number})$ since you can't choose the same color and number chip twice
        \item $P(\text{same color}) = \frac{n - 1}{3n - 1}$
        \item $P(\text{same number}) = \frac{2}{3n - 1}$
        \item $P(\text{same color or same number}) = \frac{2+n-1}{3n-1} = \frac{n+1}{3n-1}$
        \item Second Way:
        \begin{itemize}
            \item Total number of ways to draw 2 chips from $3n$ chips is $\binom{3n}{2}$
            \item $P(\text{same color}) = P(\text{2 reds}) + P(\text{2 whites}) + P(\text{2 blues}) = 3 \times \frac{\binom{n}{2}}{\binom{3n}{2}}$
            \item $P(\text{same number}) = n \times P(\text{2 chips of number}) = n \times \frac{\binom{3}{2}}{\binom{3n}{2}}$
            \item $P(\text{same color or same number}) = \frac{3 \times \binom{n}{2} +  n \times \binom{3}{2}}{\binom{3n}{2}}$
        \end{itemize}
    \end{itemize}
    \item What is the probability that at least two people in this room were born on the same day?
    \begin{itemize}
        \item $P(\text{Born on same day}) = 1 - P^c = 1 - \frac{365 \times 364 \times \cdots \times (365 - k +1)}{365^k}$
    \end{itemize}
    \item
    \item Slide 2
    \item
    \item Conditional Probability - $P(A \mid B) = \frac{P(A \cap B)}{P(B)}$
    \item A family has two children. What is the probability that both are girl given that at least one is girl?
    \begin{itemize}
        \item $P(\text{both girls} \mid \text{at least one girl}) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{4}}{\frac{3}{4}} =\frac{1}{3}$
        \item $\frac{\frac{1}{4}}{\frac{3}{4}} = \frac{1}{3}$
    \end{itemize}
    \item $P(A \cap B) = P(A \mid B) P(B) = P(B \mid A) P(A)$
    \item Law of Total Probability - $P(B) = P(B \mid A_1)P(A_1)+ \cdots + P(B \mid A_n)P(A_n) = \sum_{i=1}^n P(B \mid A_i) P(A_i)$
    \item Bayes' Rule - A set of evens $A_1,A_2, \dots, A_n$ form a partition of the sample space. For any event $B$ where $P(B)>0$,
    $P(A_j \mid B) = \frac{P(B \mid A_j) P(A_j)}{\sum_{i=1}^n P(B \mid A_i) P(A_i)} = \frac{P(A_j \cap B)}{P(B)}$
    \item  A student is applying for a summer internship. If their interview goes well, they have a 70\% chance of getting an offer. If not, their chance drops to 20\%. The student estimates the likelihood of their interview going well is 10\%. What is the probability the student gets an offer?
    \begin{itemize}
        \item $P(\text{offer})=P(\text{offer} \mid \text{good interview})P(\text{good interview}) + P(\text{offer} \mid \text{bad interview})P(\text{bad interview}) = 0.7 \cdot 0.1 + 0.2 \cdot 0.9 = 0.07 + 0.18 = 0.25$
    \end{itemize}
    \item $P(A_j \mid B) = \frac{P(A_j \cap B)}{P(B)} = \frac{P(B \mid A_j)P(A_j)}{P(B)}$
    \begin{itemize}
        \item By Law of Total Probability, $P(B) = \sum_{i=1}^n P(B \mid A_i)P(A_i)$
        \item $P(A_j \mid B) = \frac{P(B \mid A_j)P(A_j)}{\sum_{i=1}^n P(B \mid A_i)P(A_i)}$
    \end{itemize}
    \item A biased coin, twice as likely to come up heads as tails, is tossed once. If it shows heads, a chip is drawn from urn I, which contains three white chips and four red chips; if it shows tails, a chip is drawn from urn II, which contains six white chips and three red chips. Given a white chip was drawn, what is the probability that the coin came up tails?
    \begin{itemize}
        \item $P(\text{tails} \mid \text{white chip}) = \frac{P(\text{white chip} \mid \text{tails}) P(\text{tails})}{\sum_{i=1}^n P(B \mid A_i) P(A_i)}= \frac{\frac{2}{3} \frac{1}{3}}{(\frac{2}{3} \frac{3}{7} + \frac{1}{3} \frac{2}{3})} = \frac{7}{16}$
    \end{itemize}
\end{itemize}

\section{Tutorial 2 (17/10/2025)}
\begin{itemize}
    \item Find out if order matters and if it is with or without replacement
    \item Bose-Einstein Formula for with replacement but can't distinguish order
    \item Law of Total Probability
    \begin{itemize}
        \item Suppose $B_1,B_2,B_3$ form partition of $S$
        \item By Axiom 3, $A = (A \cap B_1) \cup (A \cap B_2) \cup (A \cap B_3)$
        \item By the definition of conditional probability, we have $P(A) = P(A \mid B_1)P(B_1) + P(A \mid B_2)P(B_2) + P(A \mid B_3)P(B_3)$
    \end{itemize}
\end{itemize}

\section{Lecture 3 (20/10/2025)}
\begin{itemize}
    \item Tests: As detailed as I can under limited amount of time
    \begin{itemize}
        \item Lecture/Tutorial good benchmarks
        \item Partial credit awarded
    \end{itemize}
    \item Independence
	\begin{itemize}
		\item Events $A$ and $B$ are independent if $P(A \cap B) = P(A) \cdot P(B)$
		\item If $P(A) > 0$ and $P(B) > 0$, then $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{P(A) \cdot P(B)}{P(B)} = P(A)$
		\item $P(A \mid B) = P(A)$ and $P(B \mid A) = P(B)$
	\end{itemize}
    \item Independence and mutually exclusiveness does not imply each other in general
    \begin{itemize}
        \item $A$ and $A^c$ are mutually exclusive, but one happening tells us the other
    \end{itemize}
    \item Independence of Multiple Events: $P(A \cap B) = P(A)(B), P(B \cap C) = P(B)(C), P(A \cap C) = P(A)(C), P(A \cap B \cap C) = P(A)P(B)P(C)$
    \item A fair coin is tossed twice. Let event A be tail on first toss and event B be head on second toss. Are events A and B independent?
    \begin{itemize}
        \item $P(A) = \frac{1}{2}$, $P(B) = \frac{1}{2}$, $P(A \cap B) = \frac{1}{2} \cdot \frac{1}{2} = P(A) \cdot P(B)$
    \end{itemize}
    \item We roll a fair die once, event $A = \{2, 4, 6\}$, event $B = \{1, 2, 3, 4\}$, are $A$ and $B$ independent?
    \begin{itemize}
        \item $P(A \mid B) = \frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{3}}{\frac{2}{3}} = \frac{1}{2} = P(A)$
    \end{itemize}
    \item Consider tossing a fair coin twice. Let $A$ be head on first toss, $B$ head on second toss, and $C$ both tosses have the same result
    \begin{itemize}
        \item $P(A)=\frac{1}{2}, P(B)=\frac{1}{2}, P(C)= \frac{1}{2}$
        \item $P(A \cap B) = \frac{1}{4} = P(A)P
        (B)$
        \item $P(A \cap C) = \frac{1}{4} = P(A)P(C)$
        \item $P(B \cap C) = \frac{1}{4} = P(B)P(C)$
        \item $P(A \cap B \cap C) = P(A \cap B) = \frac{1}{4} \neq P(A)P(B)P(C)$
    \end{itemize}
    \item If $P(A) = 0, P(A \cap B \cap C)=P(A)P(B)P(C)$ holds, but the pairwise independence of $B$ and $C$ is not confirmed
    \item Suppose you are decorating the lobby for an event with a string of lights which has 1000 light bulbs. Each bulb functions independently and has a 99.9\% probability of working. The whole string fails when at least one bulb fails because of the way the bulbs are connected on the string. What is the probability that the string fails?
    \begin{itemize}
        \item $P(\text{fail}) = 1 - P(A \cap B \cap C \dots) = 1 - P(\text{bulb working})^{1000} = 1 - 0.99^{1000} - 0.6$
    \end{itemize}
    \item Monty Hall Paradox
    \begin{flalign*}
        &P(C_1) = P(C_2) = P(C_3) = \frac{1}{3} \\
        &P(M_2 \mid C_1) = \frac{1}{2} \\
        &P(M_2 \mid C_2) = 0 \\
        &P(M_2 \mid C_3) = 1 \\
        &P(C_1 \mid M_2) = \frac{P(M_2 \mid C_1)P(C_1)}{P(M_2 \mid C_1)P(C_1) + P(M_2 \mid C_2) P(C_2) + P(M_2 \mid C_3) P (C_3)} = \frac{\frac{1}{2} \times \frac{1}{3}}{\frac{1}{2} \times \frac{1}{3} + 0 \times \frac{1}{3} + 1 \times \frac{1}{3}} = \frac{1}{3} \\
        &P(C_3 \mid M_2) = 1 - P(C_1 \mid M_2) = \frac{2}{3}
    \end{flalign*}
    \item
    \item Slides 3
    \item
    \item $P(\text{having } k \text{ heads}) = \binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}$
    \item Binomial Theorem - $\binom{n}{k} \cdot p^k \cdot (1-p)^{n-k}$
    \item Discrete Random Variable - variable with a discrete number of possible events (countable number of values)
    \item Random Variable - function that assigns numbers to outcomes $s \in S \rightarrow X(s)$
    \begin{itemize}
        \item Random variables allow us to speak about a particular aspect of possible outcomes and potentially redefining the sample space 
        \item In the previous example, reduced sample space from $2^n$ to $n+1$ by limiting question to number of heads
    \end{itemize}
    \item Probability Density (Mass) Function - the likelihood of different outcomes for a random variable
    \begin{itemize}
        \item $p_X(x)=P(\{s \in S: X(s) = x\})$
    \end{itemize}
    \item Lower-case letters used to denote specific or relatized values of random variable
    \item Probability distribution for $X$ is $P(X \in A) = P(\{s \in S : X(s) \in A \})$ for any event $A$ which is a subset of real numbers in the range of $X$
    \begin{itemize}
        \item Random variable $X$ maps each outcome $s \in S$ to $X(s)$
    \end{itemize} 
    \item PDF Properties
    \begin{itemize}
        \item $0 \le p(x) \quad \forall x \in \mathbb{R}$
        \item $\sum_i p(x_i)=1,$ i.e., the sum of the probabilities of all possible values of $X$ (i.e., $X$ takes on values $x_1,x_2, \dots ,x_i$) is 1
        \item Mutually exclusive property given by the definition of pdf
    \end{itemize}
    \item Cumulative Distribution Function - the probability that $X$ takes on a value $\le x$
    \begin{itemize}
        \item $F(x)=P(\{s \in S \mid X(s) \le x\})$
    \end{itemize}
    \item CDF Properties
    \begin{itemize}
        \item $F(x)$ is a step function, remaining constant in all intervals between possible values of $X$
        \item At a possible value $x_i$ of $X$, $F(x)$ jumps up by the amount $p(x_i)=P(X=x_i)$
        \item At such an $x_i$, the value of $F(x_i)$ is the value at the top of the jump ($F_X$ is right-continuous)
    \end{itemize}
    \item Continuous Random Variable - random variable $X$ that can take on any values in some interval of the real line
    \begin{itemize}
        \item Can take more than countably many values, and therefore PDF becomes a little bit more involved
        \item Probability of any particular value typically has to be zero ($P(X=x)=0$)
        \item Discretize the distribution by putting the possible values the random variable can take into "bins" ($P(x_1 \le X \le x_2$)
    \end{itemize}
    \item Probability Density Function - non-negative function $f_X(x)$ defined as $[a,b] \subset S, P(X \in [a,b]) = \int_a^b f(x)dx$
    \item Histogram approximations to derive PDF is often used to estimate probability distribution in practice
    \item CDF PDF Properties
    \begin{itemize}
        \item $f(x) \ge 0 \quad \forall x \in \mathbb{R}$
        \item $\int_{-\infty}^\infty f(x)=1$
    \end{itemize}
    \item Uniform Distribution - distributed over $[a,b], a < b, f(x)= \left\{ \begin{array}{cc}
        \frac{1}{b-a} & \text{if } a \le x \le b  \\
        0 & \text{otherwise}
    \end{array} \right. $
    \begin{itemize}
        \item Write as $X \sim U(a,b)$
    \end{itemize}
\end{itemize}

\section{Lecture 4 (27/10/2025)}
\begin{itemize}
    \item Suppose $X$ has pdf
    \[ f(x) = \left\{ \begin{array}{cc}
        ax^2 & \text{if } 0 < x < 3 \\
        0 & \text{otherwise}
    \end{array} \right. \]
    What is $a$?
    \begin{flalign*}
        \int_{-\infty}^{\infty} f(x)dx = \int_0^3 ax^2 dx = a \left[ \frac{x^3}{3} \right]_0^3 = \frac{3^3a}{3} - 0 = 1 \Rightarrow a = \frac{1}{9}
    \end{flalign*}
    \item Definition of cdf for a continuous random variable $X$ is $F(x) = P(X \le x)$, same as for discrete random variables
    \item For continuous random variables in general, since $P(X=x)=0$, $P(a < X < b) = P(a < x \le b)$, $P(a \le x < b) = P(a \le x \le b)$
    \item We can relate the cdf and pdf of a continuous random variable using the following equations:
    \begin{itemize}
        \item $F(x)=\int_{-\infty}^x f(t)dt$
        \item $F'(x)=\frac{d}{dx} F(x)=f(x)$
    \end{itemize}
    \item If $X \sim U(a,b)$, what is its cdf?
    \begin{flalign*}
        F(x) &= \int_{-\infty}^x f(t)dt \\
        &= \left\{ \begin{array}{cl}
            \int_{-\infty}^x 0 dt & \text{if } x < a \\
            \int_a^x \frac{1}{b-a} dt & \text{if } a \le x < b \\
            \int_{a}^b \frac{1}{b-a} dt & \text{if } x \ge b
        \end{array} \right. \\
        &= \left\{ \begin{array}{cl}
            0 & \text{if } x < a \\
            \frac{t}{b-a} \vert_a^x & \text{if } a \le x < b \\
            \frac{t}{b-a} \vert_a^b & \text{if } x \ge b
        \end{array} \right. \\
        &= \left\{ \begin{array}{cl}
            0 & \text{if } x < a \\
            \frac{x-a}{b-a} & \text{if } a \le x < b \\
            1 & \text{if } x \ge b
        \end{array} \right. \\
    \end{flalign*}
    \item The following properties of cdf applies to both discrete and continuous random variables
    \begin{itemize}
        \item $0 \le F(x) \le 1$ for all $x \in \mathbb{R}$
        \item $P(X > x)=1-F(x)$
        \item $F(x_1) \le F(x_2)$ for $x_1 < x_2$
        \item $\lim_{x \rightarrow -\infty} F(x) = 0, \lim_{x \rightarrow +\infty} F(x) = 1$
        \item $F(x)$ is right continuous
    \end{itemize}
\subsection{Slide 4}
    \item Midterm Exam
    \begin{itemize}
        \item Wednesday Nov. 5th
        \item Exam covers topics from week 1 to today
        \item Closed book, no internet access
        \item Notes on one A4 size paper (double-sided) allowed
        \item College approved calculator allowed
        \item No need to prove properties proven in slides
        \item Point is to test on how to apply those tools in solving problems
    \end{itemize}
    \item Expected Value - of discrete random variable $X$ is
    \[ E(x) = x_1 \cdot p(x_1) + x_2 \cdot p(x_2) + \dots = \sum_{x_i} x_i \cdot p(x_i) = \mu_X, \]
    where $x_i$ are all possible values $X$ can take on
    \begin{itemize}
        \item For a continuous variable $Y$, the expected value is
        \[ E(Y) = \int_{-\infty}^\infty y \cdot f(y) dy = \mu_Y \]
    \end{itemize}
    \item Suppose $X$ is a binomial random variable with $p = \frac{5}{9}$ and $n=3$. What is the expected value of $X$?
    \begin{itemize}
        \item $p(x_i)=\binom{3}{x_i} \left( \frac{5}{9} \right)^{x_i} (1 - \frac{5}{9})^{(3-x_i)}, $ where $x_i= 0,1,2,3$
        \begin{flalign*}
            E(X) &= \sum_{x_i=0}^3 x_i \cdot \binom{3}{x_i} \left( \frac{5}{9} \right)^{x_i} (1 - \frac{5}{9})^{(3-x_i)} \\
            &= 0 \times \frac{64}{729} + 1 \times \frac{240}{729} + 2 \times \frac{300}{729} + 3 \times \frac{125}{729} = \frac{5}{3} = 3 \times \frac{5}{9}
        \end{flalign*}
    \end{itemize}
    \item In general, if $X$ is binomial with parameters $n$ and $p$, $E(X)=np$
    \item Suppose $X$ is a binomial random variable with parameters $p$ and $n$, the expectation $X, E(X) = \sum_{x_i=0}^n x_i \cdot \binom{n}{x_i} p^{x_i} (1-p)^{(n-x_i)}$
    \begin{flalign*}
        &= \sum_{x_i=0}^n x_i \cdot \frac{n!}{x! (n-x_i)!} p^{x_i} (1-p)^{(n-x_i)} \\
        &= \sum_{x_i=1}^n \frac{n!}{(x-1)! (n-x_i)!} p^{x_i} (1-p)^{(n-x_i)} \qquad \text{Case when } x_i=0 \text{ is zero}\\
        &= np \sum_{x_i=1}^n \frac{(n-1)!}{(x-1)! (n-x_i)!} p^{x_i-1} (1-p)^{(n-x_i)} \\
        &= np \underbrace{\sum_{x_i=1}^n \binom{(n-1)}{(x_i-1)} p^{x_i-1} (1-p)^{[n-1-(x_i-1)]}}_{=1} \qquad \text{sum of all possible values of the binominal distribution is 1} \\
    \end{flalign*}
    \item Let $Y = X-1$, $Y$ takes on value $y_j = x_i - 1 = 0,1, \dots, (n-1),$ then $E(X)=np \sum_{y_j=0}^n-1 \binom{n-1}{y_j} p^{y_j} (1-p)^{[(n-1)-y_j]} = np \underbrace{\sum_{y_j=0}^{n-1} p(y_j)}_{=1}=np$,, where $Y$ is a binomial random variable with parameters $p$ and $n-1$
    \item Researchers studying unemployment often assumes the inter-arrival time $X$ of two consecutive job offers follows an exponential distribution with parameter $\lambda: f(x)=\lambda e^{-\lambda x}, x \ge 0; f(x)=0$ otherwise
    \item What is the expected value of $X$?
    \begin{flalign*}
        E[X] &= \int_{-\infty}^\infty xf(x)dx=\int_0^{\infty} \lambda x e^{-\lambda x} dx = \qquad u = x, dv= \lambda e^{-\lambda x} dx, du = dx, v = -e^{-\lambda x} \\
        &= -xe^{-\lambda x} \vert_0^\infty - \int_0^\infty -e^{-\lambda x} dx = 0 - \frac{1}{\lambda}[e^{-\lambda x}]_0^\infty = -\frac{1}{\lambda} [0-1] = \frac{1}{\lambda}
    \end{flalign*}
    \item Often, we're not interested in $X$ itself, but in some function of it $profit=g(demand)$
    \item If $X$ is a discrete random variable with pdf $p(x_i)$, where $x_i$ represents all possible values $X$ takes on. Let $g(X)$ be a function of $X$. Then the expected value of the random variable $g(X)$ is given by $E[g(X)]=\sum_{x_i} g(x_i)p(x_i)$, if $\sum_{x_i} \vert g(x_i) \vert p(x_i) < \infty$
    \item If $Y$ is a continuous random variable with pdf $f(y)$, and if $g(Y)$ is a continuous function, then the expected value of the random variable $g(Y)$ is $\int_{-\infty}^\infty g(y)f(y)dy$, if $\int_{-\infty}^\infty \vert g(y) \vert f(y) dy < \infty$
    \item As a special case, for random variable $W$, discrete or continuous, $E(aW+b)=aE(W)+b$
    \begin{itemize}
        \item This is because $g(W)=aW+b$ is a linear function of $W$
    \end{itemize}
    \begin{flalign*}
        E(aW+b) &= \int_{-\infty}^\infty g(W)f(w)dw = \int_{-\infty}^\infty \left( aW + b \right)f(w)dw \\
        &= a\int_{-\infty}^\infty W f(w)dw + b \int_{-\infty}^\infty f(w)dw = aE(W) + b \cdot 1 = aE(W) + b
    \end{flalign*}
    \item In general, $E[g(W)] \neq g(E[W])$
    \item A fair coin is tossed until a head appears. You will be given $\frac{1}{2}^k$ dollars if the first head occurs on the $k$th toss. How much money can you expect to be paid?
    \begin{flalign*}
        &1+a+a^2+a^3+\cdots = \sum_{k=0}^\infty a^k = \frac{1}{1-a}, \text{if } \vert a \vert <1 \\
        &E[X] = \sum_{k=1}^\infty \frac{1}{2}^k \frac{1}{2}^{k} = \sum_{k=1}^\infty \left( \frac{1}{2}^2 \right)^k = \sum_{k=1}^\infty \frac{1}{4}^k = \frac{1}{1-\frac{1}{4}} - \frac{1}{4}^0= \frac{4}{3} - 1 = \frac{1}{3}
    \end{flalign*}
    \item One reasonable way of measuring dispersion is to calculate the possible deviation fromt he mean $X-\mu$, where $\mu = E[X]$
    \item Since the deviation is a random variable, we can take the expected value fo the squared deviation: variance
    \item Variance - for a discrete random variable $X$ with pdf $p(x)$ and expectation $\mu_X$, variance is 
    \[ Var(X) = E[(X-\mu_X)^2] = \sum_{x_i} (x_i - \mu_X)^2 p(x_i) = V(X) = \sigma_X^2 \]
    \begin{itemize}
        \item For a continuous random variable $Y$ with pdf $f(y)$ and expectation $\mu_Y$,
        \[ Var(Y) = E[(Y-\mu_Y)^2] = \int_{-\infty}^\infty (y - \mu_Y)^2 f(y)dy = V(Y) = \sigma_Y^2 \]
    \end{itemize}
    \item Using the definition of variance, for both discrete and continuous random variables, we can show $Var(X)=E[X^2] - E[X]^2$
    \begin{itemize}
        \item $E[(X-\mu_X)^2] = E[X^2 - 2\mu_X X + E[X]^2] = E[X^2] - E[X]^2$
        \item We are implicitly assuming $E[X^2]$ is finite
    \end{itemize}
    \item Since variance is measured in the square of the units for the random variable, an alternative measure, standard deviation, is also commonly used: $\sigma = \sqrt{\text{variance}} = \sqrt{\sigma^2}$
    \item For linear functions of random variable $W$, we also know $Var(aW+b)=a^2Var(W)=a^2\sigma^2$
    \begin{flalign*}
        Var(aW+b) &= E[ \left((aW+b)-E[aW+b]\right)^2] \\
        &= E[ \left(aW+b-aE[W]+b\right)^2] \\
        &= E[ \left(aW-aE[W] \right)^2] \\
        &= E[ a^2 \left(W-E[W] \right)^2] \\
        &= a^2 E[(W-E[W])^2] = a^2 Var(W)
    \end{flalign*}
    \item If $X$ follows an exponential distribution with parameter $\lambda$, what is $Var(X)$?
    \begin{flalign*}
        E[X^2] &= \int_0^\infty x^2 \lambda e^{-\lambda x} dx = \int_0^{\infty} x^2 d(-e^{-\lambda x}) \\
        &= x^2 (-e^{-\lambda x}) \vert_0^\infty + 2 \int_0^\infty xe^{-\lambda x}dx \\
        &=0 + 2 \frac{1}{\lambda} \underbrace{\int_0^\infty \lambda x e^{-\lambda x} dx}_{=\frac{1}{\lambda}} = \frac{2}{\lambda^2} \\
        &\Rightarrow Var(X) = E[X^2] - E[X]^2 = \frac{2}{\lambda^2} - \frac{1}{\lambda^2} = \frac{1}{\lambda^2}
    \end{flalign*}
    \item Intuitively, when the arrival rate ($\lambda$) is high, the variability of inter-arrival time, $Var(X)$, will be low
\end{itemize}

\section{Tutorial 4 (31/10/2025)}
\begin{itemize}
    \item Random Variable is a function
    \item Probability Mass Function (for discrete random variables)
    \begin{itemize}
        \item To recover pdf, take differences of the cdf
    \end{itemize}
    \item Probability Density Function (for continuous random variables)
    \begin{itemize}
        \item To recover pdf, take derivative of cdf
        \item Sometimes, derivative doesn't exist
    \end{itemize}
    \item Validity conditions of pdf: nonnegative and sum is 1
    \item For a continuous random variable, $P(X=x)=0$
    \begin{itemize}
        \item You can ask for a probability over an interval: $P(a \le x \le b) = \int_a^b f_X(x)dx$
    \end{itemize}
    \item $k \sim \text{HyperGeometric}(N,r,k)$
    \item Always define your support
    \begin{itemize}
        \item For Question 1, $k$ is bounded below by either 0 or $n-w$, the number of chips after you have exhausted the white chips
        \item It is bounded above by either $n$ or $r$, the number of red chips
    \end{itemize}
    \item Consider $X := \max \{x_1,x_2\}$
    \begin{itemize}
        \item We have
        \begin{flalign*}
            F_X(x) &= P(X \le x) \\
            &= P(X_1 \le x \cap X_2 \le x) \\
            &= P(X_1 \le x)P(X_2 \le x) \\
            &= F_{X_1}(x) F_{X_2}(x)
        \end{flalign*}
    \end{itemize}
    \item Consider $X := \min \{x_1,x_2\}$
    \begin{itemize}
        \item We have
        \begin{flalign*}
            F_X(x) &= P(X \ge x) \\
            &= P(X_1 \ge x \cap X_2 \ge x) \\
            &= P(X_1 \ge x)P(X_2 \ge x) \\
            &= (1-F_{X_1}(x)) (1-F_{X_2}(x))
        \end{flalign*}
    \end{itemize}
    \item Question 4a
    \begin{flalign*}
        \int_{-\infty}^\infty g(x)dx &= \int_{-\infty}^\infty 2F(x)f(x)dx \\
        &= \int_{-\infty}^\infty 2F(x)F'(x)dx \\
        &= \int_{-\infty}^\infty \left[ \frac{d}{dx} F^2(x) \right] dx \\
        &= \left[ F^2(x) \right]_{-\infty}^\infty  = 1 \\
    \end{flalign*}
    \item Question 4b
    \begin{itemize}
        \item Since $f(x) \ge 0, \forall x \in \mathbb{R}$, $h(x) \ge 0$
        \item Let $y=-x, dy=-dx$
        \begin{flalign*}
            \int_{-\infty}^\infty f(-x)dx = \int_{y=\infty}^{y=-\infty} f(y)(-dy) = - \int_{y=\infty}^{y=-\infty} f(y)dy = \int_{y=-\infty}^{y=\infty} f(y)dy = 1
        \end{flalign*}
    \end{itemize}
\end{itemize}

\section{Lecture 5 (3/11/2025)}
\begin{itemize}
    \item $r$th moment - $\mu_r = E[W^r]$ where $r$ is a positive integer and $\int_{-\infty}^\infty \vert w \vert^r f(w)dw < \infty$
    \item $r$th moment about the mean $\mu$ - $\mu_r' = E[(W - \mu)^r]$
    \item Moment Generating Function - mgf for random variable $W$ is denoted $M_W(t)$ and given by
    \[ M_W(t) = E[e^{tw}] = \left\{ \begin{array}{cl}
        \sum_w e^{tw} p(w) & \text{if } W \text{ is discrete} \\
        \int_{-\infty}^\infty e^{tw} f(w)dw & \text{if } W \text{ is continuous} 
    \end{array}\right. \]
    for all values of $t$ for which the expected value exists
    \item Note $M_W(t)$ is a function of real numbers $t$
    \item mgf is a convenient tool to derive means and variance:
    \[ M_X'(0)= E[X] \text{ and } M_X''(0) = E[X^2] \]
    and we have $Var(X) = M_X''(0) - (M_x'(0))^2$
    \item We can obtain other moments $M_X^{(k)}(0) = E[X^k], k \in \mathbb{N}$
    \item Suppose that $X$ and $Y$ are random variables for which $M_X(t) = M_Y(t)$ for some interval of $t$ containing $0$. Then $X$ and $Y$ have the same distribution.
    \item If $Y=aX+b$, $X$ is a random variable, $a$ and $b$ are constants, then $M_Y(t)=e^{bt}M_X(at)$
    \begin{itemize}
        \item Can be used to prove Central Limit Theorem and compare to different types of distributions
    \end{itemize} 
    \item Example: Binomial
    \begin{itemize}
        \item Suppose $X$ is a binomial random variable with parameter $n$ and $p$, $p(x_i) = \binom{n}{x_i} p^{x_i} (1-p)^{(n-x_i)}$
        \item $M_X(t) = E[e^{tx}] = \sum_{x_i=0}^n e^{tx_i} \cdot \binom{n}{x_i}p^{x_i} (1-p)^{(n-x_i)} = \sum_{x_i=0}^n \binom{n}{x_i}(pe^t)^{x_i} (1-p)^{(n-x_i)}$
        \item Since we know $(x+y)^n = \sum_{k=0}^n \binom{n}{k} x^k y^{n-k}$, let $x = pe^t$, $y=1-p$, $k=x_i$, then $M_X(t) = (1-p+pe^t)^n$
        \item We get $M_X'(t) = n(1-p+pe^t)^{n-1}pe^t$ and $M_X''(t) = n(n-1)pe^t(1-p+pe^t)^{n-2}pe^t + n(1-p+pe^t)^{n-1}pe^t$
        \item Thus $E[X] = M_X'(0)=np$, $E[X^2] = M_X''(0)=n(n-1)p^2 + np$, and $Var(X) = n(n-1)p^2 + np - (np)^2 = np - np^2 = np(1-p)$
    \end{itemize}
    \item Example: Exponential
    \begin{itemize}
        \item If $X$ follows an exponential distribution with paramter $\lambda$, derive its mgf, expectation, and variance
        \item $M_X(t)=E[e^{tx}] = \int_0^\infty e^{tx}f(x)dx = \int_0^\infty e^tx \lambda e^{-\lambda x} dx = \int_0^\infty \lambda e^{-(\lambda - t)x} dx = \frac{\lambda}{\lambda - t} \underbrace{\int_0^\infty (\lambda - t)e^{-(\lambda - t)x} dx}_{=1} = \frac{\lambda}{\lambda - t}$
        \item Above equation holds when $t < \lambda$, and $M_X(t)$ is not defined when $t \ge \lambda$
        \item We get $M'_X(t) = \frac{\lambda}{(\lambda - t)^2}$ and $M_X''(t) = \frac{2\lambda}{(\lambda - t)^3}$, so $E[X] = M_X'(0) = \frac{1}{\lambda}$ and $E[X^2] = M_X''(0) = \frac{2}{\lambda^2}$, and $Var(X) = \frac{2}{\lambda^2} - (\frac{1}{\lambda})^2 = \frac{1}{\lambda^2}$
    \end{itemize}
\end{itemize}

\section{Lecture 6 (10/11/2025)}
\begin{itemize}
    \item Motivation
    \begin{itemize}
        \item Treat eeach observation in data as a random variable, then the size of the sample is the length of the multivariate random variable of interest
        \item Often interested in relationship between two or more variables
    \end{itemize}
    \item Joint Probability Density (Mass) Function - for random variables $X,Y$, $p(x,y)=P(s \mid X(s) = x, Y(s) = y) = P(X=x, Y=y)$
    \begin{itemize}
        \item $p(x,y) \ge 0$
        \item $\sum_x \sum_y p(x,y) = 1$
    \end{itemize}
    \item For multivariate random variables $X_1, X_2, \cdots , X_n$, the join pdf $p(x_1,x_2, \cdots, x_n) = P(X_1=x_1, X_2=x_2, \cdots, X_n=x_n)$
    \item Example: A supermarket has two express check-out lines. Let $X$ and $Y$ denote the number of customers in the first and in the second, respectively, at any given time. The joint pdf of $X$ and $Y$ is summarized by the following table:
    \[ \begin{array}{cc|cccc}
         & & & & X & \\
         & & 0 & 1 & 2 & 3 \\
        \hline
         & 0 & 0.1 & 0.2 & 0 & 0 \\
         & 1 & 0.2 & 0.25 & 0.05 & 0 \\
        Y & 2 & 0 & 0.05 & 0.05 & 0.025 \\
         & 3 & 0 & 0 & 0.025 & 0.05 
    \end{array}\]
    \item What is the probability that the two lines are of the same length?
    \begin{itemize}
        \item $p(x=y)=p(0,0)+p(1,1)+p(2,2)+p(3,3) = 0.1 + 0.25 + 0.05 + 0.05 = 0.45$
    \end{itemize}
    \item What is the probability that the lengths of the two lines differ by 1?
    \begin{itemize}
        \item $p(\vert x - y \vert = 1)=p(0,1)+p(1,0)+p(1,2)+p(2,1)+p(2,3)+p(3,2) = 0.2 + 0.2 + 0.05 + 0.05 + 0.025 + 0.025 = 0.55$
    \end{itemize}
    \item Marginal PDF - Suppose that $p(x,y)$ is the joint pdf of the discrete random variables $X,Y$, then $p(x) = \sum_y p(x,y), p(y) = \sum_x p(x,y)$
    \item Back to the supermarket example, what is the probability $X \ge 2$?
    \begin{itemize}
        \item $p(x \ge 2)= \sum_y \sum_{x=2}^3 p(x,y)=p(2,\cdot)+p(3, \cdot) = (0 + 0.05 + 0.05 + 0.025) + (0 + 0 + 0.025 + 0.05) = 0.2$
    \end{itemize}
    \item Joint PDF (Continuous) - $P[(X,Y) \in \mathbb{R}]  = \int_R \int f(x,y) dxdy$
    \begin{itemize}
        \item $f(x,y) \ge 0$
        \item $\int_{-\infty}^\infty \int_{-\infty} ^\infty f(x,y) dxdy = 1$
    \end{itemize}
    \item Suppose the joint pdf of two continuous random variables $X,Y$ can be written as $f(x,y)=cxy, 0 < y < x < 1$
    \begin{itemize}
        \item $\int_{-\infty}^\infty \int_{-\infty} ^\infty f(x,y) dxdy =  \int_0^1 \int_y^1 cxy dxdy = c \int_0^1 y[\frac{x^2}{2}]_y^1 dy = c \int_0^1 \frac{1}{2} y - \frac{y^3}{2} dy = c [\frac{y^2}{4} - \frac{y^4}{8}]_0^1 = [\frac{1}{4}-\frac{1}{8}] c = 1 \Rightarrow c = 8$
    \end{itemize}
    \item Marginal PDF (Continuous) - for continuous random variables $X,Y$, $f(x) = \int_{-\infty}^\infty f(x,y)dy, f(y) = \int_{-\infty}^\infty f(x,y)dx$
    \item Suppose two conitnuous random variables, $X,Y$ with joint uniform pdf $f(x,y)=\frac{1}{6}, 0 \le x \le 3, 0 \le y \le 2$. What is marginal pdf $f(x)$?
    \begin{itemize}
        \item $f(x)=\int_{-\infty}^\infty f(x,y)dy = \int_0^2 \frac{1}{6} dy = \left[ \frac{1}{6} y \right]_0^2 = \frac{1}{3}$
    \end{itemize}
    \item Joint Cumulative Distribution Function (CDF) - for random variables $X,Y$, $F(x,y) = P(X \le x, Y \le y)$
    \item Conditional Probability Distribution - probability that $Y$ takes on the value $y$ given that $X=x$ is $p_{Y \mid X} (y) = P(Y=y,X=x) = \frac{p_{X,Y}(x,y)}{p_X(x)}, p_X(x) > 0$
    \item Conditional Probability Distribution Function (Continuous) - $f_{Y \mid X}(y) = \frac{f_{X,Y}(x,y)}
    {f_X(x)}, f_X(x) > 0$
    \begin{itemize}
        \item $f_{Y \mid X}(y) \ge 0, p_{Y \mid X}(y) \ge 0$\
        \item $\int_{-\infty}^\infty f_{Y \mid X}(y) dy = 1, \sum_y p_{Y \mid X}(y) = 1$
    \end{itemize}
    \item Let $X$ and $Y$ be continuous random variables with joint pdf $f_{X,Y}(x,y) = \left\{ \begin{array}{cll}
        \frac{1}{8} (6-x-y), & 0 \le x \le 2, & 2 \le y \le 4 \\
        0, & \text{elsewhere}
    \end{array} \right. $. What is $P(2 < Y < 3 \mid X = 1)$?
    \begin{itemize}
        \item $P(2 < Y < 3 \mid X = 1) = \int_2^3 \frac{f_{X,Y}(1,y)}{f_X(1,\cdot)} dy = \int_2^3 \frac{\frac{1}{8}(6-1-y)}{\int_2^4 \frac{1}{8} (6-1-y)dy}dy = \int_2^3 \frac{\frac{1}{8}(5-y)}{\frac{1}{8} [5y-\frac{y^2}{2}]_2^4}dy = \frac{1}{4} \int_2^3 (5-y)dy = \frac{1}{4} [5y-\frac{y^2}{2}]_2^3 = \frac{1}{4} [\frac{5}{2}] = \frac{5}{8}$
        \item Another way: 
        \begin{flalign*}
            f_{Y \mid X}(y) &= \frac{f_{X,Y}(x,y)}{f_X(x)}=\frac{\frac{1}{8}(6-x-y)}{\frac{1}{8}(6-2x)}= \frac{6-x-y}{6-2x}, 0 \le x \le 2, 2 \le y \le 4 \\
            f_X(x) &= \int_2^4 \frac{1}{8} (6-x-y)dy = \frac{1}{8} \left[ 6y-xy-\frac{y^2}{2} \right]_2^4 = \frac{1}{8} (6-2x), 0 \le x \le 2 \\
            \text{When } x=1, &f_{Y \mid X} (y) = \frac{5-y}{4} \\
            P(2 < Y < 3 \mid X = 1) &= \int_2^3 \frac{5-y}{4} dy = \frac{5}{8}
        \end{flalign*}
    \end{itemize}
    \item Independence of Random Variables - for discrete random variables $X,Y$, $p_{X,Y}(x,y)=p_X(x)p_Y(y)$ or $p_{Y \mid X}(y)=p_Y(y)$ or $p_{X \mid Y} (x) = p_X(x),  \forall x,y,p_X(x) > 0$
    \begin{itemize}
        \item $p(x_1,x_2, \cdots, x_n) = p(x_1)p(x_2) \cdots p(x_n)$
        \item For continuous case, replace $p(\cdot)$ with $f(\cdot)$ in the above definitions
    \end{itemize}
    \item Independent and Identically Distributed (i.i.d.) sample - set of $n$ independent random variables, all having the same pdf
    \item Coin is tossed $n$ times
    \begin{itemize}
        \item Assume the tosses are independent
        \item The probability of head on each toss is $p$
        \item For $i \in \{1,\dots, n\},$ let $X_i$ be a random variable that equals 1 if the $i$th coin toss yields a head, and $0$ otherwise
        \item $X_i \sim Bernoulli(p)$ for each $i$
        \item $X_1,X_2,\dots, X_n$ are i.i.d.
        \item We are interested in the total number of heads in $n$ tosses, $Y = \sum_{i=1}^n X_i$, which is itself a discrete random variable with possible values $Y = \{0,1, \cdots, n\}$
        \item Since $x_1, \dots, x_n \in \{0,1\}$ and $\sum_{i=1}^n x_i = y, x_1, \dots, x_n$ is one sequence that contains exactly $y$ ones and $n-y$ zeros
        \item By independence, the probability of observing this one sequence is $P(X_1=x_1, X_2=x_2, \cdots, X_n = x_n) = P(X_1=x_1)P(X_2=x_2) \cdots P(X_n = x_n)$
        \item Since $P(X_i = 1) = p$ and $P(X_i = 0) = 1 - p$, $P(X_1=x_1)P(X_2=X_2)\cdots P(X_n=x_n) = p^y (1-p)^{n-y}$
        \item To compute $P(Y=y)=P(\sum_{i=1}^n X_i = y) = P\left( \bigcup_{\sum_{i=1}^n X_i=y} \{X_1=x_1, X_2=x_2, \cdots, X_n=x_n\} \right)$
        \item Since the different ways of observing $y$ heads in $n$ tosses are mutually exclusive,
        $P(Y=y)=\sum_{\sum_{i=1}^n X_i =y} P(X_1=x_1, X_2=x_2, \cdots, X_n=x_n) = \sum_{\sum_{i=1}^n X_i = y} p^y (1-p)^{n-y}$
        \item There are $\binom{n}{y}$ different ways of observing $y$ heads, so $P(Y=y)=\binom{n}{y} p^y (1-p)^{n-y}$
    \end{itemize}
\end{itemize}

\section{Lecture 7 (13/11/2025)}
\subsection{Slide 7}
\begin{itemize}
    \item For functions of multivariate random variables, their expectations have very similar expressions to the univariate random variable case
    \begin{itemize}
        \item $X$ and $Y$ are discrete random variables with joint pdf $p_{X,Y}(x,y)$ and $g(X,Y)$ is a function of $X$ and $Y$
        \[
        E[g(X,Y)] = \sum_x \sum_y g(x,y) p_{X,Y}(x,y)
        \]
        where $\sum_x \sum_y \vert g(x,y) \vert p_{X,Y}(x,y) < \infty$
        \item $X$ and $Y$ are continuous random variables with joint pdf $p_{X,Y}(x,y)$ and $g(X,Y)$ is a function of $X$ and $Y$
        \[
        E[g(X,Y)] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y) f_{X,Y}(x,y) dxdy
        \]
        where $\int_{-\infty}^\infty \int_{-\infty}^\infty \vert g(x,y) \vert f_{X,Y}(x,y) dxdy < \infty$
    \end{itemize}
    \item A supermarket has two express check-out lines. Let $X$ and $Y$ denote the number of customers in the first and in the second, respectively, at any given time. The joint pdf of $X$ and $Y$ is summarized by the following table:
    \[
    \begin{array}{cc|cccc}
         & & & & X & \\
         & & 0 & 1 & 2 & 3 \\
         \hline
         Y & 0 & 0.1 & 0.2 & 0 & 0 \\
         & 1 & 0.2 & 0.3 & 0.125 & 0.075
    \end{array}
    \]
    \item What is the expected value of $g(X,Y) = 3X - 2XY + Y$?
    \begin{flalign*}
        E[g(X,Y)] &= \sum_{x=0}^3 \sum_{y=0}^1 (3x - 2xy + y) p_{X,Y}(x,y) \\
        &= 0 + 0.2 \cdot 3 + 0 + 0 + 0.2 \cdot 1 + 0.3 \cdot 2 + 0.125 \cdot 3 + 0.075 \cdot 4 \\
        &= 0.6 + 0.2 + 0.6 + 0.375 + 0.3 \\
        &= 2.075
    \end{flalign*}
    \item Let $X$ and $Y$ be any two random variables, discrete or continuous, dependent or indpeendent, and let $a$ and $b$ be two constants. Then
    \[
    E[aX+bY] = aE[X] + bE[Y]
    \]
    where $E[X]$ and $E[Y]$ are both finite
    \[
    E[\sum_{i=1}^n a_iX_i] = \sum_{i=1}^n a_i E[X_i]
    \]
    \item Prove this for the continuous case:
    \begin{flalign*}
        E[aX+bY] &= \int_{-\infty}^\infty \int_{-\infty}^\infty (ax+by)f_{X,Y}(x,y)dxdy \\
        &= a \int_{-\infty}^\infty \int_{-\infty}^\infty xf_{X,Y}(x,y)dxdy + b \int_{-\infty}^\infty \int_{-\infty}^\infty yf_{X,Y}(x,y)dxdy \\
        &= a \int_{-\infty}^\infty x \left[ \int_{-\infty}^\infty f_{X,Y}(x,y)dy \right] dx + b \int_{-\infty}^\infty y \left[ \int_{-\infty}^\infty f_{X,Y}(x,y)dx \right] dy \\
        &= a \int_{-\infty}^\infty xf_{X}(x) dx + b \int_{-\infty}^\infty yf_{Y}(y)dy \\
        &= a E[X] + bE[Y]
    \end{flalign*}
    \item Revisiting binomial example: $Y = \sum_{i=1}^n X_i$, where $X_1, X_2, \dots, X_n$ and i.i.d., $X_i \sim Bernoulli(p)$
    \begin{itemize}
        \item We know $E[X_i] = 1 \cdot p + 0 \cdot (1-p) = p$
        \item Since $X_i$s are identically distributed,
        \[
        E[Y] = E[\sum_{i=1}^n X_i] = \sum_{i=1}^n E[X_i] = np
        \]
        \item Note we did not use the independent condition
    \end{itemize}
    \item Unlike the sum of random variables, there is no general formula for computing the expectation of the product of random variables (apart from treating it as a function of random variables)
    \begin{itemize}
        \item If $X,Y$ are independent,
        \[
        E[XY] = E[X]E[Y]
        \]
        where $E[X]$ and $E[Y]$ are finite
        \item More generally, if $X,Y$ are independent,
        \[
        E[g(X)h(Y)] = E[g(X)] \cdot E[h(Y)]
        \]
    \end{itemize}
    \begin{flalign*}
        E[XY] &= \int_{-\infty}^\infty \int_{-\infty}^\infty xyf_{X,Y}(x,y) dxdy \\
        &= \int_{-\infty}^\infty \int_{-\infty}^\infty xyf_X(x)f_Y(y) dxdy \quad f_{X,Y}(x,y) = f_X(x)\cdot f_Y(y) \leftarrow \text{Independence Condition} \\
        &= \int_{-\infty}^\infty xf_X(x) \left[ \int_{-\infty}^\infty yf_Y(y) dy \right] dx \\
        &= \int_{-\infty}^\infty xf_X(x) E[Y] dx \\
        &= E[Y] \int_{-\infty}^\infty xf_X(x) dx \\
        &= E[X] \cdot E[Y]
    \end{flalign*}
    \item We need a measure of the relationship between two random variables
    \begin{itemize}
        \item If two random variables are not independent, we would like to measure how strong their association is
        \item Covariance and correlation is one way to measure association and is also a key part of the motivation and interpretation of regression analysis and many other statistical analyses
        \item $X$ and $Y$ are random variables with finite variances, the covariance of $X$ and $Y$ is
        \[
        Cov(X,Y) = E[(X-E[X])(Y-E[Y])] = E[XY] - E[X]E[Y]
        \]
    \end{itemize}
    \begin{flalign*}
        E[(X-E[X])(Y-E[Y])] &= E[XY - E[X] \cdot Y - E[Y] \cdot X + E[X] \cdot E[Y]] \\
        &= E[XY] - E[X]E[Y] - E[Y]E[X] + E[X]E[Y] \\
        &= E[XY] - E[X]E[Y]
    \end{flalign*}
    \item Properties of Covariance
    \begin{itemize}
        \item If $X$ is a random variable and $a$ is a constant,
        \[
        Cov(a,X) = E[aX] - E[a]E[X] = aE[X] - aE[X] = 0
        \]
        \item If $X,Y$ are random variables,
        \[
        Cov(X,Y) = Cov(Y,X)
        \]
        \item If $X,Y,Z$ are random variables, and $a,b$ are constants,
        \[
        Cov(X,aY+
        bZ) = aCov(X,Y) + bCov(X,Z)
        \]
        \begin{flalign*}
            Cov(X,aY+bZ) &= E[X(aY+bZ)] - E[X]E[aY+bZ] \\
            &= aE[XY] + bE[XZ] - aE[X]E[Y] - bE[X]E[Z] \\
            &= a Cov(X,Y) + bCov(X,Z)
        \end{flalign*}
        \begin{itemize}
            \item This property reveals one disadvantage of covariance measure: it is not "scale-free"
        \end{itemize}
        \item If $X,Y$ are independent, $Cov(X,Y) = E[XY] - E[X]E[Y] = E[X]E[Y] - E[X]E[Y] = 0$
        \item Note $Cov(X,Y) = 0 \centernot \Longleftrightarrow X,Y$ are independent
        \begin{itemize}
            \item Example: $Y = X^2, X \sim U(-1,1), Cov(X,Y) = 0$
            \item $X$ and $Y$ are not independent
        \end{itemize}
        \item For random variables $X$ and $Y$, their correlation is defined as
        \[
        Corr(X,Y) = \frac{Cov(X,Y)}{\sqrt{Var(X)Var(Y)}}
        \]
        \begin{itemize}
            \item We can see from the above equation that correlation is the "scale-free" version of covariance: it is covariance scaled by the standard deviation of $X$ and the standard deviation of $Y$ (always between +1 and -1)
            \begin{flalign*}
                Corr(aX+b,cY+d) &= \frac{Cov(aX+b,cY+d)}{\sqrt{Var(aX+b)}\sqrt{Var(cY+d})} \\
                &= \frac{acCov(X,Y)}{a\sigma_{X} c \sigma_{Y}} \\
                &=\frac{Cov(X,Y)}{\sigma_X \sigma_Y} \\
                &= Corr(X,Y)
            \end{flalign*}
        \end{itemize}
        \item If $Y=aX+b$, $Corr(X,Y) = \left\{ \begin{array}{cc}
            -1 & a>0 \\
            1 & a<0
        \end{array} \right.$
    \end{itemize}
    \item $X$ and $Y$ are random variables with finite variances,
    \[
    Var(aX+bY) = a^2Var(X) + b^2Var(Y) + 2abCov(X,Y)
    \]
    \item For univariate random variables, $Var(aX)=a^2Var(X)$
    \begin{flalign*}
        Var(aX+bY) &= E[(aX+bY)^2] - E[aX+bY]^2 \\
        &= E[a^2X^2 + b^2Y^2 + 2abXY] - (aE[X] + bE[Y])^2 \\
        &= a^2E[X^2] + b^2 E[Y^2] + 2abE[XY] - a^2E[X]^2 - b^2E[Y]^2 -2abE[X]E[Y] \\
        &= a^2(E[X^2] - E[X]^2) + b^2(E[Y^2] - E[Y]^2) +2ab(E[XY] - E[X]E[Y]) \\
        &= a^2 Var(X) + b^2 Var(Y) + 2abCov(X,Y)
    \end{flalign*}
    \begin{itemize}
        \item When variables move together (positive covariance), total variance increases
        \item When variables offset each other (negative covariance), total variance decreases
    \end{itemize}
    \item If $X_1,X_2, \dots, X_n$ are random variables with finite variances
    \[
    Var\left( \sum_{i=1}^n a_iX_i \right) = \sum_{i=1}^n a_i^2Var(X_i) + 2\sum_{i<j} a_ia_jCov(X_i,X_j)
    \]
    \item If $X_1,X_2,\dots,X_n$ are independent random variables with finite variances, $Var \left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i)$
    \item Revisiting our binomial example: $Y = \sum_{i=1}^n X_i$, where $X_1,X_2, \dots, X_n$ are i.i.d., $X_i \sim Bernoulli(p)$
    \begin{itemize}
        \item We know $E[X_i] = 1 \cdot p + 0 \cdot (1-p) = p$
        \item We can compute $E[X_i]^2 = 1 \cdot p + 0 \cdot (1-p)$, so $Var(X_i) = E(X_i^2) - [E(X_i)]^2 = p - p^2 = p(1-p)$
        \item Since $X_1,X_2, \dots, X_n$ are i.i.d.,
        \[
        Var(Y) = Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n Var(X_i) = np(1-p)
        \]
        \item If the variables are only independent and not identically distributed,
        \[
        Var(Y) = Var\left( \sum_{i=1}^n X_i \right) = \sum_{i=1}^n p_i(1-p_i)
        \]
    \end{itemize}
    \item $X$ is normally distributed with mean $\mu$ and variance $\sigma^2$, or $X \sim N(\mu,\sigma^2)$
    \begin{itemize}
        \item The pdf of $X$ is $f(x)=\frac{1}{(2\pi \sigma^2)^{\frac{1}{2}}}e^{-\frac{1}{2}\frac{(x-\mu)^2}{\sigma^2}}$
        \item We often need to transform $X$ into a variable with standard normal distribution, $\frac{X-\mu}{\sigma} \sim N(0,1)$
        \begin{itemize}
            \item $E\left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma} E[X-\mu] = \frac{1}{\sigma} [E[X] - \mu] = 0$
            \item $Var\left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma^2}Var(X) = 1$
        \end{itemize}
    \end{itemize}
    \item Suppose $X_1,X_2,$ are normally distributed random variables with $X_i \sim N(\mu_i,\sigma_i^2)$ for $i=1,2,\dots,n$ and $a_1,a_2,\dots,a_n$ and $b$ are constants, then
    \[
    \sum_{i=1}^n a_iX_i + b \sim N(\mu,\sigma^2), \quad \mu = \sum_{i=1}^n a_i \mu_i + b, \quad \sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2 + 2\sum_{i<j} a_ia_jCov(X_i,X_j)
    \]
    \item If $X_i$s are independent or just uncorrelated, the variance simplifies to $\sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2$
\end{itemize}

\section{Tutorial 5 (14/11/2025)}
\begin{itemize}
    \item Tips:
    \begin{itemize}
        \item Understand the logic
        \item Recognize particular patterns and use them
        \item Understanding and processing the question is time well spent
        \begin{itemize}
            \item Like going to war without knowing who you are fighting
        \end{itemize}
    \end{itemize}
    \item Ten fair dice are rolled: four blue and six yellow. Write down numerical expression for the following probabilities. If your answer involves factorials, binomial coefficients, or fractions, you do not need to calculate them.
    \begin{itemize}
        \item The probability exactly one blue die shows a six, and exactly two yellow dice show even numbers: $\binom{4}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)^3 \binom{6}{2} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^4$
        \begin{itemize}
            \item Probability that two die are rolled and first shows a six while the second doesn't: $\left( \frac{1}{6} \right) \left( \frac{5}{6} \right)$
            \item Probability that two die are rolled and second shows a six while the first doesn't: $\left( \frac{5}{6} \right) \left( \frac{1}{6} \right)$
            \item Probability that two die are rolled and one shows a six: $\binom{2}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)$
            \item Probability that any one of four blue die rolled shows a six:
            $\binom{4}{1} \left( \frac{1}{6} \right) \left( \frac{5}{6} \right)^3$
        \end{itemize}
        \begin{itemize}
            \item Probability that two die are rolled and first shows a even number and the second doesn't:
             $\left( \frac{1}{2} \right) \left( \frac{1}{2} \right)$
             \item Probability that two die are rolled and one shows a even number: $\binom{2}{1} \left( \frac{1}{2} \right) \left( \frac{1}{2} \right)$
             \item Probability that exactly two yellow die show even numbers: $\binom{6}{2} \left( \frac{1}{2} \right)^2 \left( \frac{1}{2} \right)^4$
        \end{itemize}
        \item The probability that exactly three of the ten dice show a six, regardless of color: $\binom{10}{3} \left( \frac{1}{6} \right)^3 \left( \frac{5}{6} \right)^7$
    \end{itemize}
    \item A box contains 998 black balls and 2 white balls. Let $X$ be the number of white balls in 500 random draws with replacement from this box. Calculate the probability that $X$ equals to 1 given we already know $X$ is either 1 or 2
    \begin{itemize}
        \item White ball is a success (counting total number of sucessess)
        \item $p=\frac{2}{1000}$
        \item $n=500$
        \item Apply Binomial: $P(X=1) = \binom{500}{1} \left( \frac{2}{1000} \right)^1 \left( \frac{998}{1000} \right)^{2-1}$
        \item $P(X=2) = \binom{500}{2} \left( \frac{2}{1000} \right)^2 \left( \frac{998}{1000} \right)^{2-2}$
        \item $P(X=1 \mid X=1 \cup X=2) = \frac{P(X=1 \cap \left( X=1 \cup X=2 \right))}{P(X=1 \cup X=2)} = \frac{P(X=1)}{P(X=1)+P(X=2)}$
        \begin{itemize}
            \item If $A \subseteq B$, then $P(A \cap B) = P(A)$
        \end{itemize}
        \begin{itemize}
            \item $\frac{P(X=1)}{P(X=2)} = \frac{\binom{n}{1} p (1-p)^{n-1}}{\binom{n}{2} p^2 (1-p)^{n-2}}= \frac{\frac{n!}{(n-1)!} (1-p)}{\frac{n!}{(n-2)!2!} p} = \frac{2(1-p)}{(n-1)p}$
        \end{itemize}
        \item $\frac{\frac{P(X=1)}{P(X=2)}}{\frac{P(X=1)}{{(X=2)}} + 1} = \frac{\frac{2(1-p)}{(n-1)p}}{\frac{2(1-p)}{(n-1)p}+1} = \frac{\frac{2(1-p)}{(n-1)p}}{\frac{2(1-p)+(n-1)p}{(n-1)p}}=\frac{2-2p}{2-2p+np-p}$
    \end{itemize}
    \item A random variable $X$ takes value $0,1,2$ with probabilities proportional to $1,2,3$. Find $E[X^2+1]$
    \begin{itemize}
        \item $X^2+1= \left\{ \begin{array}{cc}
            0^2+1 = 1 & P(1)=\frac{1}{6} \\
            1^2 + 1 = 2 & P(2) = \frac{1}{3} \\
            2^2 + 1 = 5 & P(2) = \frac{1}{2}
        \end{array} \right.$
        \item $E[X^2 + 1] = \frac{1}{6} \cdot 1 + \frac{1}{3} \cdot 2 + \frac{1}{2} \cdot 5 = \frac{1}{6} + \frac{2}{3} + \frac{5}{2} = \frac{1+4+15}{6} = \frac{20}{6} = \frac{10}{3}$
    \end{itemize}
    \item e
    \begin{itemize}
        \item Find $k$
        \item 
    \end{itemize}
    \item A factory has three suppliers: $A,B,C$. Supplier $A$ provides 50% in all 
\end{itemize}

\section{Lecture 8 (17/11/2025)}
\begin{itemize}
    \item Normal Distribution - Mean $\mu$ and variance $\sigma^2$ ($X \sim \mathcal{N} (\mu, \sigma^2)$
    \begin{itemize}
        \item Pdf of $X$ is $f(x)=\frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{1}{2} \frac{(x-\mu)^2}{\sigma^2}}$
        \item Often normalize $X$ by transforming $\frac{X - \mu}{\sigma} \sim \mathcal{N}(0,1)$
        \[
        E \left[ \frac{X - \mu}{\sigma} \right] = \frac{1}{\sigma} E[X - \mu] = \frac{1}{\sigma} [E(X) - \mu] = 0
        \]
        \[
        Var \left[ \frac{X-\mu}{\sigma} \right] = \frac{1}{\sigma^2} Var(X) = 1
        \]
    \end{itemize}
    \item Suppose $X_1, X_2, \dots, X_n$ are normally distribute random variables, with $X_i \sim \mathcal{N}(\mu_i,\sigma_i^2)$ for $i=1,2,\dots, n$ and $a_1, a_2, \dots, a_n$ and $b$ are constants, then
    \begin{flalign*}
        \sum_{i=1}^n a_i X_i + b \sim \mathcal{N}(\mu,\sigma^2) \\
        \mu = \sum_{i=1}^n a_i \mu_i + b, \quad \sigma^2 = \sum_{i=1}^n a_i^2 \sigma_i^2 + 2 \sum_{i < j} a_i a_j Cov(X_i,X_j) 
    \end{flalign*}
    \item Suppose that in the population of English people aged 16 or over:
    \begin{itemize}
        \item the heights of men (in cm) follow a normal distribution with mean 174.9 and standard deviation 7.39
        \item the heights of women (in cm) follow a normal distribution with mean 161.3 and standard deviation 6.85
    \end{itemize}
    \item Suppose we select one man and one woman at random and independently of each other. Denote the man’s height by X and the woman’s height by Y. What is the probability that the man is at most 10 cm taller than the woman?
    \begin{itemize}
        \item $w = x - y$
        \item $\mu_w = (1)174.9 + (-1) 161.3 = 13.6$
        \item $\sigma_w^2 = (1)^2 7.39^2 + (-1)^2 6.85^2 = 10.08^2$
        \item $p(w \le 10) = p\left(\frac{w - 13.6}{10.08} \le \frac{10-13.6}{10.08}\right) = p(z \le -0.36)$
        \begin{itemize}
            \item $z= \frac{w - 13.6}{10.08^2} \sim \mathcal{N}(0,1)$
        \end{itemize}
    \end{itemize}
    \item Conditional Expectation - conditional expectation of $Y$ given $X=x$ is defined as
    \[
    E[Y \mid X = x] = \left\{ \begin{array}{cc}
        \sum_{\text{all}} y \cdot p_{Y \mid X} (y) & Y \text{ is discrete} \\
        \sum_{-\infty}^\infty y \cdot f_{Y \mid X} (y) dy & Y \text{ is continuous}
    \end{array} \\
    \right.
    \]
    \item We can interpret it as the average value of $Y$ given $X=x$
    \item $E[Y \mid X=x]$ also provides us with a useful way of summarizing the relationship, or dependence, between $X$ and $Y$
    \begin{itemize}
        \item If $Y$ tends to take on higher values when $X$ also takes on higher values, the mapping $x$ to $E[Y \mid X=x]$ will trace out an increasing function
        \item $E[Y \mid X=x]$ can also be shown to give the best possible prediction that we can make about $Y$, using (only) the information that $X=x$
    \end{itemize}
    \item Conditional Expectation Properties
    \begin{itemize}
        \item If $X$ and $Y$ are independent, $E[Y \mid X=x] = E[Y]$
        \item Conditional expectation has similar property as unconditional expectation when it comes to linear functions of random variables:
        \[ E[aY+bZ + c \mid X=x] = aE[Y \mid X=x] + bE[Z \mid X=x] +c
        \]
        \item Law of iterated expectations - $E[E[Y \mid X] = E[Y]$, where the inner expectation is taken over $Y$ conditional on $X=x$, the outer expectation is taken over $X$
        \begin{itemize}
            \item If we make our best guess of $Y$ when $X=x$ and average over all possible values of $X$, then we recove the guess we would have make about $Y$, which is $E[Y]$
        \end{itemize}
    \end{itemize}
\end{itemize}
\subsection{Slide 8}
\begin{itemize}
    \item Toss a coin $n$ times where each toss has probability $p$ of yielding a head, and we derived the total number of heads $Y \sim Binomial(n,p)$
    \begin{itemize}
        \item In statistics, we invert the problem: we might have data on the total, number of successes from independent experimental trials and we would like to use this data to learn about $p$ (can we estimate $p$?)
    \end{itemize}
    \item Statistics is the science of using data to learn about the world
    \begin{itemize}
        \item How can we come up with a good estimator? What properties does the estimator have? What properties do we want the estimated have
    \end{itemize}
    \item Suppose we are studying the impact of current inflation on people's spending patterns in the UK
    \begin{itemize}
        \item To conduct the analysis, we need to factor in individual's income level and perhaps estimate the income distribution
        \item The size of the UK population is about 67 million
        \item One reasonable thing to do is to draw a random sample of size $n$ people, and collect data about their income $x_1,x_2, \dots, x_n$
        \item We typically assume $X_1, X_2, \dots, X_n$ are i.i.d. random variables
        \begin{itemize}
            \item Why?
        \end{itemize}
        \item For the first observation or data point in our sample, we randomly select an individual to record their income level
        \item There is randomness to who the particular individual we select is, therefore we treat $X_1$ as a random variable
        \item After the first observation has been selected, there is no more randomness, and $X_1=x_1$ is the realized value of $X_1$
        \begin{itemize}
            \item Data are realizations of random variables drawn from an underlying population distribution
        \end{itemize}
        \item $X_1,X_2,\dots, X_n$ are i.i.d. for the following reasons
        \begin{itemize}
            \item Because $X_1, X_2, \dots, X_n$ are randomly drawn from the same population, the marginal distribution of $X_i$ is the same for $i=1,2,\dots,n$
            \item Under simple random sampling, knowing the value of $X_1$ provides no information about $X_2$
        \end{itemize}
    \end{itemize}
    \item Suppose $X_1, X_2, \dots, X_n$ are i.i.d. with mean $\mu$ and variance $\sigma^2$
    \begin{itemize}
        \item $\mu, \sigma^2$ are the unknown parameters we are interested in estimating using the data
        \item If $X$ is continuous, $\mu = \int_{-\infty}^\infty x \cdot f(x)dx$
        \item Sample mean $\hat{u} = \frac{1}{n} \sum_{i=1}^n X_i$
    \end{itemize}
    \item More generally, an estimator $\hat{\theta}$ (for unknown parameter $\theta$) is a function of the random sample $X_1, X_2, \dots, X_n$ with distribution $f(x_1, x_2, \dots, x_n; \theta)$
    \begin{itemize}
        \item $\hat{u} = \frac{1}{n} \sum_{i=1}^n X_i$ is an estimator for $\mu = \int_{-\infty}^\infty x \cdot f(x)dx$ 
        \item Here we are looking at the first moment, but moment estimator can be constructed for higher order moments, covariance of two random variables, etc.
    \end{itemize}
    \item Since $\hat{\theta}$ is a random variable itself, it is reasonable to consider $E[\hat{\theta}] =\theta$ a desirable property
    \begin{itemize}
        \item If $\hat{\theta}$ is evaluated many times with many samples, on average, we would get the right answer
    \end{itemize}
    \item Formally, if $E[\hat{\theta}]=\theta$, $\hat{\theta}$ is an unbiased estimator
    \item The unbiasedness of an estimator does not depend on the size of the sample $n$
    \item $\tilde{\mu} = X_1$ is an estimator of $\mu$ and it is unbiased because $E[X_1] = \mu$. Is $\tilde{\mu}$ a desirable estimator?
    \begin{itemize}
        \item Throwing away $X_2, \dots, X_n$ observations
        \item Variance increases with less observations
        \item Perhaps unbiasedness alone is not sufficient for an estimator to be desireable
    \end{itemize}
    \item We might also want to compute the variance of the estimator, which tells us how reliable the estimator is
    \begin{itemize}
        \item If $\hat{\theta}$ and $\tilde{\theta}$ are two estimators for $\theta$, and $\tilde{\theta}$ has a smaller variance than $\tilde{\theta}$, then $\hat{\theta}$ is said to be more efficient than $\tilde{\theta}$
        \item $Var(\hat{\mu}) = Var\left( \frac{1}{n} \sum_{i=1}^n X_i \right)= \frac{1}{n^2} \sum_{i=1}^n \sigma^2 = \frac{\sigma^2}{n} < \sigma^2 = Var(X_1) = Var(\tilde{\mu})$
        \item Standard Error - the estimated standard deviation of the estimator
    \end{itemize}
    \item Consider a third estimator for $\mu$: $\overline{\mu} = \frac{1}{n} (\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n)$
    \begin{itemize}
        \item For convenience, assume $n$ is even, then $E[\overline{\mu}] = E[\frac{1}{n} (\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n)] = \frac{1}{n} \frac{1}{2} E[X_1] + \frac{3}{2} E[X_2] + \frac{1}{2} E[X_3] + \frac{3}{2} E[X_4] + \cdots + \frac{1}{2} E[X_{n-1}] + \frac{3}{2} E[X_n]) = \frac{1}{n} \left( \frac{1}{2}\frac{3}{2} \mu + \frac{1}{2} \frac{1}{2} \mu \right)  = \mu$
        \item $Var(\overline{\mu})= \frac{1}{n^2} Var(\frac{1}{2} X_1 + \frac{3}{2} X_2 + \frac{1}{2} X_3 + \frac{3}{2} X_4 + \cdots + \frac{1}{2} X_{n-1} + \frac{3}{2} X_n) \\ = \frac{1}{n^2} \left( \frac{1}{4} Var[X_1] + \frac{9}{4} Var[X_2] + \cdots + \frac{1}{4} Var[X_{n-1}] + \frac{9}{4} Var[X_n] \right) = \frac{1}{n^2} \frac{5}{2} \frac{n}{2}z \sigma^2 = 1.25 \frac{\sigma^2}{n} > Var(\hat{\mu})$
    \end{itemize}
    \item In the class of unbiased estimators which can be written as $\frac{1}{n} \sum_{i=1}^n a_iX_i$, where $a_i$s are constants, $\hat{\mu}$ has the smallest variance, and is called the best linear unbiased estimator (BLUE)
    \item How do we compare two estimators if one is biased and the other is not?
    \begin{itemize}
        \item We can use mean squared error: $MSE(\hat{\theta}) =  E[(\hat{\theta} - \theta)^2]$
        \item $MSE(\hat{\theta}) = \left[ bias(\hat{\theta}) \right]^2 + Var(\hat{\theta}^2)$ where $bias(\hat{\theta}) = E[ \hat{\theta}] - \theta$
        \begin{flalign*}
            MSE(\hat{\theta}) &=  E[(\hat{\theta} - \theta)^2] \\
            &= E[(\hat{\theta} - E[\hat{\theta}] + E[\hat{\theta}] - \theta)^2] \\
            &= E[(\hat{\theta} - E[\hat{\theta}])^2 + 2(\hat{\theta} - E[\hat{\theta}])(E[\hat{\theta}] - \hat{\theta}) + (E[\hat{\theta}] - \theta)^2] \\
            &= \underset{Var(\hat{\theta})^2}{\underbrace{[(\hat{\theta} - E[\hat{\theta}])^2}} + \underset{2(E[\hat{\theta}] - \theta)(E[\hat{\theta}] - E[\hat{\theta}]) = 0}{\underbrace{2E[(\hat{\theta} - E[\hat{\theta}])(E[\hat{\theta}] - \hat{\theta})]}} + \underset{bias(\hat{\theta})^2}{\underbrace{(E[\hat{\theta}] - \theta)^2}} \\
            &= \left[ bias(\hat{\theta}) \right]^2 + Var(\hat{\theta}^2)
        \end{flalign*}
    \end{itemize}
    \item For an estimator to have low MSE, both bias and variance has to be small
    \begin{itemize}
        \item For many estimators, there is often a bias-variance tradeoff
        \item For this reason, it is more meaningful to discuss efficiency for tow unbiased estimators
    \end{itemize}
    \item So far we have discussed properties of estimators regardless of the sample size $n$
    \begin{itemize}
        \item In practice, we know it is often the case that the larger the sample size is, the more reliable our evidence is
        \item Large sample properties provide the mathematical reasoning behind such intuition
        \item In addition, as we typically do not have knowledge about the distribution of even a simple estimator $\hat{\mu} = \frac{1}{n} \sum_{i=1}^n X_i$, unless $X_i$s are all normally distributed, we need some tools to analyze the distribution of our estimators
        \item Large sample properties provide us with the tool: analyze the distribution of our estimators when $n$ is large
    \end{itemize}
\end{itemize}

\section{Tutorial 6 (21/11/2025)}
\begin{itemize}
    \item Remember that $f_X \cdot f_Y = f_{X,Y}$ if $X$ and $Y$ are independent
    \begin{itemize}
        \item $f_X(x,y) = \int_{-\infty}^\infty f_{X,Y}(x,y) dy$
        \item $f_Y(x,y) = \int_{-\infty}^\infty f_{X,Y}(x,y) dx$
    \end{itemize}
\end{itemize}

\section{Lecture 9 (24/11/2025)}
\begin{itemize}
    \item A sequence of random variables $Y_1, Y_2, \dots, Y_n$ converges in probability to a constant $c$, denoted $Y_n \overset{p}{\rightarrow} c$, if for any small $\epsilon > 0$,
    \[
    P(\vert Y_n - c \vert > \epsilon) \rightarrow 0
    \]
    as $n \rightarrow \infty$
    \begin{itemize}
        \item As $n$ increases, the distribution of $Y_n$ collapses to a constant $c$, or grows more and more concentrated around the constant $c$
        \begin{itemize}
            \item Probability that $c - \epsilon \le Y_n \le c + \epsilon$ is close to 1
        \end{itemize}
        \item $\hat{\theta}$ is a consistent estimator for $\theta$, if $\hat{\theta} \overset{p}{\rightarrow} \theta$ (as sample size $n \rightarrow \infty$)
    \end{itemize}
    \item Law of Large Numbers (LLN) - suppose $X_1, X_2, \dots, X_n$ are i.i.d. with $E(X_i) = \mu$ and $Var(X_i) < \infty$, then
    \[
    \overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i = \hat{\mu} \overset{p}{\rightarrow} \mu
    \]
    \begin{itemize}
        \item If the data is collected by random sampling, the i.i.d. assumption is likely to hold
        \item The finite variance assumptions can be interpreted as outliers are unlikely or observed infrequently
        \item Intuition: as we collect more and more data, our estimators constructed using sample average gets more and more concentrated around the true parameter value
    \end{itemize}
    \item Chebyshev's Inequality - $P(\vert X - \mu \vert \ge k \sigma) \le \frac{1}{k^2}$
    \item Applies to any random variable that takes on positive values
    \begin{flalign*}
        P(x > \epsilon) &\le \frac{E[X]}{\epsilon}, \quad x >0, \epsilon>0 \\
        E[x] &= \int_0^\epsilon xf(x)dx + \int_\epsilon^\infty xf(x)dx \\
        &\ge \int_\epsilon^\infty xf(x)dx, \quad x \ge \epsilon > 0 \\
        &\ge \epsilon \int_\epsilon^\infty f(x)dx = \epsilon P(x > \epsilon) \\ \\
        P(\vert \overline{x} - \mu \vert > \epsilon) &= P((\overline{x}-\mu)^2 >. \epsilon^2) \\
        &\le \frac{E\left[(\overline{x}-\mu)^2\right]}{\epsilon^2} \\
        &= \frac{Var(\overline{x})}{\epsilon^2} \\
        &= \frac{\sigma^2}{n\epsilon^2} \\
        \lim_{n \rightarrow 0} \frac{\sigma^2}{n\epsilon^2} &= 0
    \end{flalign*}
    \item Consistency tells us the distribution of an $\hat{\theta}$ collapses to a constant $\theta$, but we would like to know more about its shape
    \item Sequence of random variables $Y_1,Y_2, \dots, Y_n$ converges in distribution to a random variable $Y$ with cdf $F(y)$, denoted $Y_n \overset{d}{\rightarrow} Y$, if for all $y$, 
    \[
    F_{Y_n}(y) = P(Y_n \le y) \rightarrow F(y)
    \]
    as $n \rightarrow \infty$
    \item Intuition: convergence in distribution tells us we can treat $Y_n$ as if it has the same distribution as $Y$ when $n$ is large
    \item Central Limit Theorem (CLT) - Suppose $X_1,X_2, \dots, X_n$ are i.i.d. with $[E[X_i]=\mu$ and $Var(X_i)=\sigma^2$, then
    \[
        \frac{\sqrt{n}(\overline{X}_n - \mu)}{\sigma} \overset{d}{\rightarrow} N(0,1)
    \]
    where $X_n = \frac{1}{n} \sum_{i=1}^n X_i = \hat{\mu}$
    \begin{itemize}
        \item Note $E[\overline{X}_n]=\mu, Var(\overline{X}_n) = \frac{\sigma^2}{n}$, so we can treat the left-hand side as the standardization of a nomrally distributed random variable: $\frac{\overline{X}_n - E[\overline{X}_n]}{\sqrt{Var(\overline{\overline{X}_n}}}$
        \item Intuition: as we collect more data, estimator fluctuates normally around true parameter value
    \end{itemize}
    \item CLT provides one way to approximate the average or sum of a random sample using normal distribution
    \item Suppose a flight from London to New York has 168 economy class seats. The arline has estimated that each ticket holder has a probability 90\% of showing up at gate. If the arline has sold 178 tickets, what is the probability that the flight is overbooked?
    \item We often use $\phi(z)$ to denote the cdf of standard normal distribution, $\phi(z) = P(Z \le z)$, where $Z \sim N(0,1)$
    \begin{itemize}
        \item $\phi(z)$ can be looked up in the standard normal distribution table or Z-table
        \item Exact calculation is possible but inconvenient for large $n$
    \end{itemize}
    \begin{flalign*}
        x &= \text{\# of } \text{people } \text{showing} \\
        x &\sim Bin(178,0.9) \\
        P(X > 168) &= P\left(\frac{x-np}{\sqrt{np(1-p)}} \ge  \frac{169}{\sqrt{np (1-p)}} \right) \\
        &= P(z \ge 2.1999999) \\
         &= 1 - \phi(2.199) = 0.014
    \end{flalign*}
    \begin{itemize}
        \item We have looked at sample mean as an estimator for $\mu$
        \item Next, we will look at estimators and their properties for $\sigma^2$
        \item Recall our setting: $\{ X_1, \dots, X_n\}$ is a random sample drawn from a distribution with negative mean $\mu$ and variance $\sigma^2$
        \item Applying our methods of moments to construct an estimator by replacing expectations with averages of the sample:
        \[
        \hat{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2
        \]
    \end{itemize}
    \item Is $\hat{\sigma}^2$ an unbiased estimator for $sigma^2$
    \begin{flalign*}
        E[\hat{\sigma}^2] &= E[\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2] \\
        &= E[\frac{1}{n} \sum_{i=1}^n (X_i^2 - 2\overline{X}X_i + \overline{X}^2)] \\
        &= E[\frac{1}{n}  \left(\sum_{i=1}^nX_i^2 - 2\overline{X} \sum_{i=1}^nX_i + n\overline{X}^2\right)] \\
        &= E[\frac{1}{n}  \left(\sum_{i=1}^nX_i^2 - 2\overline{X} n \overline{X} + n\overline{X}^2\right)], \quad \sum_{i=1}^n X_i = n\overline{X} \\
        &= E[\frac{1}{n} \sum_{i=1}^nX_i^2 - \frac{1}{n}n\overline{X}^2] \\
        &= E[X_i^2] - E[\overline{X}^2] \\
        &= E[X_i^2] - E[X_i]^2 + E[X_i]^2 - E[\overline{X}^2] \\
        &=\sigma^2 + E[X_i]^2 - E[\overline{X}^2] \\
        &=\sigma^2 + E[X_i]^2 - \frac{1}{n^2} E[\sum_{i=1}^n X_i^2 + 2 \sum_{i < j} X_iX_j] \\
        &=\sigma^2 + E[X_i]^2 - \frac{1}{n^2} n E[X_i^2] -\frac{1}{n^2} n(n-1) E[X_i]^2, \quad E[X_iX_j] = E[X_i]E[X_j] = E[X_i]^2 \\
        &= \sigma^2 - \frac{1}{n}\sigma^2 = \frac{n-1}{n} \sigma^2
    \end{flalign*}
    \[
    E[\frac{n}{n-1}\hat{\sigma}^2] = \sigma^2, \quad \frac{n}{n-1}\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2
    \]
    \item Sample Variance - $\tilde{\sigma}^2 = s_X^2 = \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2$ and is an unbiased estimator for $\sigma^2$
    \begin{itemize}
        \item As $n$ increases, the difference between $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ becomes smaller and smaller, so in large samples, $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ should have very similar properties
        \item We can show that $\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}^2$
        \begin{flalign*}
            \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \\
            &= \frac{1}{n} \sum_{i=1}^n (X_i^2 - 2X_i\overline{X} + \overline{X}^2) \\
            &= \frac{1}{n} \left( \sum_{i=1}^n X_i^2 - 2\overline{X}(\sum_{i=1}^n X_i) + n\overline{X}^2 \right) \\
            &= \frac{1}{n} \left( \sum_{i=1}^n X_i^2 - 2n\overline{X}^2 + n\overline{X}^2 \right) \\
            &= \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}^2 \\
        \end{flalign*}
        \item By LLN, $\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X}_n)^2 \overset{p}{\rightarrow} \sigma^2$, so $\hat{\sigma}^2$ and $\tilde{\sigma}^2$ are both consistent estimators for $\sigma^2$
        \item Sketch of proof
        \begin{itemize}
            \item Functions of independent variables are independent, so $\{X_1^2,X_2^2, \dots, X_n^2\}$ is a random sample
            \item By LLN, $\frac{1}{n} \sum_{i=1}^n X_i^2 \overset{p}{\rightarrow} E[X^2]$
            \item By LLN, $\overline{X}_n = \frac{1}{n} \sum_{i=1}^n X_i \overset{p}{\rightarrow} E[X]$
            \item Converges in probability is preserved by continuous operations such as addition, multiplication, etc.
            \begin{itemize}
                \item As a result, $\overline{X}_n^2 \overset{p}{\rightarrow} E[X]^2$
                \item And $\tilde{\sigma}^2 = \frac{1}{n} \sum_{i=1}^n X_i^2 - \overline{X}_n^2 \overset{p}{\rightarrow} E[X^2] - E[X]^2$
            \end{itemize}
        \end{itemize}
    \end{itemize}
    \item We would like to test hypotheses about unknown parameters of interest $\theta$
    \begin{itemize}
        \item For example, we might want to test $H_0: \mu = 3, \quad H_1 : \mu \neq 3$
    \end{itemize}
    \item There are two types of errors
    \begin{itemize}
        \item Type I Error: null hypothesis $(H_0)$ is true, but we reject it
        \item Type II Error: alternative hypothesis $(H_1)$ is true, but we accept the null
    \end{itemize}
    \item The significance level of the test is defined as the probability of Type I error, $\alpha$
    \begin{itemize}
        \item Testing procedure: suppose $H-0$ is true, we want to check if $\overline{X}_n$ is far from 3 or not $(\vert \overline{X}_n - 3 \vert > c)$
    \end{itemize}
    \item Suppose $H_0$ is true, $\mu = 3$, then we know $\overline{X}_n \sim N(3,\frac{\sigma^2}{n})$ approximately, or $\frac{\overline{X}_n - 3}{\sqrt{\frac{\sigma^2}{n}}} \sim N(0,1)$
    \begin{itemize}
        \item If we set significance level at $\alpha = 10\%$ or 0.1, we would like
        \[
        P\left( \frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\sigma^2}{n}}} \ge k \right) = 0.1
        \]
        where $k$ is called critical value, and we know $k = 1.645$
        \item We reject $H_0$ if $\frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\sigma^2}{n}}} \ge 1.645$
        \item Hypothesis testing assesses whether sample evidence is too inconsistent with the null
        \item Critical value found by adding $(1-\alpha) + \frac{\alpha}{2}$
    \end{itemize}
    \item In practice, since $\sigma^2$ is often unknown, we replace it by its estimator $s_X^2$:
    \[
    \frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{\tilde{\sigma}^2}{n}}} \ge 1.645
    \]
    or equivalently, $\overline{X} \ge 3 + 1.645\sqrt{\frac{s_X^2}{n}}$ or  $\overline{X} \le 3 - 1.645\sqrt{\frac{s_X^2}{n}}$
    \begin{itemize}
        \item $\frac{\vert \overline{X}_n - 3 \vert }{\sqrt{\frac{s_X^2}{n}}}$ is referred to as t-statistic, and in some references, $\sqrt{\frac{s_X^2}{n}}$ is referred to as the standard error of $\overline{X}_n$ (estimate of the standard deviation of $\overline{X}_n$
    \end{itemize}
\end{itemize}

\section{Lecture 10 (01/12/2025)}
\begin{itemize}
    \item In hypothesis testing, we also often compute the p-value
    \begin{itemize}
        \item Specify $H_0$ and $H_1$
        \item Obtain data and compute $\overline{X}_n = \overline{x}_n^{actual}$
        \item Use $\overline{x}_n^{actual}$ to calculate the critical value
        \item Ask what is the smallest significance level to reject $H_0$?
    \end{itemize}
    \item In our example $H_0: \mu = 3$ vs $H_1: \mu \neq 3$
    \[
    p\text{-value} = P\left( \frac{\vert \overline{X}_n - 3 \vert}{\sqrt{\sigma^2 / n}} \ge \frac{\vert \overline{x}_n^{actual} - 3 \vert}{\sqrt{\sigma^2 / n}} \right)
    \]
    \item Since $\frac{\vert \overline{X}_n - 3\vert}{\sqrt{\sigma^2 /n}} \sim N(0,1)$, if $\frac{\vert \overline{x}_n^{actual} - 3 \vert}{\sqrt{\sigma^2 / n}} \ge 1.645$, p-value $\le 0.1$
    \begin{itemize}
        \item We reject $H_0$ if the significance level is 10%
    \end{itemize}
    \item In some cases, we might be interested in conducting one-sided tests
    \item For example, when studying whether additional years of education change earnings after graduation, the alternative hypothesis might be that people with higher education degrees earn more, since we believe it is unlikely that additional years of education lead to lower earnings
    \item In this case, we might want to test $H_0: \mu = 3$ vs $H_1: \mu > 3$
    \begin{itemize}
        \item Sometimes we write $H_0: \mu \le 3$ or $H_0: \mu < 3$
    \end{itemize}
    \item If the significance level is set at 0.1, we have $P \left( \frac{\vert \overline{X}_n - 3\vert}{\sqrt{\sigma^2 /n}} \ge 1.28 \right) = 0.1$
    \item Small sample size
    \begin{itemize}
        \item We can also conduct both one-sided and two-sided test for population mean using the $t$-statistic when the sample size is small
        \item In this case, the $t$-statistic $= \frac{Z}{\sqrt{W/(n-1)}}$ follows student $t$ distribution with $(n-1)$ degrees of freedom, $Z \sim N(0,1), W \sim \chi_{n-1}^2$, $Z$ and $W$ are independent
        \item We can compute critical values based on the student $t$ distribution
    \end{itemize}
    \item Difference between two means
    \begin{itemize}
        \item Suppose we continue our study about returns to education
        \item Suppose we have two random samples, one for population age 40 and above, and one for population below 40, the two populations have similar education levels (know if average earnings are different)
        \item Let $\hat{d} = \overline{Y}_{old} - \overline{Y}_{young}$
        \item We would like to test $H_0: d=0, H_1: d \neq 0$
        \item At 5\% significance level, $P\left( \frac{\vert \hat{d} - 0 \vert}{\sqrt{Var(\hat{d})}} \ge k \right) = 0.05, k = 1.96$
        \item We need an estimator of $Var(\hat{d})$
        \item First note $Var(\hat{d}) = Var(\overline{Y}_{old} - \overline{Y}_{young}) = Var(\overline{Y}_{old}) + Var(\overline{Y}_{young})$ since both sample are drawn randomly
        \item Then, $Var(\hat{d}) = \frac{\sigma_{old}^2}{n_{old}} + \frac{\sigma_{young}^2}{n_{young}}$, and we can find an estimator easily:
        \[
        \hat{Var}(\hat{d}) = \frac{s_{old}^2}{n_{old}} + \frac{s_{young}^2}{n_{young}}
        \]
        \item Since we know the distribution of $\hat{d}$, we can assess the uncertainty around the estimated difference in mean directly
        \item Given $\hat{d} \sim N(d, Var(\hat{d}))$, we have
        \begin{flalign*}
            0.95 &= P\left( \frac{\vert \hat{d} - d \vert}{\sqrt{Var(\hat{d})}} \le 1.96 \right) \\
            &= P \left(-1.96 \le \frac{\hat{d} - d}{\sqrt{Var(\hat{d})}} \le 1.96 \right) \\
            &= P \left(-1.96 \sqrt{Var(\hat{d})} - \hat{d} \le - d \le 1.96\sqrt{Var(\hat{d})} - \hat{d} \right) \\
            &= P \left( \hat{d} - 1.96 \sqrt{Var(\hat{d})} \le d \le \hat{d} + 1.96\sqrt{Var(\hat{d})} \right) \\
        \end{flalign*}
        \item We can replace $Var(\hat{d})$ with its estimator $\hat{Var}(\hat{d})$
        \item With probability 95\%, the true parameter $d$ falls in the interval $\left( \hat{d} - 1.96 \sqrt{Var(\hat{d})}, \hat{d} + 1.96\sqrt{Var(\hat{d})} \right)$
        \item We call this interval the 95\% confidence interval for $d$
        \item Intuition: if we draw our sample repeatedly, the confidence interval contains the true value $d$ 95\% of the time
    \end{itemize}
\subsection{Slide 10}
    \item We start with the simplest case: linear regression with single regressor, $Y$ on $X$ 
    \item Linear regression with single regressor lets us measure the slope, $\Delta Y / \Delta X$
    \begin{itemize}
        \item Interpretation: what is the effect on $Y$ when $X$ changes by one unit
    \end{itemize}
    \item Take test scores and STR as an example
    \begin{itemize}
        \item We collect data on students' test scores and STRs $(Y_i,X_i)$
        \item Answer: $\underset{b_0,b_1}{\text{argmin}} \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2$
        \begin{itemize}
            \item We choose the line with the smallest total vertical squared distance from the points
            \item We minimize the vertical distance because $Y_i$ is the variable whose variation we are interested in explaining using $X_i$
        \end{itemize}
    \end{itemize}
    \item The ordinary least squares (OLS) estimator solves
    \[
    \underset{b_0,b_1}{\text{argmin}} \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2
    \]
    \item We can show the solution to this problem is
    \[
    \hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}, \hat{\beta}_0 = \overline{Y} - \overline{X} \hat{\beta}_1
    \]
    \item We compute the derivative and set it equal to zero:
    \begin{flalign*}
        \sum_{i=1}^n (Y_i - (b_0 + b_1X_i))^2 &= (Y_1 - (b_0 + b_1X_1))^2 + (Y_2 - (b_0 + b_1X_2))^2 + \dots + Y_n - (b_0 + b_1X_n))^2 \\
        \text{First Order Condition: } -2 \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i) &= 0 \\
        \Rightarrow \sum_{i=1}^n Y_i - n\hat{\beta}_0 - \hat{\beta}_1 \sum_{i=1}^n X_i &= 0 \\
        \Rightarrow \hat{\beta}_0 &= \overline{Y} - \overline{X} \hat{\beta}_1 \\
        -2 \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1X_i)X_i &= 0 \\
        \Rightarrow \sum_{i=1}^n X_iY_i - \hat{\beta}_0 \sum_{i=1}^n X_i  - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0 \\
        \Rightarrow \sum_{i=1}^n X_iY_i - \overline{Y} \underset{n\overline{X}}{\underbrace{\sum_{i=1}^n X_i}} + \hat{\beta}_1 \overline{X} \underset{n\overline{X}}{\underbrace{\sum_{i=1}^n X_i}}  - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0 \\ 
        \Rightarrow \sum_{i=1}^n X_iY_i - n \overline{Y} \overline{X} + n \hat{\beta}_1 \overline{X} \overline{X} - \hat{\beta}_1 \sum_{i=1}^n X_i^2 &= 0 \\ 
        \Rightarrow \hat{\beta}_1 \left( n \overline{X}^2 - \sum_{i=1}^n X_i^2 \right) &= n \overline{X} \overline{Y} - \sum_{i=1}^n X_iY_i \\
        \Rightarrow \hat{\beta}_1 \left( \sum_{i=1}^n X_i^2 - n \overline{X}^2 \right) &= \sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y} \\
        \Rightarrow \hat{\beta}_1 &= \frac{\sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y}}{\sum_{i=1}^n X_i^2 - n \overline{X}^2} \\
        \sum_{i=1}^n X_iY_i - n \overline{X} \overline{Y} &= \sum_{i=1}^n \left( X_iY_i - \overline{X} \overline{Y} \right) \\
        &= \sum_{i=1}^n \left( X_iY_i - \overline{X}Y_i - \overline{Y}X_i + \overline{X} \overline{Y} \right) \qquad \because \sum_{i=1}^n \overline{X}Y_i = \sum_{i=1}^n \overline{Y}X_i = n \overline{x}\overline{y} \\
        &= \sum_{i=1}^n (X_i - \overline{x})(Y_i - \overline{y}) \\
        \sum_{i=1}^n X_i^2 - n \overline{X}^2 &= \sum_{i=1}^n (X_i^2 - \overline{X}^2) \\
        &= \sum_{i=1}^n (X_i^2 - 2x\overline{X} + \overline{x}^2) \qquad\because \sum_{i=1}^n x\overline{x} = n \overline{x}^2 \\
        &= \sum_{i=1}^n (X_i - \overline{X})^2 \\
        \Rightarrow \hat{\beta}_1 &= \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}
    \end{flalign*}
    \item Two observations given the expressions of the OLS estimator $\hat{\beta}_1 = \frac{\sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i - \overline{X})^2}$
    \begin{itemize}
        \item Little variation in $X_i$ can create problems, as the denominator can not be too close to zero
        \item Larger variation in $X_i$ leads to more precisely estimated $\beta_1$
    \end{itemize}
    \item In theory, we can also estimate $X = \delta_0 + \delta_1 Y$ by solving $\underset{\delta_0,\delta_1}{\text{argmin}} \sum_{i=1}^n (X_i - \delta_0 - \delta_1 Y_i)^2$, but $\hat{\beta} \neq \frac{1}{\hat{\delta}}$
    \item Population linear regression model:
    \[
    Y_i = \beta_0 + \beta_1 X_i + u_i
    \]
    $i = 1,2,\dots,n$
    \begin{itemize}
        \item $Y_i$ is the dependent variable
        \item $X_i$ is the independent variable (or regressor or covariate)
        \item $u_i$ is the error term and captures other factors which affect $Y_i$
        \item $\beta_0$ is the intercept, $\beta_1$ is the slope
    \end{itemize}
    \item There are three assumptions we are making:
    \begin{itemize}
        \item $E[u_1 \mid X_i] = 0$
        \begin{itemize}
            \item Equivalent to $E[Y_i \mid X_i] = \beta_0 + \beta_1 X_i$
            \begin{flalign*}
                E[Y_i \mid X_i] &= E[\beta_0 + \beta_1X_i + u_i \mid X_i] \\
                &= \beta_0 + \beta_1 E[X_i \mid X_i] + E[u_i \mid X_i] \\
                &= \beta_0 + \beta_1 X_i
            \end{flalign*}
            \item Assumes the conditional expectation is always a linear function (lots of cases when this is not true)
            \item For a given value of $X_i$, the expectation taken of $u_i$ needs to average to zero and $Y_i$ needs to average to the linear regression line
        \end{itemize}
        \item $(Y_i,X_i), i=1,\dots,n$ are i.i.d.
        \begin{itemize}
            \item Equivalent to assuming data are randomly sampled
            \item Some cases when this assumption may not hold are things like temporal correlation, spatial correlation, etc.
        \end{itemize}
        \item $0 < E[u_i^4] < \infty$ and $0 < E[X_i^4] < \infty$
        \begin{itemize}
            \item Can be interpreted as large outliers $(X_i, Y_i)$ are unlikely
            \item OLS can be sensitive to even a single outlier
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Tutorial 9 (05/12/2025)}
\begin{itemize}
    \item Question 1
    \begin{flalign*}
        \hat{\sigma}^2 &= \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 \\
        E[\hat{\sigma}^2] &= \frac{1}{n} \sum_{i=1}^n E[(X_i - \mu)^2] \\
        &= \frac{1}{n} n Var(X) \qquad \text{i.i.d. condition} \\
        &= Var(X) = \sigma^2
    \end{flalign*}
    \item Question 3
    \begin{flalign*}
        E[\overline{W}^2] &= Var(\overline{W}) - E[\overline{W}]^2 \\
        E[\overline{W}] &= \frac{1}{n} \sum_{i=1}^n E[W_i] \\
        &= \frac{1}{n} n\mu \qquad \text{i.i.d. assumption} \\
        Var(\overline{W}) &= Var\left( \frac{1}{n} \sum_{i=1}^n W_i \right) \\
        &= \frac{1}{n^2} Var(\sum_{i=1}^n W_i) \\
        &= \frac{1}{n^2} \sum_{i=1}^n Var(W_i) \qquad \text{independence assumption} \\
        &= \frac{1}{n^2} n\sigma^2 = \frac{\sigma^2}{n} \qquad \text{identical assumption} \\
        E[\overline{W}^2] &= \frac{\sigma^2}{n} + \mu^2
    \end{flalign*}
    \item $\hat{\theta}$ is a consistent estimator for $\theta$ if $\hat{\theta}$ converges in probability to $\theta$
    \begin{itemize}
        \item Check for asymptotic unbiasedness ($\lim_{n\rightarrow \infty} E[\hat{\theta}] = \theta$) and $\lim_{n \rightarrow \infty} Var(\hat{\theta}) = 0$
    \end{itemize}
    \item Question 4
    \begin{flalign*}
        s_X^2 &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \overline{X})^2 \\
        &= \frac{1}{n-1} \sum_{i=1}^n ((X_i - \mu) - (\overline{X} - \mu))^2 \\
        &=\frac{1}{n-1} \sum_{i=1}^n \left[(X_i - \mu)^2 + (\overline{X} - \mu)^2 - 2(X_1 - \mu)(\overline{X} - \mu)\right] \\
        &= \frac{1}{n-1} \sum_{i=1}^n (X_i - \mu)^2 + \frac{1}{n-1} n(\overline{X} - \mu)^2 - \frac{2n}{n-1} (\overline{X} - \mu)(\overline{X} - \mu) \qquad \sum_{i=1}^n X_i = n \overline{X} \\
        &= \frac{n}{n-1}\frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - \frac{n}{n-1} (\overline{X}-\mu)^2 \\
        &= \frac{n}{n-1} \left[ \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2 - (\overline{X} - \mu)^2 \right] \\
        &= \left( \frac{n}{n-1} \right) (A - B), \qquad A = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2, \qquad B = (\overline{X} - \mu)^2 \\
        E[A] &= \sigma^2, \qquad Var(A) = \frac{1}{n} Var(\tilde{X}_i^2) = \frac{1}{n} \left[ E[\tilde{X}_i^4] - E[\tilde{X}_i^2]^2 \right]
    \end{flalign*}
    \item Question 5
    \begin{itemize}
        \item Hypothesis testing recipe
        \begin{itemize}
            \item Assumptions
            \begin{itemize}
                \item Assume $X_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu_X,\sigma_X^2)$ for $i = 1,\dots,n_X$ and $Y_i \overset{i.i.d.}{\sim} \mathcal{N}(\mu_Y,\sigma_Y^2)$ for $i = 1,\dots,n_Y$, where $\sigma_X^2$ and $\sigma_Y^2$ are finite and $X_i \perp Y_i$ (independence) for all $i,j$, with $\mu_X,\mu_Y,\sigma_X^2,\sigma_Y^2$ unknown
            \end{itemize}
            \item Hypothesis
            \begin{itemize}
                \item $H_0: \mu_X - \mu_Y = 0$
                \item $H_1: \mu_X - \mu_y \neq 0$
            \end{itemize}
            \item Test statistic
            \begin{itemize}
                \item $t := \frac{(\overline{X} - \overline{Y})}{\left[ \frac{S_X^2}{n_X} + \frac{S_Y^2}{n_Y}\right]^\frac{1}{2}}$
            \end{itemize}
            \item Distribution of test statistic under $H_0$ and assumptions
            \item Significance level
        \end{itemize}
    \end{itemize}
\end{itemize}

\section{Lecture 11 (08/12/2025)}
\begin{itemize}
    \item Probability framework for OLs
    \begin{itemize}
        \item We can show that OLS estimators of $\hat{\beta}_0. \hat{\beta}_1$ are indeed reasonable estimators for $\beta_0$ and $\beta_1$
        \item $\beta_0$ and $\beta_1$ are true parameter values (constants) in the population model
        \item $\hat{\beta}_0$ and $\hat{\beta}_1$ are the parameter estimates (random variables)
        \item Similarly, $u_i = Y_i - \beta_0 - \beta_1 X_i$ is the error term defined in the population model
        \item $\hat{u}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$ is the estimated error we get by using estimates $\hat{\beta}_0$ and $\hat{\beta}_1$
        \item We start from assumption 1 ($E[u_i \mid X_i] = 0$) and apply the law of iterated expectations
        \begin{flalign*}
            E[u_i] = E[E[u_i \mid X_i]] = E[0] &= 0 \\
            E[u_i X_i] = E[E[u_iX_i \mid X_i]] = E[E[u_i \mid X_i]X_i] = E[0 \cdot X_i] &= 0 \\
            \frac{1}{n} \sum_{i=1}^n \hat{u}_i X_i &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i) X_i &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} + \hat{\beta}_1 \overline{X} - \hat{\beta}_1 X_i) X_i &= 0 \\
            \overline{y} - \hat{\beta}_0 - \hat{\beta}_1 \overline{x} &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y}) \overline{X} &= 0 \\
            \frac{1}{n} n \overline{Y} \overline{X} - \frac{1}{n} n \overline{Y} \overline{X} &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (\hat{\beta}_1 \overline{X} - \hat{\beta}_1 X_i) \overline{X} &= 0 \\
            \Rightarrow \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} + \hat{\beta}_1 \overline{X} - \hat{\beta}_1 X_i) (X_i- \overline{X}) &= 0 \\
            \frac{1}{n} \sum_{i=1}^n (Y_i - \overline{Y} - \hat{\beta}_1 \overline{X} (X_i - \overline{X})) (X_i- \overline{X}) &= 0 \\
            \Rightarrow \hat{\beta}_1 = \frac{\sum_{i=1}^n (Y_i-\overline{Y})(X_i - \overline{X}))}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
            \hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X} \\
        \end{flalign*}
        \item We use the above two equations and method of moments to construct estimators for $\beta_0$ and $\beta_1$
        \item We can show the estimates $\hat{\beta}_0$ and $\hat{\beta}_1$ have the same expressions as those obtained by solving the problem $\min_{b_0,b_1} \sum_{i=1}^n (Y_i - b_0 - b_1X_i)^2$
    \end{itemize}
    \item Properties of OLS estimators: unbiasedness
    \begin{flalign*}
        Y_i &= \beta_0 + \beta_1 X_i + U_i \\
        \overline{Y} &= \beta_0 + \beta_1 \overline{X} + \overline{U} \\
        Y_i - \overline{Y} &= \beta_1 (X_i - \overline{X}) + (U_i - \overline{U}) \\
        \hat{\beta_1} &= \underset{\beta_1}{\underbrace{\frac{\sum_{i=1}^n \beta_1 (X_i - \overline{X})(Y_i - \overline{Y})}{\sum_{i=1}^n (X_i -\overline{X})^2}}} + \frac{\sum_{i=1}^n \beta_1 (X_i - \overline{X})(U_i - \overline{U})}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
        \hat{\beta_1} &= \beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2} - \frac{\sum_{i=1}^n (X_i - \overline{X})\overline{U}}{\sum_{i=1}^n (X_i -\overline{X})^2} \\
        \sum_{i=1}^n (X_i - \overline{X})\overline{U} &= \frac{1}{n} n \overline{X}\overline{U} - \frac{1}{n} n \overline{X} \overline{U} = 0 \\
        \Rightarrow \hat{\beta}_1 &=\beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2}
    \end{flalign*}
    \item By the law of iterated expectations, since $E[U_i \mid X_1, \dots, X_n] = 0$,
    \[
    E[\hat{\beta}_1] = E\left[ \beta_1 + \frac{\sum_{i=1}^n (X_i-\overline{X})E[U_i \mid X_1, \dots, X_n]}{\sum_{i=1}^n (X_i - \overline{X})^2} \right] = \beta_1
    \]
    \item We can also show $E[\hat{\beta}_0] = \beta_0$ similarly
    \item The OLD estimators $\hat{\beta}_0$ and $\hat{\beta}_1$ are unbiased
\subsection{Slide 11}
    \item Properties of OLS estimators: consistency
    \begin{itemize}
        \item We can show the true parameter $\beta_1 = \frac{Cov(X_i,Y_i)}{Var(X_i)}$
        \item We have shown $\hat{\beta}_1 = \frac{\sum_{i=1}^n (Y_i-\overline{Y})(X_i - \overline{X}))}{\sum_{i=1}^n (X_i -\overline{X})^2} = \frac{\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})(Y_i - \overline{Y})}{\frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2}$
        \item We can see the numerator of $\hat{\beta}_1$ approximates $Cov(X_i,Y_i)$ and the denominator approximates $Var(X_i)$
    \end{itemize}
    \item Propoerties of OLS estimators: asymptotic distribution
    \begin{itemize}
        \item We can also use the central limit theorem to derive the asymptotic (large sample) distribution of the OLS estimates
        \item $\hat{\beta}_1 \sim \mathcal{N}(\beta_1,\sigma_{\hat{\beta}_!}^2), \hat{\beta}_0 \sim \mathcal{N}(\beta_0, \sigma_{\hat{\beta}_0}^2)$ approximately in large sample
        \item We take $\hat{\beta}_1$ as an example to describe the intuition
        \begin{itemize}
            \item Since $\hat{\beta}_1 = \beta_1 + \frac{\sum_{i=1}^n (X_i - \overline{X})U_i}{\sum_{i=1}^n (X_i -\overline{X})^2} = \beta_1 \frac{\frac{1}{n}\sum_{i=1}^n (X_i - \overline{X})U_i}{\frac{1}{n} \sum_{i=1}^n (X_i -\overline{X})^2}, \frac{1}{n} \sum_{i=1}^n (X_i - \overline{X})^2 \overset{p}{\rightarrow} Var(X_i)$, then $\sigma_{\hat{\beta}_1}^2 = \frac{Var[(X_i - E[X_i])U_i]}{n[Var(X_i)]^2}$
        \end{itemize}
        \item $\sigma_{\hat{\beta}_0}^2$ has a similar expression of $\frac{constant}{n}$
    \end{itemize}
    \item Standard errors of $\hat{\beta}_0$ and $\hat{\beta}_1$, $SE(\hat{\beta}_0)$ and $SE(\hat{\beta}_1)$ are the estimates of $\sigma_{\hat{\beta}_0}$ and $\sigma_{\hat{\beta}_1}$
    \item Standard errors tell us how reliable our estimates are
    \item Confidence intervals (CI): 95\% confidence intervals cover the true parameter value with probability 95\%
    \item Given the large sample distribution of $\hat{\beta}_1$, 95\% $CI = [\hat{\beta}_1 - 1.96 SE[\hat{\beta}_1], \hat{\beta}_1 + 1.96 SE[\hat{\beta}_1]]$
    \begin{itemize}
        \item The same CI expression appies to $\hat{\beta}_0$
    \end{itemize}
    \item We might be interested in testing for specific values of $\beta_1$
    \begin{itemize}
        \item $H_0 : \beta_1 = \gamma$ and $H_1: \beta_1 \neq \gamma$
    \end{itemize}
    \item The procedure is the same as the population mean test we discussed before
    \item Set $t$-statistic $= \frac{\hat{\beta}_1 - \gamma}{SE[\hat{\beta}_1]} \sim \mathcal{N}(0,1)$ under $H_0$
    \item For 5\% significance level, reject $H_0$ if $\vert t\text{-statistic} \vert > 1.96$
    \item The predicted value of the dependent variable is $\hat{Y}_i = \hat{\beta}_0 + \hat{\beta}_1 X_i$
    \item We can write predicted error $\hat{u}_i = Y_i - \hat{Y}_i = Y_i - \hat{\beta}_0 - \hat{\beta}_1 X_i$, which can also be called the residual
    \item Sum of square residuals, $SSR = \sum_{i=1}^n \hat{u}_i^2$
    \item Note that SSR is what we were minimizing when solving for $\hat{\beta}_0$ and $\hat{\beta}_1$, or how far the fitted line is to the data points
    \item Standard error of the regression $SER = \sqrt{S_{\hat{u}}^2}$, where $S_{\hat{u}}^2 = \frac{1}{n-2} \sum_{i=1}^n \hat{u}_i^2 = \frac{SSR}{n-2}$, which measures the average size of the regression residual or "mistake" made by OLS line
    \item Total sum of squares $TSS = \sum_{i=1}^n (Y_i - \overline{Y})^2$ measures the total variation in the dependent variable which is what we are trying to explain using the regression model
    \item Explained sum of squares $ESS = \sum_{i=1}^n (\hat{Y}_i - \overline{Y})^2$ measures the variation we are able to explain using the regression model
    \item $R^2 = \frac{ESS}{TSS} = 1 - \frac{SSR}{TSS}$, the fraction of variation in the dependent variable explained by the regression model, $0 \le R^2 \le 1$
    \item If $(\forall i) \ \hat{u}_i = 0$, then $Y_i = \hat{Y}_i, R^2 = 1$, we have a perfect fit
    \item If $\hat{\beta}_1 = 0, \hat{Y}_i = \hat{\beta}_0 = \overline{Y} - \hat{\beta}_1 \overline{X} = \overline{Y}$, then $ESS = 0, R^2 = 0$, which can be interpreted as $X_i$ does not explain any variation in $Y_i$
    \item Note $R^2$ increases with the number of independent variables, so when comparing models with different numbers of covariates, $R^2$ needs to be adjusted
    \item We can conduct the following hypothesis test: $H_0: \beta_1 = 0$ vs. $H_1: \beta_1 \neq 0$
    \begin{itemize}
        \item 95\% confidence interval $CI=[-2.28-1.96 \times 0.52, -2.28 + 1.96 \times 0.52] = [-3.30,-1.26]$, does not cover 0
        \item We say $\hat{\beta}_1$ is statistically significantly different from 0 at the 5\% level, or simple, statistically significant at the 5\% level
        \item This shows STR is a relevant factor for test score, however, given the low $R^2$, there might be other factors we are missing in the regression model, which may or may not change the relevance of STR
    \end{itemize}
\end{itemize}

\end{document}

